<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"augf.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Life&#39;s Notes">
<meta property="og:url" content="https://augf.github.io/page/5/index.html">
<meta property="og:site_name" content="Life&#39;s Notes">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yun-Pan Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://augf.github.io/page/5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Life's Notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Life's Notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">岁月数载，愿不负韶华</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/gnn-parallel-gnn-overview-paper-details/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/gnn-parallel-gnn-overview-paper-details/" class="post-title-link" itemprop="url">gnn-parallel-gnn-overview-paper-summarygnn summary paper read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:52:13" itemprop="dateCreated datePublished" datetime="2019-09-19T12:52:13+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/gnn/" itemprop="url" rel="index"><span itemprop="name">gnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="abstract">1. Abstract</h1>
<ol type="1">
<li>深度学习应用很广，特别是在图像分类和视频过程识别以及自然语言理解</li>
<li>这些任务的数据都表示在欧式空间中</li>
<li>有其他对于非欧式空间的需求， 将图表示为复杂的关系和交互依赖</li>
<li>图的复杂性很大</li>
</ol>
<p>文章： 1. 一个调查报告，深度学习在数据挖掘和机器学习领域的运用， 2.
将图的状态设计神经网络的计数分为不同的种类 3. 图的建设性的可替代结构,
包括图注意网络，图自动编码器，图生成网络和图的空间瞬时网络结构 4.
接下来，讨论了图的神经网络在各个领域的应用和找了开源代码和benchmark算法针对不同的任务。
5. 最后，提出了有意思的方向</p>
<h1 id="introduction">2. Introduction</h1>
<p>CNN，LSTM,等的成功</p>
<p>!欧式距离 vs 非欧式距离</p>
<p>？
核心机器学习算法样例之间一般互相独立，但是图数据中节点与节点之间有着相互的关系，比如引用，友元，相互作用</p>
<p>如何降低图的复杂性，最基本的方法还是模拟卷积的思想，即卷积核的大小可以来模拟图的不同权重值</p>
<blockquote>
<p>但是不与图像数据相同的是，图中每个节点的邻居节点是无序的而且大小不确定</p>
</blockquote>
<p><font id="fig1" color="red">fig1</font></p>
<p><img data-src="fig1.png" /></p>
<blockquote>
<p>? 这里说的是进行取平均？但是实际是怎么做的啊？</p>
</blockquote>
<h2 id="gnn过去发展的历史">2.1 GNN过去发展的历史</h2>
<p>第一个就是基于卷积的简单表示[21] 基于光谱图理论 [22],[23],[24]
对图的全局做处理，无法很好地并行化和应对大规模图</p>
<blockquote>
<p>这些方法的思想都是直接通过节点的邻居信息的特点， 加采样来做的</p>
</blockquote>
<h2 id="近期的工作">2.2 近期的工作</h2>
<p>其他人做的不好</p>
<h2 id="graph-neural-network-vs-network-embedding">2.3 graph neural
network vs network embedding</h2>
<p>graph neural network grph embedding</p>
<p>network embedding &gt;
目标是将网络节点表示在一个低维空间，并且保留拓扑和本身的内容信息 &gt;
用途是分类，聚类和推荐系统(支持向量机) &gt; 通常是半监督算法，常见类别有
&gt; - matrix factorization &gt; - random walk &gt; - deep learning</p>
<p><img data-src="fig2.png" /></p>
<h2 id="贡献">2.3 贡献</h2>
<ol type="1">
<li><p>新的分类学，五类 graph convolution networks GCN graph attention
networks GAN graph auto-encoders GAE graph generative networks GGN graph
spatial-temporal networks GSN</p></li>
<li><p>易于理解</p></li>
<li><p>丰富的资源</p></li>
<li><p>未来的方向</p></li>
</ol>
<h1 id="definition">3. Definition</h1>
<p><img data-src="table1.png" /></p>
<h1 id="categorization-and-frameworks">4. Categorization and
frameworks</h1>
<h2 id="种类">4.1 种类</h2>
<p><img data-src="works.png" /></p>
<h3 id="gcn">4.1.1 GCN</h3>
<blockquote>
<p>卷积+池化</p>
</blockquote>
<ol type="1">
<li>basic 通过考虑自己和邻居的特征信息，从而学习到表示该节点的函数</li>
</ol>
<p><img data-src="GCN1.png" /></p>
<ol start="2" type="1">
<li>+pooling <font id="GCN" color="red">GCN</font></li>
</ol>
<p><img data-src="GCN2.png" /></p>
<blockquote>
<p>卷积的理解：卷积核主要的目的是利用卷积核的设计使得能够保留原图像的局部信息；并且实现原图像的；卷积实际是一个抽象函数
<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/54033473">常见的卷积方法</a></p>
</blockquote>
<blockquote>
<p>池化的理解：当对每个图像做完卷积后，如果直接将特征拿去学习会太消耗；所以会考虑使用池化的方法，即学习这些数据中的聚集特征，比如最值、平均值、众数等等
<a
target="_blank" rel="noopener" href="https://blog.csdn.net/chenyuping333/article/details/82531047">常见的池化方法</a>
### 4.1.2 GAN Attention,
加入了注意力机制，思想即关注重要信息；主要的做法是为更重要的节点，路径或者模型安排权重</p>
</blockquote>
<blockquote>
<p>目标与GAN相同，试图学习一种新的结点的表达方式，能够整合邻居节点，随机路径和候选模型</p>
</blockquote>
<ol type="1">
<li>Attention vs Convolution</li>
</ol>
<p><img data-src="GAN.png" /> &gt; ???</p>
<h3 id="gae">4.1.3 GAE</h3>
<p>一种半监督学习的框架
目标在于通过编码器学习到低维表示，并能够通过译码器翻译回去</p>
<p>graph embedding</p>
<p><img data-src="GAE.png" /></p>
<h3 id="ggn">4.1.4 GGN</h3>
<p>目标在于从数据中学习到貌似合理的结构</p>
<p>通过一个图的经验分布来产生模型，本身就极具挑战的一件事</p>
<blockquote>
<p>一般的做法是首先形成产生节点和边，是否可替代，然后应用产生对抗训练进行筛选</p>
</blockquote>
<p>应用在化学成分分析中比较多</p>
<h3 id="gsn">4.1.5 GSN</h3>
<p>目标在于学习未见的模式通过暂时性空间结构的图
应用比如在交通预测和人类活动预测，图的结构往往是变化的
主要的想法就是同时考虑空间依赖和瞬时依赖 &gt; 空间依赖——GCNs来做 &gt;
瞬时依赖——RNN或CNN来做</p>
<p><img data-src="GSN.png" /></p>
<h2 id="框架">4.2 框架</h2>
<p>GCNs试图去复试CNN的成功在图中运用spectral theory和spatial
locality信息</p>
<ul>
<li>输入 图的结构信息和节点的内容信息</li>
<li>输出 不同的图的分析任务
<ul>
<li>Node-level
关于节点的回归或者分类任务，只需要直接给出节点的表示，通常用感知机或者softmax函数作为GCN的最后一层</li>
<li>Edge-level
关于边的分类或者链接预测任务。为了预测关于边的lable/connection的强度，一个额外的函数需要能够将两个节点的表示作为输入</li>
<li>Graph-level
关于图的分类任务，为了获得在图级别的联合表示，pooling需要能够把一个图改为子图来求和或者平均节点的表示</li>
</ul></li>
</ul>
<p><strong>main GCNs methods</strong></p>
<p><img data-src="summary.png" /></p>
<p><strong>end-to-end Traning Frameworks</strong></p>
<blockquote>
<p>这里的end-to-end理解是关注输入到输出吧
这里根据训练任务和标记信息是否可以直接获得可以分为监督、半监督和无监督学习</p>
</blockquote>
<ul>
<li>半监督学习 node-level classification.
首先通过部分有标记的图的节点的网络学习到robust model,
它可以高效地为未标记数据进行分类。
end-to-end框架可以通过堆大量的跟随softmax层用来做多标记学习 &gt;
对！这里会给出一个网络结构只有部分节点有标记</li>
<li>监督学习 graph-level classification
给出整个图数据集，图级别的分类任务旨在预测关于整个图的标记
end-to-end框架可以通过GCNs加polling来完成, 最后加些softmax层, <a
href="#GCN">GCN结构</a></li>
<li>无监督学习 graph embedding
这些算法采取两种角度来开发edge-level的信息
<ul>
<li>编码器+译码器</li>
<li>利用负采样的方法，采样一部分节点作为负pairs，并且已经存在一些节点在图中是正样本了，最后应用一个逻辑回归层
&gt; 负采样？？？</li>
</ul></li>
</ul>
<h1 id="graph-convolution-networks">5. Graph Convolution Networks</h1>
<p>spectral-based &gt;
介绍filters从图信号的角度，将卷积的操作看作消除噪声</p>
<p>spatial-based &gt; 从邻居节点收集特征信息</p>
<p>当GCNS操作在node level, pooling可以交替在GCN
layer之间，从而将图转换为高级别的子结构</p>
<h2 id="基于谱分析">5.1 基于谱分析</h2>
<h3 id="基本套路背景">5.1.1 基本套路（背景）</h3>
<p>然后每个节点利用的是拉普拉斯矩阵，以及特征值分解来做 首先，定义 <span
class="math display">\[L=I_n - D^{-1/2}AD^{-1/2}\]</span> robust
数学表示</p>
<p>实际上 <span class="math display">\[L =
\frac{A_{21}+A_{31}+..+A_{n1}}{A_{11}+A_{21}+A_{31}+..+A_{n1}}\]</span>
然后，该矩阵一定为半正定矩阵，所以得到特征向量U和特征值</p>
<p>那么，由可逆运算就可知 <span class="math inline">\(x=U
(U^Tx)\)</span> 然后如果再对x加上一个filter，那么就有 <span
class="math inline">\(x*_G g_\theta = U(U^T(x) .* U^T(g))\)</span> &gt;
这里.*是一种定义的新的运算，就是矩阵的每个元素进行相乘</p>
<p>化简后结果即为 <span class="math display">\[x*_G g_\theta = U
g_\theta U^T(x)\]</span> ？ &gt;
这里实际上就形成了图的卷积操作，这里的关键就在于如何进行设计<span
class="math inline">\(g_\theta\)</span></p>
<h3 id="实际的方法">5.1.2 实际的方法</h3>
<ol type="1">
<li><p>Spectral CNN <img data-src="equation4.png" /> 假设<span
class="math inline">\(g_\theta = \Theta_{i,j}^k\)</span>是可学习的参数
这里假设了转换前和转换后参数的个数，即又称为channel</p></li>
<li><p>Chebysheve Spectral CNN <span class="math inline">\(g_\theta =
\sum_{i=0}^K \theta_i T_k(\widehat{\Lambda})\)</span> 这里<span
class="math inline">\(T_k是递归函数定义\)</span>
经过化简操作，可以发现？？ <span class="math display">\[x*_G g_\theta =
\sum_{i=0}^{K-1} \theta_i T_i(\widehat{L})x\]</span> 这里<span
class="math inline">\(\widehat{L} = 2L/ \lambda_{max} - I_N\)</span>
&gt; 将时间复杂度从O(N^3)降低到了O(KM) 而且<span
class="math inline">\(T_i(\widehat{L})\)</span>计算时，只需要空间中的局部信息。</p></li>
<li><p>1阶ChebNet K=1, <span
class="math inline">\(\lambda_{max}=2\)</span>
该方法沟通了spectial-based和spatial-method。 在batch
training中，随着层数的数量的增加，计算开销会指数级扩张？？？</p></li>
<li><p>Adaptive Graph Convolution Network
为了扩展Laplacian矩阵的应用范围，因为Laplacian矩阵只能建模在无向图上。定义了一个叫做剩余矩阵的东西，可以用用来衡量两个节点之间的距离
尽管可能计算多余的东西，但是时间复杂度只有<span
class="math inline">\(O(N^2)\)</span></p></li>
</ol>
<h3 id="评价">5.1.3 评价</h3>
<p>Spectral CNN依赖于拉普拉斯矩阵的特征值分解 1.
图的任何一点扰乱将导致特征分解的重做 2. 学习到的filter是domin
dependent???, 意味着他们不能应用再一个不一样的结构中 3.
特征分解需要O(N<sup>3)的时间和O(N</sup>2)的空间</p>
<p>ChebNet and 1st ChebNet
学习到的filters定义在局部的空间，所以可以在图的不同位置进行共享。</p>
<p><strong>总结</strong> 关于该方法需要将所有图加载到内存中进行graph
convolution, 不利于解决大的图???ChebNet</p>
<h2 id="spatial-based">5.2 Spatial-based</h2>
<p><a href="#fig1">Fig1</a> figA,
对于image而言，每个像素点直接与周围的像素点像关联 figB,
为了扩展节点能容纳的深度和宽度，一个常用的方法就是将图的多个卷积层叠加在一起</p>
<p>Recurrent-based: same graph convolution layer来更新隐藏的表示</p>
<p>Composition-based: differnt graph convolution layer</p>
<p>比较</p>
<p><img data-src="rb-cb.png" /></p>
<h3 id="recurrent-based">5.2.1 Recurrent-based</h3>
<ul>
<li>impose constraints on recurrent functions</li>
<li>employ gate recurent unit architectures</li>
<li>update node latent representations asynchronously and
stochastically</li>
</ul>
<h4 id="graph-neural-networks">1) Graph Neural Networks</h4>
<p>递归计算直到收敛，扩散直到相等</p>
<p><span class="math display">\[h_v^t = f(l_v,, l_{co}[v],
h_{ne}^{t-1}[v], l_{ne}[v])\]</span></p>
<p><span class="math inline">\(l_v\)</span>-node v的属性 <span
class="math inline">\(l_{co}[v]\)</span>-node v相关的边的属性 <span
class="math inline">\(h_{ne}^{t-1}[v]\)</span>-v's 邻居在第t步的表示
<span class="math inline">\(l_{ne}[v]\)</span>- v's邻居的属性</p>
<p>为了保障收敛，f函数应该是收缩映射的
f是神经网络，那么对于参数的雅克比矩阵？应该得到惩罚</p>
<p>the Almeida-Pineda algorithm #### 2) Gate Graph Neural Networks gated
recurrent units——循环函数 recurrence循环</p>
<p><span class="math display">\[h_v^t = GRU(h_v^{t-1}, \sum_{u \in N(v)}
W h_u^t)\]</span></p>
<p>与GNN不同的是，GGCN通过时间上的后向传播来传递参数。</p>
<p>好处是不用保证收敛性，缺点会牺牲时间和空间上的效率，需要多次在全部节点上计算recureent函数，以及需要将所有状态都存储在内存中</p>
<h4 id="stochastic-steady-state-embedding">3) Stochastic Steady-state
Embedding</h4>
<p>SSE： 每个GCN,
随机地选取t个节点，进行更新表示然后随机第选取P个已经更新？的节点来更新梯度</p>
<p>为了提高预测效率，SSE随机异步地更新节点的表示。为了保证收敛性，recurrent函数定义为了历史状态和新状态的平均权重</p>
<p><span class="math display">\[h_v^t = (1-\alpha ) h_v^{t-1} +
\alpha  W_1 \delta (W_2[x_v, \sum_{u \in N(v)} [h_u^{t-1},
x_u]])\]</span></p>
<p><img data-src="SSE.png" /></p>
<h3 id="composition-based">5.2.2 Composition Based</h3>
<p>构成</p>
<h4 id="message-passing-neural-networks">1) Message Passing Neural
Networks</h4>
<p>产生已经存在的图卷积神经网络的一个框架。</p>
<p>包括两部分 - message passing phase</p>
<p>通常运行T步的基于空间的图神经网络算法。
图神经网络定义了一个信息函数和一个更新函数</p>
<p><span class="math display">\[h_v^t = U_t (h_v^{t-1}, \sum_{w \in
N(v)} M_t(h_v^{t-1}, h_w^{t-1},e_{vw}))\]</span></p>
<blockquote>
<p><span class="math inline">\(M_t\)</span>信息函数， <span
class="math inline">\(U_t\)</span>更新函数</p>
</blockquote>
<ul>
<li>readout phase</li>
</ul>
<p>通常对应于池化操作，产生基于整个图的一个表示</p>
<p><span class="math display">\[\widehat{y} = R(h_v^T | v \in
G)\]</span></p>
<p><span
class="math inline">\(\widehat{y}\)</span>用来做最终的graph-level的任务</p>
<blockquote>
<p>需要确定不同形式的Ut()和Mt()</p>
</blockquote>
<h4 id="graphsage">2) GraphSage*</h4>
<p>使用聚合的概念定义图的卷积</p>
<blockquote>
<p>?必须保证对节点的聚合值的排序的排列，那么其他的需要保证吗？ 使用mean,
sum, max等函数</p>
</blockquote>
<p><span class="math display">\[h_v^t = \delta(W_t \cdot aggregate_t
(h_v^{t-1}, \{h_u^{t-1}, \forall u \in N(v)\})\]</span></p>
<p>这里不是更新所有节点，而是进行batch-traning,适合扩展，说明是可以扩展到很大的数据集的？</p>
<p>3 steps 1. 对一个节点的k-hop邻居节点进行固定大小的采样 ？ &gt;
这里的采样范围是什么？ 一个圆吧 2.
通过聚集邻居节点的所有信息，最终产生该中心节点的最后状态 3.
使用中间节点的最终状态来做预测和前馈错误</p>
<p><img data-src="GraphSage.png" /></p>
<p>假定<span class="math inline">\(t^{th}\)</span>hop的邻居数量是<span
class="math inline">\(s_t\)</span>, one batch的时间复杂度为 <span
class="math inline">\(O(\prod _{t=1}^T s_t)\)</span>, 指数级别 &gt;
作者发现t=2是GraphSage已经取得很好的成绩</p>
<h3 id="miscellaneous-variants-of-spatial-gcns">5.2.3 Miscellaneous
Variants of Spatial GCNs</h3>
<h4 id="dcnn-扩散卷积神经网络">1) DCNN 扩散卷积神经网络</h4>
<blockquote>
<p>encapsulates 压缩</p>
</blockquote>
<p>通过图卷积网络压缩图的扩散过程</p>
<p>一个隐藏的节点的表示通过将输入 ？ 得到状态转移矩阵</p>
<p>维度搞</p>
<p><span class="math display">\[Z_{i,j,:}^m = f(W_{j,:} \odot
P_{i,j,:}^m X_{i,:}^m)\]</span></p>
<p>P是概率，X是特征， W是权重 &gt; 这里H是什么？ 应该是表示值什么的吧？
尽管转移矩阵的高维中覆盖了大量的域，需要<span
class="math inline">\(O(N_m^2H)\)</span>的内存，会造成巨大的问题 #### 2)
PATCHY-SAN</p>
<p>用一个标准的卷积神经网络CNN来解决图分类问题 graph classification
tasks</p>
<p>将图结构数据转换拿为网格结构数据</p>
<ol type="1">
<li>给每个节点安排一个排序，基于的标准有多种多样，比如度，形成图的标记</li>
<li>为每个节点根据图的标记选择固定数量的邻居节点</li>
<li>使用标准的CNN来进行学习</li>
</ol>
<p>CNN的优点：能够基于排序保证移动不变性</p>
<h4 id="lgcn">3) LGCN</h4>
<p>建议基于节点特征的信息进行排序</p>
<p>vs PATCHY-SAN:</p>
<p>产生
node-level输出，对于每个节点聚集了关于邻居节点的特征矩阵，然后再特征矩阵中为每列进行排序，
将头k行的排序特征作为输入</p>
<p>在最后，采用1D CNN在相关的输入上得到隐藏节点的表示</p>
<blockquote>
<p>PATCHY-SAN需要没复杂的预处理，而LGCN不需要预处理。
LGCN建议了子图训练的策略，即使用一小部分样本的子图进行min-batch</p>
</blockquote>
<h4 id="mixture-model-network">4) Mixture Model Network</h4>
<p>试图利用标准的CNN来统一非欧式空间域 &gt;
大部分基于空间的方法在考虑的时候都忽略了节点与邻居将的相对位置关系</p>
<p>做的方法就是为每个邻居节点的关系进行定义，然后赋值不同的权重。</p>
<blockquote>
<p>绝大部分采用CNN,也有用GCN的
MoNet建议了使用高斯核来进行学习参数来动态调节权重</p>
</blockquote>
<h3 id="summary">5.2.4 Summary</h3>
<p>循环神经网络获得节点的稳定状态
基于组成的方法试图集合高级别有用的信息</p>
<p>两种结构中，每一层在训练中都需要更新所有节点隐藏的状态，但是内存往往很难将这些状态存下。</p>
<p>所以，进行子图训练的方法： 1. GraphSage 2. 随机异步训练的方法 SSE</p>
<p>但近几年的进步还是通过空间方法来建立更复杂的网络机构 1.
用门控机制来控制节点的深度和宽度 GeniePath 2.
设计两个图的卷积网络来切入本地的一致性和全局一致性 DuraGCN 3.
超参数来影响节点接收域的大小</p>
<h2 id="图池化模块">5.3 图池化模块</h2>
<p>很重要， CNNs, down-sampling</p>
<p>其中，mean/max/sum
pooling是最原始和最有效的加快计算，降低维度的方法</p>
<p><span class="math display">\[h_G = mean/max/sum(h_1^T, h_2^T, \cdots,
h_n^T)\]</span></p>
<p>已经证明，先池化可以减少傅里叶开销</p>
<p>ChebNet 对输入图形首先进行coarsen处理，如图5a;
其次，顶点的输入和coarsen后的版本在平衡二叉树中得到了改进。
当节点最粗糙的版本传递到了平衡二叉树的最低级别时产生了一个特别好的信号，池化带来巨大好的效果</p>
<p>DGCNN SortPooling,
先按SortPooling得到一个顺序，再依次取出这些点染成WL colors,
再排序。根据排序的结果取最有用的前k个，如果不够则置为默认值(0).</p>
<p>这个方法增加了池化网络来解决一个图结构的任务，该任务需要保证排列不变性</p>
<p>DIFFPOOL</p>
<p>产生图分层的表示，然后不止于CNNs相结合，并且考虑使用其他变化的神经网络结构。
&gt; 池化层嘛，所以难道这里说的是这里的池化层可以处理不同CNNs来的东西
与之前的方法相比，不再简单地聚聚一个图中的节点，而是提供一种大类的解决办法来分等级低池化不同的输入图
&gt; 这里的意思是该框架可以对不同的图进行操作吗？</p>
<p>主要的想法是学习一个cluster S在不同的层次l代表不同的内容，如<span
class="math inline">\(S^{(l)} \in R^{n_l \times n_l +
1}\)</span>.两个分割的GNNs同时处理两个输入cluster的节点特征<span
class="math inline">\(X^{(l)}\)</span> 以及coarsened的邻接矩阵<span
class="math inline">\(A^{(l)}\)</span>用来产生新的分配矩阵<span
class="math inline">\(S^{(l)}\)</span>以及embedding矩阵<span
class="math inline">\(Z^{(l)}\)</span> <span
class="math display">\[Z^{(l)} = GNN_{l,embed}(A^{(l)},
X^{(l)})\]</span></p>
<p><span class="math display">\[S^{(l)} = softmax(GNN_{l,pool}(A^{(l)},
X^{(l)}))\]</span></p>
<blockquote>
<p>？？？ cluster在这里是指啥意思？？，聚集的意思，
等于是分层，l是层次，所以说这里l是input的第l个元素？，然后A,X是每个元素的特征？</p>
</blockquote>
<p>可扩展到多种标准的GNN模块，他们拥有相同的数据输入，却有种不同的参数，因为各自的角色不同。
<span class="math inline">\(GNN_{l,embed}\)</span>可以产生新的embeddings
而<span
class="math inline">\(GNN_{l.pool}\)</span>产生输入节点可能安排到哪个clusters的一个指派。
结果，<span class="math inline">\(S^{(l)}\)</span>的每一行代表一个<span
class="math inline">\(n_l\)</span>节点或者clusters在layer l,
每一列表示下一层的<span class="math inline">\(n_l\)</span>.
一旦我们拥有了<span class="math inline">\(Z^{(l)}\)</span>和<span
class="math inline">\(S^{(l)}\)</span> <span
class="math display">\[X^{(l+1)} = S^{(l)^T} Z^{(l)} \in R^{n_{l+1}
\times d}\]</span> &gt; 通过S的支配去Z的新的embdding中计算新的X的表示,
初始化的Z应该为节点的表示 <span class="math display">\[A^{l+1} =
S^{(l)^T} A^{(l)} S^{(l)} \in R^{n_{l+1} \times n_{l+1}}\]</span> &gt;
使用邻接矩阵为输入，然后产生一个变粗的coarsened的邻接矩阵，根据clusters中每个pair的链接性的强弱
总之，DIFFPOOL重定义了图的pooling模块，通过使用两个GNNs来聚集节点。
任何一个标准的GCN模块都可以进行联合，不仅可以使性能增强，还可以加快卷积的速度</p>
<blockquote>
<p>两个GNNs,
一个来获得新的表示，一个来获得新的指派；如果是多个，是用来构成一个大矩阵的吗？</p>
</blockquote>
<h2 id="对比-spectral-and-spatial">5.4 对比 Spectral and Spatial</h2>
<p>在早期，主要的进步是靠基于频域的，但是有着明显的缺点。 - efficiency:
图一改变就需要重新特征分解， 基于空间的可以只训练a batch of nodes,
当节点增加时，可以使用sampling技术来提高效率 - generality:
基于频域的假定一个固定的图，使他们普遍变为一个新的不同的图。基于空间
的图，他们在每个节点上做卷积，他们的权重依然被不同的位置和结构共享 -
flexibility:
基于频域的只适用于无向图，Laplacian矩阵并未在有向图上有很好的定义，唯一的方法只有将有向图转为无向图。基于空间的更加灵活，可以处理多源的输入，比如边的特征和方向，因为这些输入可以聚集在聚集函数中
所以， 基于空间的模型更受欢迎</p>
<h1 id="beyond-graph-convolutional-networks">6. Beyond Graph
Convolutional Networks</h1>
<h2 id="gan">6.1 GAN</h2>
<p>注意力机制在序列基准的任务中已经成为了标准，优点在于能够集中在最重要的东西。
这一特性被证明是非常重要的。</p>
<p>用注意力带来很多很出，在聚集的时候 ### 6.1.1 方法</p>
<h4 id="gat">1) GAT</h4>
<p>就是考虑所有邻居节点的表示和考虑邻居节点的权重 &gt;
不同的子空间是指的，随着当前所处的不同位置，可能得到的数据不同，所以就把所有子空间的邻居节点的位置堆叠吗。感觉是这样的</p>
<p><span class="math display">\[h_i^t = \delta(\sum_{j \in N_i} \alpha
(h_i^{t-1},h_j^{t-1}) W^{t-1}h_j^{t-1})\]</span></p>
<p><span
class="math inline">\(\alpha(\cdot)\)</span>是注意力函数，控制节点和邻居节点之间的贡献
为了在不同的子空间中学习权重</p>
<p><span class="math display">\[h_i^t = ||_{k=1}^K \delta(\sum_{j \in
N_i} \alpha_k (h_i^{t-1},h_j^{t-1}) W_k^{t-1}h_j^{t-1})\]</span></p>
<p>这里||表示级联 &gt; ?</p>
<h4 id="gaan">2) GAAN</h4>
<p>考虑multi-head, 不过不是分配等选中，屙屎进行不同的权重分配</p>
<p><img data-src="equation22.png" /> #### 3) GAM
循环神经网络来解决图分类问题，通过访问不同序列的重要节点来展示图的各部分的信息
<img data-src="equation23.png" /> <span
class="math inline">\(f_h(\cdot)\)</span>是LSTM网络 <span
class="math inline">\(f_s\)</span>是网络在过程中选取当前节点的邻居，并且优先选取有高排名的节点，它有policy
network生成</p>
<p><span class="math display">\[r_t = f_r(h_t;\theta_r)\]</span>
其中<span
class="math inline">\(r_t\)</span>是随机排名向量预测哪个节点更加重要，从而应该给该节点配置更高的优先级
<span
class="math inline">\(h_t\)</span>包含了历史信息，agent已经从图中获得的。agent是用来给图的标记做预测的
#### 4) Attention Walks 不像DeepWalk使用固定的先验，Attention
Walks产生共现矩阵通过不同的注意力权重</p>
<p><span class="math display">\[E[D] = \tilde{P}^{(0)} \sum_{k=1}^C a_k
(P)^k\]</span> 这里D是共现矩阵 <span
class="math inline">\(P^{(0)}\)</span>是初始化矩阵
P指示转移矩阵的概率</p>
<h3 id="总结">6.1.2 总结</h3>
<p>注意力机制对图神经网络有3种方式 1.
在整个邻居节点时为每个邻居赋予不同的权重 2. 通过注意力权重整合多个模型
3. 通过注意力来指导random walks</p>
<p>它们同时也可以认为是基于空间的卷积神经网络
GAT和GAAN的优点在于可以动态学习邻居节点的权重
但是会带来时间和空间的消耗，当每对邻居被计算的时候</p>
<p><img data-src="GAN.png" /></p>
<h2 id="gae-1">6.2 GAE</h2>
<p>GCN, GCN+GAN, LSTM+GAN来设计图的自动编码</p>
<h3 id="gcn-based">6.2.1 GCN based</h3>
<h4 id="gae-2">1) GAE</h4>
<p><span class="math display">\[Z=GCN(X,A)\]</span> decoder: <span
class="math inline">\(\tilde{A} = \delta (Z Z^T)\)</span></p>
<p>优化目标为最小variational lower bound &gt; 简单理解为之间的距离 <img
src="equation28.png" /></p>
<h4 id="agra">2) AGRA</h4>
<p>GCN做编码器</p>
<p>GANs 采用min-max策略在产生和测试之间训练生成模型。
一个生成器同概率生成真假模型 然后测试器在真的样例中区分出假的样例</p>
<p>GANs使得节点的表示服从某个先验分布</p>
<p>生成器生成假的模型，测试其判断得出真的模型</p>
<h3 id="混合变体">6.2.2 混合变体</h3>
<ol type="1">
<li>NetRA</li>
</ol>
<p>与ARGA想法相同，不过它还正则化节点的隐藏表示，使其服从先验分布，在对抗性训练的同时。
不是重建邻接矩阵，它是恢复节点的序列，通过sequence-to-sequence结构的random
walks中采样得到</p>
<ol start="2" type="1">
<li>DNGR 用堆叠的降噪的自动编码器来重建pointwise的互信息矩阵PRMI
当图形采用random walks进行序列化得到的序列是 PRMI捕捉到节点的共现矩阵
<img data-src="equation29.png" /></li>
</ol>
<p>其中<span class="math inline">\(|D|=\sum_{v_1,v_2}
count(v_1,v_2)\)</span></p>
<p>堆叠的降噪的自动编码器能够学习到高度非线性的规律，不同于传统的神经自动编码器，它增加了输入的噪声，通过随机地将输入入口置为0
当缺失值存在时，学习的潜力也就越大</p>
<ol start="3" type="1">
<li>SDNE 保护一阶接近度和二阶联合</li>
</ol>
<p>一阶proximity定义为节点隐藏表示和邻居节点隐藏表示之间的距离，
目标是尽可能让表示互相接近，特别地，loss函数可以定义为 <span
class="math display">\[L_{1st}=\sum_{i,j=1}^n A_{i,j} ||h_i^{(k)} -
h_j^{(k)}||^2\]</span>
第二节proximity定义为节点的输入和重新构造之间的距离。输入是邻接矩阵中对应的行，目标是用来保护邻居节点的信息，损失函数定义为
<span class="math display">\[L_{2nd}=\sum_{i=1}^n ||(\hat{x}_i-x_i)\odot
b_i||^2\]</span> <span
class="math inline">\(b_i\)</span>是用来惩罚非0元素的输入的，当矩阵高度稀疏时
当<span class="math inline">\(A_{i,j}=0\)</span>, <span
class="math inline">\(b_{i,j}=1\)</span>;<span
class="math inline">\(A_{i,j}&gt;0\)</span>, <span
class="math inline">\(b_{i,j}&gt;1\)</span> 因此，总的目标函数就定义为
<span class="math display">\[L = L_{2nd} + \alpha L_{1st} + \lambda
L_{reg}\]</span></p>
<ol start="4" type="1">
<li>DRNE 重建的目标是节点表示，而不是整个图的数据</li>
</ol>
<p>loss function <span class="math display">\[L = \sum_{u \in V} ||h_v -
aggregate(h_u \in N(v)||^2\]</span></p>
<p>LSTM作为聚集函数来聚集根据阶排完的节点的序列</p>
<h3 id="总结-1">6.2.3 总结</h3>
<p>DNGR和SDNE仅仅使用了图的拓扑数据，
GAE,ARGA,NetRA,DRNE同时考虑了两方面</p>
<p>最大的挑战是关于邻接矩阵的稀疏性，导致译码器的正实体的数量远远小于负实体，为了解决这个办法，DNGR构造了密集矩阵，SDNE为0实体处理了惩罚，GAE重新赋值了权重，NetRA将图线性化为序列</p>
<h2 id="ggn-1">6.3 GGN</h2>
<p>目标是通过观察一系列图产生图。 大部分的方法都是domin specific.
化学分子图SMILES, 一些工作提供可替代性的点或边，然后进行对抗性训练 GCN
building blocks或用其他不同的结构</p>
<h3 id="gcn-based-1">6.3.1 GCN based</h3>
<ol type="1">
<li>MolGAN 整合了GCN,提高的GAN和RL来产生图。
GAN包括产生器和测试器，与其他一起竞争从而提高产生器的真实性。
产生器试图产生真实的数据，测试器试图辨别。然后还会再增加一个奖惩网络</li>
</ol>
<p><img data-src="MolGAN.png" /></p>
<ol start="2" type="1">
<li>DGMG spatial-based graph convolution networks来获得图的表示。
产生图的节点和边有产生的图的表示来决定</li>
</ol>
<p>加入一个顶点到增长的图，如此递归操作, 直到达到停止的标准
当decision变为false时，开始加边 当decision为true,
它会评估将性加入的节点连接到已存在的节点的可能性分布，并且由分布随机选择一个节点，
当一个新的节点和它的连接都被加入到图中后，DCMG再次更新图的表示</p>
<h3 id="杂项">6.3.2 杂项</h3>
<ol type="1">
<li><p>GraphRNN 扩展深度图产生模型通过两个级别的循环神经网络。
graph-level RNN增加一个节点到一个节点序列 edge--level
RNN产生一个二进制序列只是新节点和之前产生节点之间的链接 为了训练graph
level RNN 为了将图线性化为序列，GrapRNN采用BFS. 为了建模edge-level RNN,
GraphRNN假设多元伯努利分布或条件伯努利分布</p></li>
<li><p>NetGAN 联合LSTM和Wassertein GAN通过random-walk-based方法来产生图
该框架包括两个模块，产生器和测试器
产生器尽可能去通过LSTM来产生最合理的random walks,
测试器尽可能地辨别其中的真假 经过训练，就会获得节点的共现矩阵</p></li>
</ol>
<h3 id="总结-2">6.3.3 总结</h3>
<p>大问题，不像同步的图像和音频，可以直接由专家判断判断，质量无法评估。
MolGAN和DCNG用来很多额外的知识来评估产生的图的合理性。
GrapRHH和NetGAN通过图的统计数据，比如图的阶数来做
DCNG和GrapRNN产生节点和边序列化的方式 MolGAN和NetGAN共同产生节点和边</p>
<p>之前的方法的问题在于，随着图变大，产生一个长的序列必然带来问题。
后面的方法是一个图的全局属性很难维护
最近的方法采用了变化的自动编码器来产生图，建议了使用惩罚机制的邻接矩阵，然而会增加输出空间从n到n^2
还没有方法具有很好的大规模可扩展性</p>
<h2 id="gsn-1">6.4 GSN</h2>
<p>需要同时维护空间和瞬时性因素。
这类图往往有一个全局的图结构，但是会随着时间而改变
交通中，每个传感器作为一个节点记录交通的速度，边作为节点之间的距离，
图的任务就是预测节点将来的值或者标记，以及图的标记</p>
<p>GCNs, GCN+RNN/CNN, 循环结构</p>
<h3 id="gcn-based-2">6.4.1 GCN based</h3>
<ol type="1">
<li>DCRNN 扩散的卷积——空间因素 GRU序列到序列的结构——瞬时</li>
</ol>
<ul>
<li>空间 <img data-src="equation34.png" /> <span
class="math inline">\(D_O\)</span>是出度矩阵，<span
class="math inline">\(D_I\)</span>是入度矩阵 为了允许多输入输出channel
<img data-src="equation35.png" /></li>
</ul>
<p>P是输入的channel的数量 - 瞬时
循环单元会同时接收上一步临时节点的历史信息和邻居节点的卷积信息。
GRU-&gt;DCGRU <img data-src="equation36.png" />
为了适应多步的需求，采用sequence-to-sequence的结构</p>
<ol start="2" type="1">
<li><p>CNN-GCN 1D-CNN和GCN学习spatial-temporal 图数据 对于一个输入tensor
<span class="math inline">\(X \in R^{T \times N \times D}\)</span>,
1D-CNN随着时间轴来收集每个节点的瞬时性信息<span
class="math inline">\(X_{[:,i,:]}\)</span>
GCN操作来聚集空间信息，在每次的时间步骤中， <span
class="math inline">\(X_{[i,:,:]}\)</span>
收集空间信息，输出层是一个线性信息，产生关于每个节点的预测</p></li>
<li><p>ST-GCN
扩展不同的时间流来作为图的边用一个统一的GCN模型来统计两种信息
它还有一个标记函数来为每个边安排标记，根据两个相关节点之间的距离
所以，邻接矩阵可以用一个K邻接矩阵的求和来表示，K是标记的数量
它应用GCN到不同的权重到K个邻接矩阵上，并且求和他们 <img
src="equation37.png" /> ### 6.4.2 杂项</p></li>
</ol>
<ul>
<li>Structural-RNN 目标是预测节点在每个时间节点的标记
它包括两种类型的RNN，nodeRNN和edgeRNN
每个节点和边的瞬时信息由nodeRNN和edgeRNN传递
因为不同的RNN对不同的节点和边会急剧地增加模型的复杂性，所以他们代替为将nodes和edges活粉到不同的语义组。
比如，一个人主题关系图，包括多种两组节点，人节点和物品节点；三种边，人与人的边，人与物品的边，人与物品的边。
在同一语义组的节点或边使用相同的RNN模型
为了合并空间信息，nodeRNN将会将edgeRNN的输出作为输入</li>
</ul>
<h3 id="总结-3">6.4.3 总结</h3>
<p>DCRNN的优点在于能够处理长时间，因为它的循环神经网络结构
CNN-GCN会更优效率得益于1D CNN
ST-GCN考虑瞬时流作为节点的边，将会导致邻接矩阵平方式地增长。一方面，会增加计算图卷积层的计算开销。另一方面，为了捕捉长期依赖，卷积层需要堆叠很多次。
Structural-RN提高了模型的效率，通过共现相同的RNN在同语义组中。但是，它需要人们的先验知识来分割语义组</p>
<h1 id="applications">7. Applications</h1>
<p>很多的应用，文学 4类数据集</p>
<p>开源的图神经网络</p>
<p>实际的运用在各种领域</p>
<h2 id="datasets">7.1 Datasets</h2>
<ol type="1">
<li>Citation Networks 引用网络
数，作者和他们之间的引用关系，作者之间的创作关系。
无向图，相关的任务比如node classification, link prediction, node
clustering tasks</li>
</ol>
<p>三个有名的数据集 - Cora: 7个类 - Citeseer：6个类 - Pubmed：
通过TF-IDF向量来宝石 - DBLP: 计算机科学书目，参考文献</p>
<ol start="2" type="1">
<li>Social Networks 网上服务用户之间的相互关系</li>
</ol>
<ul>
<li>BlogCatalog: bloggers, label就是兴趣</li>
<li>Reddit:
无向图，论坛收集到的帖子，两个帖子之间可能包含相同用户者的评论</li>
<li>Epinions:
多关系图，关于一件在线商品的评论，其中评论可能有多种关系，trust,
distrust, coreview, co-rating</li>
</ul>
<ol start="3" type="1">
<li>Chemical/Biological Graphs 化学元素的组成可以用化学原子作为点，
化学键作为边组成 &gt; 由此来考虑产生图分类任务 graph classification
performance</li>
</ol>
<ul>
<li>NCI-1 NCI-9</li>
<li>MUTAG</li>
<li>D&amp;D 蛋白质结构，属于两种</li>
<li>QM9 分子式</li>
<li>Tox21</li>
</ul>
<ol start="4" type="1">
<li>Unstructured Graphs 无结构图
为了探索图神经网络在无结构化数据上的泛化性，k-NN使用最广泛</li>
</ol>
<ul>
<li><p>MNIST: 70000 28*28 &gt; ? convert to graph:
8-NN?graph基于像素的位置</p></li>
<li><p>Wikipedia dataset &gt; 单词共现网络</p></li>
<li><p>NewsGroup datasets 多个文档被划分为了20多种类别 &gt;
将文档作为节点，相似性作为节点的边的权重</p></li>
</ul>
<ol start="5" type="1">
<li>Others</li>
</ol>
<ul>
<li>METR-LA: 交通数据库</li>
<li>MOvieLens-1M: 6k用户为1million商品的排序,推荐系统的基准</li>
<li>NELL： 语言学习项目，事实以及实体之间的关系 <img
src="datasets.png" /></li>
</ul>
<h2 id="benchmarks-开源扩展">7.2 Benchmarks 开源扩展</h2>
<p>最常用的数据集 Cora, Pubmed, Citesser, PPI
大量的超参数，需要开源的程序才能获得同样的结果 <img
src="methods.png" /></p>
<p>开源代码 <img data-src="open-codes.png" /></p>
<h2 id="实际运用">7.3 实际运用</h2>
<p>GNNs 用于节点聚类，关系预测，图划分</p>
<ul>
<li>Computer Vision 计算机视觉
最大的应用，通过利用图神经网络探索视频中的图形结构，进行点云分类和分割，动作识别以及其他方向
在场景图的生成过程中，语义之间的关系有利于理解视觉场景背后的意义</li>
</ul>
<p>通过给定图片场景，图生成模型检测和识别物体以及预测物体之间的语义关系</p>
<p>另一个应用就是通过给定的场景图生产实际的图像</p>
<p>每个自然语言解释为一个对象，那么就可以根据给定的语义说明类构件出图像</p>
<p>在点云分类和分割中，一个点云通常被看做是一个3D点。那么更觉设备探索周围的㕂，有可以识别出有问题的汽车。
为了辨别物体通过点云，将点云转化为k邻居图或者抄点图，可以用图结构来探索其拓扑结构
&gt; 点云？？</p>
<p>在动作识别中，可以将人的关节分解，架构成图，从而用空间瞬时网络结构来学习人的动作行为</p>
<p>图像分类，人与人之间的行为，语义分割，视觉推理和问题解答</p>
<ul>
<li><p>推荐系统
得到高质量的推荐，推荐的关键就是为一个物品的重要性为用户打分。link
prediction</p></li>
<li><p>交通 graph based
空间瞬时图，路建模成点，路之间的距离建模成边，每个时间段的值建模为特征，目标来预测在某个时间段某段路的平均速度。
节省资源和节约能源</p></li>
</ul>
<p>taxi-demand prediction
给出taxi需求的历史数据，位置信息，天气信息和时间特征，可以德奥一个对于每个位置的joint表达，从而来一段时间内的汽车需求</p>
<ul>
<li>化学 学习化学式的结构</li>
</ul>
<p>node 分类，图分类，图形生成
在分子图上，学习分子指纹，预测分子特性，推理蛋白质种类，合成化学物</p>
<ul>
<li>其他</li>
</ul>
<p>节目评测，时间推理，社会原因分析，恐怖时间预测等等 # 8. Future
Directions 图的复杂性 ## Go Deep
深度的网络结构，经过无限次的卷积，所有节点的表示可以聚集为一个单节点；所以提高层数仍然是一个好的办法
## Receptive Field &gt; 感受野？？？ 什么东西？
这里的意思就是一个节点的感受野就是指中心节点以及他的所有邻居节点。
邻居的数量服从一个合法的分布，每个节点可能有一个邻居，可能多成千上百个邻居
采样引入后，任何选择一个节点的感受野值得研究 ## Scalability
可扩展性，大规模的图并不能扩展的很好。
主要的原因是当打包层次的图卷积时，该节点的最终装填会引起邻居的最终状态的改变，所以就会带来高复杂性的后向传播
现有的解决办法有快采样和子图训练，但是不够可扩展来解决大规模图的深度结构
## 动力学和异质性 现有的图神经网络做的都是静态的同质的图。
一方面，图的结构需要固定，另一方面，节点和边是同一来源。这两个假设不符合实际情况
比如在社会网络中，有人加入和退出 在推荐系统中，输入可能为图形或者文字
所以需要考虑新的方法</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/english-pronuncation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/english-pronuncation/" class="post-title-link" itemprop="url">english pronuncation</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:44:31" itemprop="dateCreated datePublished" datetime="2019-09-19T12:44:31+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/english/" itemprop="url" rel="index"><span itemprop="name">english</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/english/pronuncation/" itemprop="url" rel="index"><span itemprop="name">pronuncation</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>70</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>音标复习</p>
<p><img data-src="pronuncation.jpg" /></p>
<h1 id="单元音">1. 单元音</h1>
<h2 id="section">1</h2>
<p><img data-src="listen-basic-1.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th>## 2 <img data-src="listen-basic-2.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## 3 <img data-src="listen-basic-3.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-1">4</h2>
<p><img data-src="listen-basic-4.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th>## 5 <img data-src="listen-basic-5.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## 6 <img data-src="listen-basic-6.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-2">7</h2>
<p><img data-src="listen-basic-7.png" /></p>
<h1 id="双元音">2. 双元音</h1>
<h2 id="section-3">1</h2>
<p><img data-src="listen-basic-di-1.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th>## 2 <img data-src="listen-basic-di-2.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## 3 <img data-src="listen-basic-di-3.png" /></td>
</tr>
<tr class="even">
<td># 3. 辅音1 ## 1 <img data-src="listen-basic-f-1.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-4">2</h2>
<p><img data-src="listen-basic-f-2.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th>## 3 <img data-src="listen-basic-f-3.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## 4 <img data-src="listen-basic-f-4.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-5">5</h2>
<p><img data-src="listen-basic-f-5.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<thead>
<tr class="header">
<th>## 6 <img data-src="listen-basic-f-6.png" /></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>## 7 <img data-src="listen-basic-f-7.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-6">8</h2>
<p><img data-src="listen-basic-f-8.png" /></p>
<h1 id="辅音2">4. 辅音2</h1>
<h2 id="section-7">1</h2>
<p><img data-src="listen-basic-f2-1.png" /></p>
<table style="width:7%;">
<colgroup>
<col style="width: 6%" />
</colgroup>
<tbody>
<tr class="odd">
<td>## 2 <img data-src="listen-basic-f2-2.png" /></td>
</tr>
</tbody>
</table>
<h2 id="section-8">3</h2>
<p><img data-src="listen-basic-f2-3.png" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/TOEFL-word-overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/TOEFL-word-overview/" class="post-title-link" itemprop="url">TOEFL word overview</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:40:58" itemprop="dateCreated datePublished" datetime="2019-09-19T12:40:58+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/english/" itemprop="url" rel="index"><span itemprop="name">english</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/english/TOEFL/" itemprop="url" rel="index"><span itemprop="name">TOEFL</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/english/TOEFL/word/" itemprop="url" rel="index"><span itemprop="name">word</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="词缀">词缀</h1>
<p>五星级词缀 - im,in: 否定，进入，加强 - dis: 低级别 no, 高级别 away;
apart - ad: to， toward - re: back - a+p: 强调，一再 - mis: bad -
phi：good - homo 相同的 hetero不同的 - suc= sub 下 - se 分开，诱使 =
seduce - uni 单一的 - per aeay, entirely</p>
<p>前缀 - 时间： ex,pre,pro re=back - 位置 inter/out over,super/sub
trans under - 数量 uni bi centi milli mono multi - 程度 super over under
hyper well - 方式 auto co mal mis self - 态度 anti counter pro - 否定
ab, an, de, in, non, under</p>
<p>后缀 - v: ate, ish, ify, ch, en, ize, ise - n: or, ee, ess, an/ian,
ance, ant, ity, th, ary, um, age, let, ness, al, ese, hood, ics, ion,
ism, ist, ment, ship - a al, tle, tive, id, que, ous, less, able, free,
ful, ic, like, some, y</p>
<h1 id="其他">其他</h1>
<p>换词方法 1. 元音字母 2. 辅音字母组 - h,c,k,g,q,qu - b,p,ph,f,v - f,t
- s,t,d - m,n,r,l,y,j,g - z,j,d - u,v,w</p>
<h1 id="词根">词根</h1>
<h2 id="总览">总览</h2>
<ol type="1">
<li><p>学术： sci, science, log, eco, nomy, ics</p></li>
<li><p>数学 meter, equ, part, numer</p></li>
<li><p>物理</p>
<ul>
<li>力 tract, grav, griev, pel,plus, mob, mot, celer</li>
<li>热 therm, calori, frig,friger, ferv, zeal</li>
<li>声 son, voc, vok, audi,audit, tone, phony, plod, verber, echo</li>
<li>光 lumin, lustr,lusc, lust,radi,cand, flect,flex, sol, sight,splend,
vis,vid</li>
<li>电 electro,tele, z</li>
</ul></li>
<li><p>化学: pos,posit,pon,pound,part,
port,mini,micro,macro,acid,acri,acu,acrid,ac,dia,sal
tox,struct,mod,form,cruc,crus,curx,norm,
solu,solv,solute,di,lut,lav,lux,rupt,spers,
lect,brev,bridge,vers,vert</p></li>
<li><p>天文: astro,aster,aer,aero,aeri,stell,atory</p></li>
<li><p>地质地理: terr,tele,cata,caco,call,ign</p></li>
<li><p>生物: bio,vi,li,herb,flor,flour,agr,agri,agro</p></li>
<li><p>历史: chron,
annual,ann,eon,loc,stat,stit,stiut,grap,gram,vict,vinc,vol,volv,volt</p></li>
<li><p>政治:
arch,tect,dem,cracy,crat,popul,publ,fus,fug,fun,mon,monit,polis,polic,polit,range,
serve,sequ,sent,sue,urb</p></li>
<li><p>人类学:
anthrop,anthr,andry,gyn,gynec,gamy,patri,matri,matr,amor,amot,am,phil,philo,ster,
path,soph,phob,phobia,flu,sect,seg,gen,gener,genit,gress,soci,migr,ori,orig,kara,oke,cur,
curs,cour,cours,man,manu,pass,pat,sens,sent,tim</p></li>
<li><p>文化:
creed,cred,fid,liber,ment,nov,prais,preci,duc,duct,art,lingu</p></li>
<li><p>法学:
leg,legis,jur,juris,dict,dic,jurg,judic,cern,cert,cret,fin,fort,her,hers,hibit,odin,
prov,prob,strict,strain,string,sur,term,treat</p></li>
</ol>
<p>校园生活类:
act,ag,alter,altern,don,dot,cre,creaters,mir,not,nounce,nunci,pend,pens,pli,ple,plen,
plet,plex,port,press,pute,quest,quer,quir,quis,scend,scent,scens,sid,st,sta,stant,stat,sist,
simil,simul,sembl,spir,sum,tag,tact,tig,ting,tain,ten,tin,tend,tent,tens,tom,turb,us,ut,ven,
vent,vail,val</p>
<h2 id="细节">细节</h2>
<p>人文科学 1. 学术 - sci: to know; conscious - science=knowledge:
conscientious - log=word/speech logue monologue - log=reason/idea logic
anthropology - eco=house,dwelling,place economy ego=self egecentric -
nomy=rule, subject(agronomy astronomy); management, rule(autonomy) -
ics=logic 学科 physcis dynamics 2. 数学 - algebra 代数 - meter: 1.
measure (optometrist, telemeter); 2. dimension (kilometer) 3. gemotry
(diameter) - equ=equal, even adequate 充分的 inequilty - par=equal, pair
parity 平价 compare - numer=number numerable 很多的，可数的 enumerate 3.
物理 - 力 - tract=draw abstract traction extract - grav, griev=to dig,
heavy; gravity aggravate - pel,puls=drive, push, shake,swing expel
compel - mob=move,乌合之众 mobile autombile - mot=1.move(motion, motive)
2. a witty saying (motto) - celer=quick,speed celerity accelerate - 热 -
therm=heat, warm thermal diatheraml isotherm - calori=heat calorie
calorify<br />
- frig,friger=cold frigid 冷的 refrigerator 冰箱 - ferv=boil perfervid
fervent - zeal=ardor zealous jealous - 声 - son=sound sonic resonance -
voc,vok=call,voice vocal evoke - audi, audit=hear audit audience -
tone=sound monotonous 单调的 - phony euphony 悦耳之声 symphony - plod
applaud explode - verber=beat, vibrate reverberatem - echo=sound
anechoic - 光 - lumin=light illuminate luminosity - lustr,luc=light
lust:desire,wish,longing lustre illustrate - light lighter enlighten -
radi=ray,root radio, radical ,radix - cand=white candle, candid - flect,
flex=bend reflect - sol=1. sun (solor solace console) 2. solo (sole,
solitary) - sight=vision sightly - splend=be right,shine splendid -
vis,vid=to see, to know,wise (visible envy provide); separate (divide) -
opt,opto=sight optic - 电 - electro=electric electron - tele=distant
telephone - Z=fast zigzag 4. 化学 - pos,posit=1.put
(pose,composite);2.pause (repose, 休息) - pon,pound=put,pause componet
compound - part, port=part particle depart - mini minimal, minimum -
micro microsoft - macro macroword - acid,acri,acu,acrid=sour, sharp
acid, acerbic - ac=acid,angle acute acuity - dia=across diagonal -
sal=salt salty salify saline salary - tox=posion toxic toxicology -
struct=to build, pile ,assemble structure construct - mod=mode, manner,
measure modest modern - form=shape formal conform - cruc,crus,curx=cross
十字形，交叉，关键 crucial crux cruise - norm=rule, norm norm normal -
solu, solv,solute=loosen,disperse 分解 solvable solvent - di=divide,
separate - lut,lav,lux=wash lavish 浪费的 lavatory 厕所，浴室 -
rupt=break, rupture abrupt interrupt bankrupt - spers=scatter 少年凯
disperse 分散 intersperse - lect, lig=choose,gather elect collect select
- brev, bridge=make short abreviate brief - vers,vert=turn-over adverse
adversity averse 反对 convert 5. 天文 - astro, aster=star astronomy
cosmonaut - aer,aero, aeri=air aerate tongqi aerosphere - stell=star
stellar interstellar - atory=场所 laboratory lavatory<br />
6. 地质地理 - terr=1.piece of earth,ground and land(terrain);2. to
treamble抖 terrible terrific - tele=far/distant telegraph -
cata,caco=kakos,bad,down catalog catastrophe cacophony -
call=kailo,beauty,call; calligraphy - ign=fire, scarifice ignite
ignitable 7. 生物 - bio=life biology biosphere - vi=li=live - herb
herbivorous herbaceous - flor,flour=flower,florid floral flora - agr,
agri,agro=field, land agrarian, agriculture 社会科学 1. 历史 -
chron=time Cronus chronic synchronize - annual,ann,eon=year annual
anniversary - loc=location locate locomotion - stat, stit, stiut=set
up,place stature statute constitute - grap=write,draw, scratch, picture
photography telegraph - gram=write program grammer diagram - vict,
vinc=克服，fight victory convict<br />
- vol,volv,volt=roll,turn,twist voluble 有才的 revolution revolve
involve 2. 政治 - arch=ruler,chief,government archy architect
architecture archives - tect=texture,build,do - dem=people epidemic
pandemic endemic - cracy=rule, crat=ruler democracy democrat -
popul,publ=people population populous popularize - fus,fug=flee,run away
refugee refuge - fun=pour refuse confuse infuse -
mon,monit=mind,warn,advice summon monition - muni,mun=public community
communal - polis,polic,polit=govern,state,city police policy politics -
range=rank,ring arrange rearrange - serve conserve preserve -
sequ,sent=follow sequence consequence sequential - sue=follow consue
pursue - urb=city urban suburb 3. 人类学 - anthrop=human anthropology -
anthr,andry=man polyandry - gyn,gynec,gamy=queen,woman gynecology
monogyny - patri=father patriarch patriot - matri,matr=mother maternal
matriarch - amor,amot,am=love amour amatory amateur - phil,philo=love
philantropy phiology<br />
- ster=duty stern sterile stare - path=suffering,feeling,illness,disease
pathetic apathy - soph=debate,wise sophist sophisticate -
phob,phobia=run,fear phobia acrophobia - flu=flow fluency fluctuate
effluent influence - sect,seg=divide, piece, cut section insect -
gen,gener,genit=birth,produce,create genus gennerate - gress=step,go
aggress progress ingress - soci=social society asocial - migr = to
change,go and move migrant migrate - ori,orig=rise, begin orient origo
originate - karaoke kara=head,room,empty oke=orchestra - cur,curs,
cours, cour=run cursory incur currency - man,manu=hand,strength,power
over manual manicure manage -
pass,pat=1.feeling,suffering,enduring(passion impassion passive) 2.
通过(passage, passenger, passport) - sens,sent=sense,feeling agreement
sense sensible consent - tim=fear timid meticulous 4. 文化 -
creed,cred=believe,trust credit credible creditable - fid=trust, faith
confident confide - liber=free,noble liberty intellectual liberal -
ment=mind mental mentality mention - nov=new, yound,recent, shape novel
novlty innovate - prais,preci=praise, value,price appraise precious -
duc,duct = to pull, drag;lead,bring ductile educate product -
art=skill,joint,trick artful artistry artifact - lingu=language
linguistics 5. 法学 - leg,legis=law legal illegal legislate -
jur,juris=law, swear jury injury - dict,dic=say,speak, point out dictate
addict - jurg,judic=judge judgment judicial - cern, cert, cret=separate
concern discern discrete - fin=end, boundary,limit fine finite -
fort=force enforce - her,hers=stick, cling adhere hesitate -
hibit=hold,posses habit exhibit exhibition - odin=order ordinal
coordinate - prov,prob=test front probe disprove -
strict,strain,string=tighted strict stricture constrict - sur=sure
secure assure - term=limit,end, boundary terminal determine -
treat=handle treaty treatment maltreat 校园生活类 - act,ag=to do, to
drive active actual - alter, altern=to change alternate - don,dot=give
donate,donor - cre,creaters=grow,make create increase -
mir=look,laugh,wonder miracle,mirror - not(e)=observer,mark=know notice
notify denote - nounce,nunci=speak,shout anounce denounce -
pend,pens=hang pendant pending - pli,ple,plen,plet=to full,fill plenty
supplement - plex=fold complex duplex - port=carry,take import export -
press pressure express - pute=to think,pruno compute dispute -
quest,quer,quir,quis=seek,search quest inquest acquire -
scend,scent,scens=climb ascend descend - sid=sit reside residue preside
- st,sta,stant,stat, sist=stand stable obstacle assist - simil,
simul,sembl=alike,same similar simile assimilate semblance -
spir=breathe, spit soul,courge,vigor,breath spirit expire -
sum=1.total,super(summary); 2. to make up(resume) -
tag,tact,tig,ting=tangent,touch tactual contact - tain,ten,tin=hold
attain maintain - tend,tent,tens=stretch tend tendency trend contend -
tom=cut atom tome epitome - turb=stir disturb turbulence perturb -
us,ut=use usage utilize usual - ven,vent=come advent venture convene -
vail,val=valid,be of use; strong,value avail valuable evaluate</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/python-thinking/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/python-thinking/" class="post-title-link" itemprop="url">python thinking</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:35:34" itemprop="dateCreated datePublished" datetime="2019-09-19T12:35:34+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/python/thinking/" itemprop="url" rel="index"><span itemprop="name">thinking</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>333</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>python学习指南</p>
<h2 id="tuple-list-dict的基本操作">tuple, list, dict的基本操作</h2>
<ol type="1">
<li><p>增加元素，默认</p></li>
<li><p>重新赋值元素</p></li>
<li><p>删除元素</p></li>
<li><p>查找某元素是否存在</p></li>
<li><p>list的切片</p></li>
<li><p>元素统计</p></li>
<li><p>互相转化</p></li>
</ol>
<h2 id="输入输出">输入输出</h2>
<ol type="1">
<li><p>标准输入和文件输入</p></li>
<li><p>标准输出和文件输出</p></li>
<li><p>格式化输出 "%s "%('da','ada')</p></li>
</ol>
<h2 id="目录管理">目录管理</h2>
<ol type="1">
<li>打开路径、目录生成、删除等</li>
<li>保存</li>
</ol>
<h2 id="面向对象">面向对象</h2>
<ol type="1">
<li>继承，多继承</li>
<li>多态</li>
<li>参数化</li>
</ol>
<h2 id="数据处理">数据处理</h2>
<p>Numpy, ndarray - 数据整理和清理、子集构造和过滤、转换等 -
数组算法，如排序、唯一化、集合运算 - 描述统计和数据聚聚合/摘要运算 -
异构数据集的合并/连接运算 - 条件逻辑表述为数组运算 - 数据的分组运算</p>
<p>pandas</p>
<p>Matpliot</p>
<p>数学函数图形</p>
<p>平面图</p>
<p>散点图</p>
<h2 id="注释与测试">注释与测试</h2>
<ol type="1">
<li><p>文件注释</p></li>
<li><p>函数注释</p></li>
<li><p>单元测试</p></li>
</ol>
<p>file的打开，文件的打开</p>
<p>环境的特殊的变量的设置</p>
<p>函数</p>
<p>类，抽象化设计</p>
<p>随机数</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/machinelearning-cornerstone-why-can-machines-learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/machinelearning-cornerstone-why-can-machines-learn/" class="post-title-link" itemprop="url">machinelearning cornerstone why can machines learn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:32:40" itemprop="dateCreated datePublished" datetime="2019-09-19T12:32:40+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/" itemprop="url" rel="index"><span itemprop="name">machinelearning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/cornerstone/" itemprop="url" rel="index"><span itemprop="name">cornerstone</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>542</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="training-vs-testing">5. Training vs Testing</h1>
<h2 id="datasize">5.1 DataSize</h2>
<p><img data-src="00020.png" /> <img data-src="00021.png" /> <img
src="00022.png" /></p>
<blockquote>
<p>理解在精度下的容忍度，从而指导需要从实际情况中选择多少个样本？</p>
</blockquote>
<h2 id="effective-number-of-lines">5.2 Effective Number of Lines</h2>
<blockquote>
<p>union bound over-estimating ?</p>
</blockquote>
<p>如何衡量？ VC维来进行衡量，实际是哪个关注的是对数据的划分的数目。
&gt; 只需要可以将样本可分即可。</p>
<p>所以不同的算法就有假设空间</p>
<blockquote>
<p>再次理解线性函数
突然想到感知机的函数，感知机这里其实就是每个样本都有一个权值。 ？
在图像上，可以理解权就表示的是直线的斜率。
然后，x表示向量，即原点到这里的向量，那么进行更新就好了。 ？
如何预测的</p>
</blockquote>
<p>样本点是空间分布的 为什么觉得样本点是空间分布的是可行的？ &gt;
可以样本点在各个维度上的坐标作为特征来考虑。</p>
<p>2个 3个 +,-,+不可分 &gt; 可能有6种，可能有8种；最多8种。 4个， <img
src="00025.png" /> &gt; 任何情况下，都最多只有8种 &gt;
由此我们可以看出，得到了VC维的定义，在d维下存在，在d+1维下任意都不满足（考虑进行取反）。因为探究的是最多的情况</p>
<h2 id="effective-number-of-hyperthesis">5.3 Effective Number of
Hyperthesis</h2>
<p>大致过一下 Why can machine learn?
因为错误率可用VC维或R复杂度来衡量</p>
<p>How Can Machines Learn? 算法具体是怎么做的</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/machinelearning-cornerstone-when-can-machines-learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/machinelearning-cornerstone-when-can-machines-learn/" class="post-title-link" itemprop="url">machinelearning cornerstone when can machines learn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:30:31" itemprop="dateCreated datePublished" datetime="2019-09-19T12:30:31+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/" itemprop="url" rel="index"><span itemprop="name">machinelearning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/cornerstone/" itemprop="url" rel="index"><span itemprop="name">cornerstone</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="the-learning-problem">1. The Learning Problem</h1>
<h2 id="introduction">1.1. Introduction</h2>
<p>theory+technique 多个角度概念 what + why + how + better:</p>
<h2 id="what-is-ml">1.2. what is ML?</h2>
<h3 id="definetion">1.2.1 definetion</h3>
<p>observation-&gt;learning-&gt;skill data-&gt;ml-&gt;skill</p>
<blockquote>
<p>so data is more like observation, will be better. learning skill how
to use in data</p>
</blockquote>
<p>learning: ml:</p>
<p>skill: improve some performance measure.</p>
<h3 id="application">1.2.2 Application:</h3>
<p>stock data-&gt;ML-&gt;more stock gain</p>
<ul>
<li>Tree Recognition 'define' trees and hand-program: difficult learn
from data(observations) and recognize</li>
</ul>
<h3 id="use-scenarios">1.2.3 Use Scenarios</h3>
<p>navigating on Mars cannot' define the solution &gt;
一般世界人，都使用的是规则来判定一件事。 high-frequency trading
consumer-targeted marketing</p>
<h3 id="key-essence-of-ml">1.2.4 key essence of ml</h3>
<ol type="1">
<li>some underlying pattern to be learned exisits target eg: predict
whether the next cry of baby</li>
<li>but no easy definition ! eg: determining whether a given graph
contains a cycle</li>
<li>this is data about the pattern eg:whether the earth will be
destoryed</li>
</ol>
<h2 id="learning-problem">1.3. Learning Problem</h2>
<ol type="1">
<li>Daily Needs Food Clothing Housing Transportation</li>
<li>Education</li>
<li>Entertainment</li>
</ol>
<h2 id="ml-vs-dmaistatistics">1.4 ML vs DM,AI,Statistics</h2>
<p>from f -&gt; ML -&gt; g. <img data-src="00001.png" /></p>
<p>ML vs DM <img data-src="00002.png" /> DM can help ML, and vice versa.
efficient computation</p>
<p>ML vs AI <img data-src="00003.png" /></p>
<p>ML vs Statistics(inference ) <img data-src="00004.png" /></p>
<h1 id="learning-to-answer-yes_no">2. Learning to Answer Yes_No</h1>
<h2 id="example">2.1 Example</h2>
<h3 id="pla算法设计">PLA算法设计</h3>
<p><strong>设计思想</strong> 知错能改，纠正错误的思想. 图形结合法来求解
<strong>类别</strong> 局部</p>
<p><strong>步骤</strong> 线性函数，感知机。
修正参数，w_{t+1}在错误案例上进行修正。 &gt; 知错能改法 &gt; Cyclic
PLA,简单的检查是否还有错误。 &gt;
对数据饶一圈，让这第100个人都没有犯错</p>
<h3 id="pla算法的正确性">PLA算法的正确性</h3>
<p><strong>假设</strong> 1. 问题具有最优解 2. 样本真实，没有噪声</p>
<p><strong>收敛到最优解</strong></p>
<p><strong>算法会停止（求迭代轮数）</strong></p>
<p>PLA用图形的夹角来做。P8</p>
<p>PLA是否可学习？ 判断两个数是否接近？ 做累积，两个向量内积越来越大</p>
<p><img data-src="00005.png" /></p>
<p><img data-src="00006.png" /></p>
<blockquote>
<p>资料是否是线性可分？资料这里是指D，还是实际分布</p>
</blockquote>
<blockquote>
<p>从某种分布上来说，未知资料是否线性可分。</p>
</blockquote>
<blockquote>
<p>理论这里对吗？就是是否线性可分 ## 2.2 Learning with noisy data</p>
</blockquote>
<blockquote>
<p>待测数据上有噪声 假设犯错率很低，那么假设没有犯错率</p>
</blockquote>
<p>假设没有噪声，但不知道是否可学习怎么办？ 试图去找犯错误最小的。
NP-hard</p>
<p>科学家的做法：找不到全局最好的解，所以考虑使用贪心。 Pocket
Algorithm，在自己的口袋保留最好的算法，根据步长随机更新的思想。 <img
src="00013.png" /></p>
<p><img data-src="00007.png" /> &gt;
因为是最优算法所以返回的结果必然都是最优的。对于算法的比较，记住一定从时间复杂度上看。
还有一旦是线性可分，那么就意味着算法都可以不犯错。</p>
<h1 id="types-of-learning">3. Types of Learning</h1>
<h2 id="v1-输出空间角度">v1 输出空间角度</h2>
<ul>
<li>binary classification</li>
<li>mulitclass classification</li>
<li>回归问题 &gt; 对于有限的复杂的输出可以看作是多分类问题</li>
<li>结构学习问题：比如学习蛋白质的长相，讲的话的语法树（输出空间中具有某种结果）
&gt; 其实从某种意义上也是划归到多分类上去。 ## v2 资料的角度</li>
</ul>
<ol type="1">
<li>监督学习: 完全是有标记样本</li>
<li>非监督学习: 未标记样本，分群 &gt; 监督可以看作是没有答案 &gt;
聚类，文章主题</li>
</ol>
<ul>
<li>density estimation: {x_n}</li>
<li>outlier detection: &gt; 难衡量好坏，因为没有目标</li>
</ul>
<ol start="3" type="1">
<li>半监督学习：有标记样本+未标记样本。</li>
<li>强化学习: 不断强化，给予惩罚和奖励；不断学得最终的目标。 &gt;
喂给机器资料，对机器进行评价，好还是不好 &gt; partial/implicit
information</li>
</ol>
<p><img data-src="00008.png" /></p>
<h2 id="v3-取得样本的方式学习的角度">v3 取得样本的方式，学习的角度</h2>
<ol type="1">
<li>Batch Learning</li>
<li>Online learing</li>
<li>active: "question asking" --query the yn of the chosen xn &gt;
有技巧地问问题，可以用很少的问题学习到很多的东西。标记很贵的情况。</li>
</ol>
<p><img data-src="00009.png" /></p>
<h2 id="v4-特征的角度">v4 特征的角度</h2>
<p>二维向量：对称性、密度性 - concrete - abstract
客户的编号，实际就是一种抽象了。 - raw <img data-src="00010.png" /></p>
<blockquote>
<p>如何找一个最合适的特征</p>
</blockquote>
<blockquote>
<p>至少可以从这些分类中寻找一个感兴趣的东西 <img data-src="00011.png" /> # 4.
Feasibility of Learning</p>
</blockquote>
<p><img data-src="00012.png" /> ## 4.1 Learning is impossible? adversarial
teacher &gt; 此时如何学习？ &gt; 有什么问题？目标不唯一 &gt;
这也是为什么 &gt; No free lunch,
我们坚持的是什么？如果我们坚持f不知道，我们要在D之外学到东西是不可能的。f一定是定下来的，否则是无法解决。再理解</p>
<blockquote>
<p>这个道理很常见，就像是猜数一样，一定要目标一定才可猜。</p>
</blockquote>
<blockquote>
<p>这也是为什么要用机器学习算法的原因，一般地话，为什么要有测试集，就是试图用测试集来衡量训练误差。</p>
</blockquote>
<blockquote>
<p>还有对于某些题的设定有问题，这其实就只是一种解释，需要更多的解释来完成这个问题
## 4.2 Inferring Something Unknown can we infer something unknown in
other scenarios? 随机采样，这就是为什么要用采样了？ 参数估计，大数定理。
数学上的定理Hoeffding，相差很远的可能性很小；特别是当样本够大，得到的估计趋于真实的估计。
注意这里两个方面，看多个变量 <img data-src="00014.png" /></p>
</blockquote>
<blockquote>
<p>从瓶子里去橙色球和其他颜色球，怎么知道原本中含有多少橙色球？一个想法就是通过对瓶子
的球进行采样一定数量，那么，根据采样的数量来就可以来估计真实的值。</p>
</blockquote>
<h2 id="connection-to-learning">4.3 Connection to Learning</h2>
<p><img data-src="00015.png" /></p>
<p>注意，这里考虑了假设空间和问题空间是一致的。
与学习之间的联系。首先我们有一大堆的假设，并从这些假设中抽取一个假设作为真实的假设。
然后，根据这个假设我们就需要得到未来的函数。那么，想法就是从满足这些假设的数据中独立同分布地抽取多个数据。
首先，数据如果足够多，利用这些数据学得的假设，并且，我们有理由相信从这些东西中学得的假设。
在数据集特别大的情况下，来学习得到的假设会更好，更满足实际的情况。</p>
<p>测试数据从某种意义上的设置就是想得到怎么样的数据？ &gt;
但是一般来说测试数据都是中肯的
还有一个方面就是用来纠正学习到的模型，因为学习到的模型有时候是过拟合的，即可解释为泛化能力。</p>
<p><strong>Verification of One h</strong></p>
<p><img data-src="00016.png" /> &gt;
这里比较了real学习和根据经验误差来学习的差别。</p>
<p>算法总是得到相同的假设，由此可以觉得该算法不好？ &gt;
假设空间小就意味着算法不好吗？ &gt;
经验误差小就意味着算法好吗？从某个方面可以这样看</p>
<p><img data-src="00017.png" /></p>
<p>ML学得的规则，不一定对过去有效，因为只是过去的抽样数据；不一定对将来有效，因为有一定误差的存在；在未来一百天中选规则也不成，因为构不成学习，不过是很多规则的累加</p>
<h2 id="connection-to-real-learning">4.4 Connection to Real
Learning</h2>
<p>bad sample: E_in and E_out far away--can get worse when involving
"choice".</p>
<blockquote>
<p>注意，数据的合理性，即样本是否合理。从某种意义上来说，需要看样本是否满足一些基本的分布，详细见西瓜书。
就是说所选取的样本是不好的样本，从另一个角度来看，就是一个指导的感觉就是还是说的是独立同分布的问题，即是否满足独立同分布。</p>
</blockquote>
<blockquote>
<p>训练的样本不是好的样本，这是一个检查的标准。</p>
</blockquote>
<p><strong>什么是Bad Sample</strong> 这个和过拟合有关吗？
好的数据任意进行选择就可以。
但是坏的数据会使算法踩到雷，算法得到不好的结果。</p>
<p>分析：假设对于每个假设都有bad example, 那么如何。</p>
<p>就是需要选取某个数据集，所有假设基本上都不会踩到雷。</p>
<p>评价，只要有一个不好就认为不好。 那么，不好的数据集的大小是多少？
<img data-src="00018.png" /></p>
<p>M是假设的个数。</p>
<blockquote>
<p>结论：只有样例过多才能满足要求；
现实的指导意义：只有尽可能地将数据认为是正反例合理的情况，否则其他不合理。</p>
</blockquote>
<p>所以，当数据合理时，我们认为满足PAC，此时Ein=Eout,
Eout就是在未知数据集上的表现情况；注意这里还未引入泛化误差的概念。那么，实际上就是选经验误差最小的，因为经验误差在这里认为是等于泛化的。</p>
<p>而泛化误差是用来进行评价的。</p>
<p>到此为止，说明了假设空间为有限的情况，并未扩展到假设空间为无限的情况。</p>
<p><img data-src="00019.png" /> &gt; 理解！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/machinelearning-cornerstone-summary/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/machinelearning-cornerstone-summary/" class="post-title-link" itemprop="url">machinelearning cornerstone summary</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:27:58" itemprop="dateCreated datePublished" datetime="2019-09-19T12:27:58+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/" itemprop="url" rel="index"><span itemprop="name">machinelearning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machinelearning/cornerstone/" itemprop="url" rel="index"><span itemprop="name">cornerstone</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>what is ML?</p>
<ol type="1">
<li>定义 人的过程，观察，学习到技能</li>
</ol>
<p>机器的过程，数据学习到技能</p>
<p>技能即如何？</p>
<ol start="2" type="1">
<li>关键</li>
</ol>
<ul>
<li>存在潜在的模式</li>
<li>没有很简单的定义</li>
<li>有足够的数据</li>
</ul>
<ol start="3" type="1">
<li><p>应用场景 衣食住行</p></li>
<li><p>辨析 from f-&gt;ML-&gt;g: f为目标</p></li>
</ol>
<p>ML vs DM： DM is interesting pattern, ML is find underlying pattern
ML vs AI: AI is 智能 ML vs Statistics:
统计是试图从以往数据中学习点什么出来</p>
<ol start="5" type="1">
<li>例子 PLA， Perceptron算法，线性感知机法，主要想法是进行 eg：
图形结合的方法找最优解 &gt; 假设： 1. 问题具有最优解 2.
样本真实，没有噪声</li>
</ol>
<p>实际问题 - 算法怎么操作？ - 算法的正确性分析——算法能达到最优解。
是否存在线性可分的直线，如果存在，那么研究是否收敛到最优解。然后算法是否能停止。
&gt; 收敛的分析，探究与最优解的接近程度，内积是一种方法 &gt;
是否停止，即到达常数。 - 算法的时空复杂度分析： 对算法进行比较</p>
<p>对于最优的算法，其返回结果必然是最优的。</p>
<blockquote>
<p>每个算法的假设空间是否是一样的呢？
每一个问题都有假设空间，即其中参数来确定假设空间。但是由于假设空间不一定与问题空间对应，所以不一定能得到问题空间的解</p>
</blockquote>
<blockquote>
<p>这里是不是要看算法的类别？ 对！</p>
</blockquote>
<p>思考 如果假设1，2不成立怎么办？ 假设没有噪声，但不知道是否可学习？
寻找经验误差最小的。 此时，经验误差最小的难度是NP-hard. &gt;
一种做法就是选择局部最优解来做。</p>
<p>线性w^T形成的就只是线吗？ &gt;
对，只是线，这里其实是感知机模型，换句话说就只是所有样本点一起决定下的权值。
表示能力是否欠缺？ &gt; 只有可以线性可分的数据集才可以用。</p>
<p>PLA算法更新公式确实是改变了一定的角度，如何保证该改变的角度一定就引起来类别的转化？</p>
<p>随机化算法是怎么做的？</p>
<p>扩展： 假设不知道算法是否存在最优解，采用口袋算法，即 算法流程：
step1 随机选择一个w step2 如果有错误样本进行一次修正 step3
没有错误则停止，否则重复执行step2</p>
<ol start="4" type="1">
<li>分类</li>
</ol>
<ul>
<li>输出空间
<ul>
<li>2</li>
<li>多</li>
<li>回归</li>
<li>结构学习</li>
</ul></li>
<li>资料
<ul>
<li>监督</li>
<li>非监督</li>
<li>半监督</li>
<li>强化</li>
</ul></li>
<li>学习
<ul>
<li>Batch</li>
<li>online</li>
<li>active</li>
</ul></li>
<li>特征
<ul>
<li>concrete具体</li>
<li>raw原始</li>
<li>abstrut 过于抽象，编号信息</li>
</ul></li>
</ul>
<ol start="5" type="1">
<li>学习的前提 需要假设存在目标，且目标是一定的 Learning impossible,
adversarial</li>
</ol>
<p>于是在这里引入No Free Lunch, 目标不一定就无法进行学习，对！</p>
<p>学习的目标不可知, 如何学习？
还好有样本，可以使用概率论的概念即抽样来学习。</p>
<p>为什么可以学习？ 利用的就是抽样。 - 对应关系：
问题空间-&gt;假设空间</p>
<ul>
<li>关注样本，需要好的样本</li>
</ul>
<blockquote>
<p>不好的样例和不好的事</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/deeplearning-cs231n/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/deeplearning-cs231n/" class="post-title-link" itemprop="url">deeplearning cs231n</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:23:33" itemprop="dateCreated datePublished" datetime="2019-09-19T12:23:33+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deeplearning/" itemprop="url" rel="index"><span itemprop="name">deeplearning</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/deeplearning/cs231n/" itemprop="url" rel="index"><span itemprop="name">cs231n</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="introduction">1. Introduction</h1>
<ol type="1">
<li>A brief history of computer vision
<ul>
<li>creatures -&gt; vision</li>
<li>camera</li>
<li>what is the vision processing
<ul>
<li>Original picture</li>
<li>differntiated picture</li>
<li>Feature points selected</li>
</ul></li>
<li>MIT research <img
src="asserts/1-stagesOfvisualRepresentation.png" />
<ul>
<li>Inpute image
<ul>
<li>preceived intensities</li>
</ul></li>
<li>Edge image
<ul>
<li>zero crossing, blobs, edges, bars, ends, virtual lines</li>
</ul></li>
<li>2 1/2-D sketch
<ul>
<li>local surface orentation</li>
</ul></li>
<li>3D</li>
</ul></li>
<li>how to recognize and represent the world</li>
<li>image segmentation: Jitendra Malik</li>
<li>face detection</li>
<li>1999-2000, momentum动量, ML, adaBoost algorithm to do real-time face
detection, 2001</li>
<li>2006, digital camera a real-time face detector</li>
<li>SIFT feature: match and the entire object due to camera angles,
occlusion, viewppoint, lighting</li>
<li>Spatial pyramid matching: from different parts of the image and in
different resolutions and put them together in a feature</li>
<li>HoG Histogrm of Gradients</li>
<li>imagenet</li>
<li>SVM, AdaBoost, overfiting: not enought training data, cannot
generalize. benchmarking WordNet</li>
<li>2009 ImageNet Large-Scale Visual Recognition Challenge</li>
</ul></li>
<li>CS231n overview
<ul>
<li>Aims
<ul>
<li>image classification</li>
<li>object detection</li>
<li>action classification</li>
<li>image captioning</li>
</ul></li>
<li>Convolutional Neural Neteworks</li>
<li>object relationships</li>
<li>object attributes</li>
<li>Our philosophy
<ul>
<li>Thorough and Detailed</li>
<li>Practical</li>
<li>State of the art</li>
<li>Fun</li>
</ul></li>
</ul></li>
</ol>
<h1 id="image-classification-pipeline">2. Image Classification
pipeline</h1>
<p>python numpy tutorial</p>
<p>https://github.com/cs231n/cs231n.github.io/blob/master/python-numpy-tutorial.md</p>
<p>gce~tutoria ? google compute engine</p>
<ol type="1">
<li><p>cluster</p></li>
<li><p>attempts have been made</p>
<ul>
<li>find edges</li>
<li>find corners -&gt;, &lt;-, &gt; not scalerable</li>
</ul></li>
<li><p>Neareast Neighbor</p>
<ul>
<li>train: memorize all data and labels O(1)</li>
<li>predict: predict the label of the most similar training image O(N)
&gt; noisy spurious <img data-src="asserts/2-1nn.png" /> &gt; just training
data, 把所有测试数据可能的分类填上去 &gt; k-nearest neighbors <img
src="asserts/2-knn.png" /> &gt; the white place
可能选哪个都可以，随机才</li>
</ul></li>
</ol>
<p>L1: coordinate system is important &gt; you know what the coordinate
system meaning</p>
<p><img data-src="asserts/2-knn-distance_metric.png" /> numpy的基本运算</p>
<p>http://vision.stanford.edu/teaching/cs231n-demos/knn/</p>
<p>why 图像是那样的???? &gt; PRML必读</p>
<ul>
<li>Setting Hyperparameters</li>
</ul>
<p>pick the best training hyperparameters performs on validation? and
run once on the test data, then write in the paper</p>
<p>cross valildation Useful for small datasets, but not used too
frequently in deep learning</p>
<p>有个点自己理解错了，其实是看交叉验证集上的损失怎么样？如果交叉验证集上的损失没有连续操作多少次上升？
或者达到终止条件，则选择该超参数</p>
<p>不过，k-cross-validation的意思是用同样的超参数训练多次，然后得到验证集的平均损失，然后更新一次吗？</p>
<p>因为本身更新公式是作用在损失函数上的</p>
<p>validation to check how well</p>
<p>same methodology for collecting the data, go and partition it
randomly</p>
<p>时间序列分割 shift the problem</p>
<p><img data-src="asserts/2-k-cross-validation.png" /></p>
<p>L2 distance is really not doing a very good job</p>
<p>cover space densely exponential growth traning examples?</p>
<p>knn不适用与图像分类
https://blog.csdn.net/chaipp0607/article/details/77915298</p>
<ul>
<li>很差的测试效率</li>
<li>整个图像的水平距离度量可能非常不直观</li>
</ul>
<blockquote>
<p>计算距离的对象不可能是像素值，因为他不能从本质上反映出人对图像的认知，比如把一张图进行位移，人觉得是一类，但是计算结果距离很大。
所以knn计算的应该是一些描述子生成的特征。</p>
</blockquote>
<p>knn总结 1. 对于样本不平均问题，knn相比于其他监督学习算法容忍度更低。
2. 计算和存储开销大，在线学习效率低。</p>
<p>改进 1. 解决样本不平衡问题 2. 提高分类效率</p>
<p><img data-src="asserts/2-hard-cases-for-alinearclassifier.png" /></p>
<p>mutlimodal data: different regions of space</p>
<p>229</p>
<h1 id="some-concepts">3. some concepts</h1>
<h2 id="loss">3.1 loss</h2>
<ul>
<li>SVM loss: <span class="math inline">\(L_i = \sum_{j \ne y_i} max(s_j
- s_{y_i} +1)\)</span> <span class="math inline">\(L = \frac{1}{N}
\sum_{i=1}^N L_i\)</span></li>
</ul>
<p><span class="math inline">\(max(1-yf(x))\)</span>:
在分类分割线附近进行鼓励 &gt; care the correct is much greater than the
incorrect scores</p>
<p>loss function; how bad are different mistakes square: bad, very, very
bad hinge: a litte wrong, a better wrong</p>
<p>损失函数总结 https://zhuanlan.zhihu.com/p/58883095</p>
<blockquote>
<p>设置W, 来查看Loss</p>
</blockquote>
<p>W, 2W the zero loss</p>
<h2
id="regularization-term-encourages-the-model-to-somehow-pick-a-simple-w-rw">3.2
regularization term: encourages the model to somehow pick a simple W
<span class="math inline">\(R(W)\)</span></h2>
<p><img data-src="asserts/3-regularization.png" /></p>
<p>L1 encouraging sparsity in this matrix W. w1: 0.4, 0, 0.4, 0 w2: 0.2,
0.2, 0.2, 0.2 x:1,1,1,1</p>
<p>L1:w1 L2:w2 &gt;
正则化项是在原本目标得到的函数的基础上，按照你想要的某种特性去构造</p>
<p>minimize -log P(Y=yi| X=xi) multi-SVM: softmax loss &gt; min=zero,
max is infinity</p>
<blockquote>
<p>当出现错误的数据，如何处理？ 0应该处理为一个非常小的数 debuging:
minus log of one over C</p>
</blockquote>
<ul>
<li>hinge-loss vs softmax loss &gt; SVM-loss: margin between different
classes, if the data point over the bar, they don't care about it
anymore if they are correct. &gt; softmax-loss: drive probabiltity mass
all the way to one, pile the all classes, because log loss, it tends to
push correct class to 0, imcorrect classes to infinity. continually to
improve the correct classes.</li>
</ul>
<h2 id="optimization">3.3 Optimization</h2>
<ol type="1">
<li>A first very bad idea solution: Random search</li>
</ol>
<p>cifar-10 15.5%</p>
<ol start="2" type="1">
<li>fllow the slope &gt; scalar 标量</li>
</ol>
<p>如何计算梯度？ - 前向传播，然后计算一个新值，然后得到 dW <img
src="asserts/3-gradient.png" /></p>
<blockquote>
<p>debugging</p>
</blockquote>
<p>https://vision.stanford.edu/teaching/cs231n-demos/linear-classify</p>
<p>calclus 微分</p>
<p><img data-src="asserts/3-gradient-ex.png" /></p>
<p>add gate: gradient distributor max gate: gradient router mul gate:
gradient switcher</p>
<p>back to a node is add</p>
<p><img data-src="asserts/3-Jacobian.png" /> 4096*4096? Jacobian matrix:
diagonal matrix</p>
<p>step 1 <img data-src="asserts/3-gradient-vectorized-ex.png" /></p>
<p>step 2 <img data-src="asserts/3-gradient-vectorized-ex-2.png" /> &gt;
Always check: the gradient with respect to a variable should have the
same shape as the variable.</p>
<p><img data-src="asserts/3-forward_backward-api.png" />
https://github.com/BVLC/caffe/blob/master/src/caffe/layers/clip_layer.cpp
&gt; caffer layers</p>
<h2 id="layers-kinds">3.4 layers kinds</h2>
<p>http://tutorial.caffe.berkeleyvision.org/tutorial/layers.html</p>
<ul>
<li>vision layers
<ul>
<li>convolution</li>
<li>pooling</li>
<li>local response normalization(lrn)</li>
<li>im2col</li>
</ul></li>
<li>loss layers
<ul>
<li>softmax</li>
<li>sum-of-squares/euclidean</li>
<li>hinge/margin</li>
<li>sigmoid cross-entropy</li>
<li>accuracy and top-k</li>
</ul></li>
<li>Activation/Neuron Layers
<ul>
<li>ReLU/Rectified-Linear and Leaky-ReLU</li>
<li>Sigmoid <span class="math inline">\(y = \frac {1}{1 +
e^{-x}}\)</span> &gt;
https://github.com/BVLC/caffe/blob/master/src/caffe/layers/sigmoid_layer.cpp</li>
<li>TanH/Hyperbolic Tangent</li>
<li>Absolute Value</li>
<li>Power</li>
<li>BNLL</li>
</ul></li>
<li>Data Layers
<ul>
<li>In-Memory</li>
<li>HDF5 Input</li>
<li>Images</li>
<li>Windows</li>
<li>Dummy</li>
</ul></li>
<li>Common Layers
<ul>
<li>Innner Product</li>
<li>Splitting</li>
<li>Flattening</li>
<li>Concatenation</li>
<li>Slicing</li>
<li>Elemetwise Oprations</li>
<li>Argmax</li>
<li>Softmax</li>
<li>Mean-Variance Normalization</li>
</ul></li>
</ul>
<p>examples <img data-src="asserts/3-svm.png" /></p>
<p>summary <img data-src="asserts/3-summary.png" /> &gt; upstream gradient
multipy local gradient</p>
<blockquote>
<p>左乘优先 non-linearities</p>
</blockquote>
<p>question: templates, in fact, you don't have to care about it, the
only thing you concerned is the scores</p>
<p><img data-src="asserts/3-neural_networks.png" /></p>
<blockquote>
<p>Assignment2: Writing a 2-layer net</p>
</blockquote>
<h2 id="activation-functions">3.5 Activation functions</h2>
<p><img data-src="asserts/3-activation_functions.png" /></p>
<blockquote>
<p>callled 2-layer neural net</p>
</blockquote>
<p>matrix form, matrix-vector form</p>
<p>Summary - arrange neurons into fully-connected layers - the
abstraction of layer has the nice property that it allows us to use
efficient vectorized code(e.g. matrix multiplies) - neural networks are
not really neural - next time: convolutional neural networks</p>
<h1 id="convolutional-neural-networks">4. Convolutional Neural
Networks</h1>
<p>Deep Learning history - 1957: Perceptron, Frank Rosenblatt - 1960:
Adaline/Madaline &gt; stack these linear layer to multi-layers, no
back-propagation or other rules - 1986: Rumelhart et al. First time
back-propagation became popular - 2006: Reinvigorated research in Deep
Learning &gt; carefully-initialized pre-training stage - 2012, first
strongest results using for speech recognition</p>
<p>A bit of history: CNN - 1959 electrical signal from brain &gt;
topographical mapping 地形学 - Hierarchical organization: &gt; simple
cells -&gt; complex cells -&gt; hypercomplex cells - 1980 complex
cells-&gt;perform pooling - 1998 gradient-base document recognition -
2012, AlexNet - Detection, Segmentation R-CNN reinforcement learning</p>
<p>notation should rorate 180 how to slide this over all the spatial
locations &gt; 卷积层得到的结果是什么<br />
&gt; 3 filter??, 6 filters?? over the image spatially we're sampling the
edges and corners less than the other localtions</p>
<blockquote>
<p>试图去理解每一步的过程还有每一个公式的含义</p>
</blockquote>
<p>one filter-&gt; one activation map</p>
<p>we call these <img data-src="asserts/4-convolutional.png" /></p>
<p><img data-src="asserts/4-output_size.png" /></p>
<p>Output size: (N-f)/stride + 1</p>
<blockquote>
<p>if we use common to zero pad the border, we will get 7*7 output</p>
</blockquote>
<p><img data-src="asserts/4-outputsize-ex.png" /> &gt; what will channels do?
5<em>5</em>3 + 1=76 params(+1 for bias) =&gt; 76*10=760 params</p>
<p><img data-src="asserts/4-outputsize_summary.png" /></p>
<blockquote>
<p>概念辨析 - 向量的乘法 点集，标量积得到内积； 叉乘，结果为向量，向量积
- 矩阵乘法 multiply: 普通的乘法 Hadamard product, 逐点乘法 Kronecker
product: tensor product</p>
</blockquote>
<p>conv layer in torch &gt; spatial convolution ----- torch, pytorch
&gt;
torch能够放在GPU中加速计算，前提是有合适的GPU时，使用起来跟numpy来说差别不大</p>
<ul>
<li><p>torch
https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialConvolution</p></li>
<li><p>caffe
http://caffe.berkeleyvision.org/tutorial/layers/convolution.html</p></li>
</ul>
<hr />
<h1 id="training-neural-networks">5. Training Neural Networks</h1>
<p>Overview: 1. one time setup - activation functions - preprocessing -
weight initilization - regularization - gradient checking 2. Training
dynamics - babysitting the learning process parameter updates -
hyperparameter optimization 3. Evaluation - model ensembles</p>
<h2 id="activation-functions-1">5.1 Activation Functions</h2>
<ol type="1">
<li>sigmoid
<ul>
<li>introduction
<ul>
<li>squashes numbers to range[0,1] &gt; [-5,
5]变化明显，其他位置变化就有问题了 [-10, 10]=0 &gt; df = f(1-f) &gt;
梯度就是函数图像的直线，永远不要忘了最直观的理解</li>
<li>historycally popluar since they have nice interpretation as a
saturating "firing rate" of a neuron</li>
</ul></li>
<li>problem
<ul>
<li>Saturated neurons "kill" the gradients</li>
<li>Sigmoid outputs are not zero-centered &gt;
因为对x的范围的限制，所以不对称，同理也不能在全positive的数据上表现的很好；
always the sign of the upstream gradient coming down. inefficient weight
update</li>
<li>exp() is bit compute expensive</li>
</ul></li>
</ul></li>
</ol>
<p><img data-src="asserts/5-sigmoid.png" /></p>
<p><img data-src="asserts/5-sigmoid-pic.png" /> &gt; avoid allways a
direction</p>
<ol start="2" type="1">
<li>tanh 1991</li>
</ol>
<ul>
<li>Squashes numbers to range[-1,1]</li>
<li>zero centered (nice)</li>
<li>still kills when saturated :( <img data-src="asserts/5-tanh.png" /></li>
</ul>
<ol start="3" type="1">
<li>relu 2012 rectified linear unit</li>
</ol>
<p><img data-src="asserts/5-relu.png" /></p>
<ul>
<li>not zero-center output</li>
<li>any annoyance hint: what is the gradient when x&lt;0 ?</li>
</ul>
<blockquote>
<p>it is fine at the begining, then at some point, it became bad and it
died.</p>
</blockquote>
<p><img data-src="asserts/5-relu-problem.png" /></p>
<blockquote>
<p>data cloud is just your training data. wx1 + wx2上进行影响啊 people
like to initialize relu neurons with slightly positive biaes(e.g.
0.01)</p>
</blockquote>
<ol start="4" type="1">
<li><p>leaky relu <img data-src="asserts/5-prelu.png" /></p></li>
<li><p>elu <img data-src="asserts/5-erelu.png" /> &gt; how the <span
class="math inline">\(\alpha\)</span> is defined</p></li>
<li><p>Maxout Neuron <img data-src="asserts/5-max-neuron.png" /></p></li>
</ol>
<p><img data-src="asserts/5-practice.png" /></p>
<h2 id="data-preprocess">5.2 data preprocess</h2>
<p>original data -&gt; zero-centered data -&gt; normalized data</p>
<blockquote>
<p>to do at the test data, too <img
src="asserts/5-2_preprocess_data-1.png" /></p>
</blockquote>
<p><img data-src="asserts/5-2_pcaAndWhitening.png" /></p>
<p><img data-src="asserts/5-2_preprocess_data-practice.png" /></p>
<blockquote>
<p>per-channel RGB do this for entire train data, once before training.
don't do this per batch</p>
</blockquote>
<blockquote>
<p>how sigmoid need the zero-centered data, it only servers for the
first layer.</p>
</blockquote>
<h2 id="weight-intilization">5.3 Weight Intilization</h2>
<p>Q: what will happen if W is zero? &gt; 1. will disappear &gt; !!they
will all do the same thing</p>
<ul>
<li>First idea: small random numbers (gaussian with zero mean and 1e-2
standard deviation) <code>W = 0.01 * np.random.randn(D, H)</code> &gt;
works okay for small networks, but problems with deeper networks.</li>
</ul>
<p>pic1 see the hidden data is pic2: see the data distribution &gt;
zero</p>
<blockquote>
<p>All activations become zero!!</p>
</blockquote>
<p>What will W look like?</p>
<p>because X is small, so the W is getting more small, they're basically
not updating.</p>
<ul>
<li>forward doing</li>
<li>have gradient flows coming down &gt; for each line, we chain these
weight</li>
</ul>
<p>cs231n: 深度视觉识别课程P6</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/algorithm-acwing-search-graph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/algorithm-acwing-search-graph/" class="post-title-link" itemprop="url">algorithm acwing search graph</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:17:32" itemprop="dateCreated datePublished" datetime="2019-09-19T12:17:32+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">algorithm</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/acwing/" itemprop="url" rel="index"><span itemprop="name">acwing</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/acwing/basic/" itemprop="url" rel="index"><span itemprop="name">basic</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithm/acwing/basic/search-graph/" itemprop="url" rel="index"><span itemprop="name">search_graph</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.1k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="bfsdfs">1. BFS/DFS</h1>
<h2 id="dfs">1. DFS</h2>
<p>stack O(h) 不具有最短性 回溯，剪枝 思路比较奇怪的都有DFS来做
俗称暴力搜索！ 关键：使用什么顺序来进行搜索 &gt;
一个很执着的人！，回溯法，出去，回来务必注意需要恢复现场
有些题会判断最优，有些题不需要 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int N=10;</span><br><span class="line">int n;</span><br><span class="line">int path[N];</span><br><span class="line">bool st[N];</span><br><span class="line"></span><br><span class="line">void dfs(int u)&#123; // 这里与path对应</span><br><span class="line">    if(u==n)&#123;</span><br><span class="line">        for(int i=0;i&lt;n;i++) printf(&quot;%d &quot;,path[i]);</span><br><span class="line">        puts(&quot;&quot;);</span><br><span class="line">        return;</span><br><span class="line">    &#125;</span><br><span class="line">    for(int i=1;i&lt;=n;i++)&#123; // 这里与st对应</span><br><span class="line">        if(!st[i])&#123;</span><br><span class="line">            path[u]=i;</span><br><span class="line">            st[i]=true;</span><br><span class="line">            dfs(u+1);</span><br><span class="line">            st[i]=false;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">int main()&#123;</span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    dfs(0);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure> 八皇后法：依旧使用框架 n*n!
2<sup>(n</sup>2)</p>
<p>DP是无环的最短路问题
深搜可以保证一次搜索到终点，但是不能保证搜索到的路径是最短的
深度搜索可能随机搜索到一条路</p>
<h3 id="统一考虑的格式">1.1 统一考虑的格式</h3>
<p>数据范围 N 最优解： 需要输出的东西 约束条件</p>
<h3 id="全排列问题">1.1 全排列问题</h3>
<p>DFS, 全排列问题 顺序！！ <em>, </em>, <em>, </em>
要以什么样的顺序来遍历所有的方案，这里很简单地就是每一位到底选什么的方案
DFS就是递归 &gt;
当初始状态不属于什么具体的含义的时候，往往恢复现场写在，展开每个子树那里</p>
<p>N=20 最优解： path[N] 约束条件: st[N]</p>
<h3 id="n皇后问题">1.2 n皇后问题</h3>
<p>N=20 最优解： g[N][N] 棋盘 约束条件: col[N], dg[N*2] udg[N*2]</p>
<h2 id="bfs">BFS</h2>
<p>queue O(2^h) 最短性，当所有边的权都为0,1时是。</p>
<p>迷宫问题： 保证了一定存在路 技巧就在于少写循环判断语句</p>
<p>如何输出路径？就是在该输出的位置增加链表</p>
<p>八数码问题： BFS求最短路
把每个方格看作个体，当前棋盘的所有的方格组合而成为整个棋盘的状态，然后每个状态之间有路可走，研究的点就是如何进行搜索，使得能够从一个状态到达另一个状态，并且进行搜索！</p>
<p>所以问题的关键： 1. 状态表示复杂 2. 如何记录两个状态之间的距离</p>
<p>求最短路</p>
<p>最短路简单的做法： q[N],d[N]</p>
<blockquote>
<p>d[N]是什么，d[N]是BFS中用于搜索，用来更新距离计算公式的</p>
</blockquote>
<p>那么需要考虑 - 如何存储 - 如何定义最短距离 距离数组的小标如何表示
queue<string> dict: 字典，哈希表unordered_map&lt;string,int&gt;</p>
<p>queue<string> 如何从一个字符串到达另一个字符串？ 移动，恢复 # 2.
树和图 ## 2.1 存储方式 邻接矩阵: 稠密图
邻接表：拉链法，存储每个点可以走到哪个点 &gt;
可以用vector来做，但是就效率而言没有数组模拟的快， 稀疏图</p>
<h2 id="遍历方式">2.2 遍历方式</h2>
<p>O(n+m)</p>
<p>注意最大值最小 最小值最大两者的区别</p>
<blockquote>
<p>在图里，进行遍历，如果有环，怎么进行统计数？？？，想想真是一个问题！！！
在图中，因为有n个顶点，所以对于n个顶点，每两个边之间最多只可能有两种情况，所以根据握手定理，出度与入度之和就为顶点数的二倍</p>
</blockquote>
<p>深度优先和宽度优先</p>
<p>如何将一维扩展到二维，以及多维，实际上就是在多维空间中寻找一种顺序，使得这种顺序可以通过一维的方式来进行模拟</p>
<p>BFS: h[N],e[M],ne[M],idx; d[N],q[N]; //
d[N]==-1,表示是否进行了遍历</p>
<p>应用：有向图的拓扑序列。 拓扑序列的两种求的方法 拓扑排序的算法流程
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">queue&lt;- 所有入度为0的点</span><br><span class="line">while(queue不空)</span><br><span class="line">    t&lt;-队头</span><br><span class="line">    枚举t的所有出边t-&gt;j:</span><br><span class="line">        删除t-&gt;j, d[j]--;</span><br><span class="line">        if(d[j]==0) queue&lt;-j;</span><br></pre></td></tr></table></figure></p>
<p>DFS: h[N],e[M],ne[M],idx st[N]</p>
<p>搜索的扩展：人工智能技术的树</p>
<h1 id="最短路问题">3. 最短路问题</h1>
<ol type="1">
<li>单源最短路</li>
</ol>
<ul>
<li>所有边权都是正数
<ul>
<li>朴素Dijkstra O(n^2) 稠密 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for v: 1-n:</span><br><span class="line">    t&lt;- 不在S中的距离最近的点 n-i  st[i]:n</span><br><span class="line">    s&lt;-t, 共n次</span><br><span class="line">    用t来更新其他顶点 t-&gt;v， 共m次 // </span><br></pre></td></tr></table></figure></li>
<li>堆优化版Dijstra O(mlogn) 稀疏 &gt; 看哪些地方可以优化 &gt;
手写堆，可以保证n个元素 &gt; pq, 不支持修改元素， m个元素</li>
</ul></li>
<li>存在负权边
<ul>
<li>Bellman-ford O(nm) <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">struct&#123;</span><br><span class="line">    int a,b,w;</span><br><span class="line">&#125; edge[M];</span><br></pre></td></tr></table></figure> &gt; (a, b): w &gt; dist[b] =
min(dist[b], dist[a] + w); 松弛操作 &gt; 对所有的边都满足 dist[b]&lt;=
dist[a]+w 三角不等式</li>
</ul>
<blockquote>
<p>如果有负权回路不一定存在。n+1 条边，存在负环 O(nm)
特解的题：最多k条边的题
当存在负环时，不存在最短路径。如果负环为自环则无所谓
因为更新会发生串联，所以需要备份，backup[N]. &lt;1,2&gt;:1,
&lt;2,3&gt;:1, &lt;1.3&gt;: 3
只要有负环存在，可以不断更新负环中的数值，最终到达负无穷</p>
</blockquote>
<ul>
<li>SPFA 一般：O(m) 最坏：O(nm) &gt; bellman_ford 不一定 &gt;
这里利用了宽搜， BFS, 只有前面的点变小了，后面的点才有可能变小 &gt;
网格特别容易卡spfa</li>
</ul>
<blockquote>
<blockquote>
<p>应用： 1. 最短路 2. 判断负权回路（指回路的总长度为负数）
在2时，因为2本身一个环里的权会不断减小，所以没有必要赋初值</p>
</blockquote>
</blockquote></li>
</ul>
<ol start="2" type="1">
<li>多源最短路- Floyd O(n^3) &gt; 基于动态规划，三维 &gt; d[k,i,j]:
只经过k节点，从i到达j的路径 &gt; d[k,i,j] = min{d[k-1,i,j],
d[k-1,i,k]+d[k-1,k,j]}
https://www.cnblogs.com/chenying99/p/3932877.html</li>
</ol>
<p>稠密图：m~n^2 稀疏图：m~n &gt;
考虑无重边和自环的话，那么最多的边则就是完全图，即n^2; &gt;
考查的点不会是正确性，从背景中抽象为最短路问题</p>
<blockquote>
<p>无向图是一种特殊的有向图</p>
</blockquote>
<h1 id="最小生成树">4. 最小生成树</h1>
<p>Prim算法 - 朴素版 稠密图 O(n^2)</p>
<p>Kruskal算法 O(mlogm) 稀疏图 1. 对边进行排序O(mlogm) 2.
从小到大选取边，若该边与生成树形成环则不加入，否则加入；
直到加入了n-1条边即结束 &gt;
和bellman-ford一样只需要用结构体存储边即可</p>
<blockquote>
<p>选择，稠密图用Prim算法；稀疏图用Kruskal算法</p>
</blockquote>
<h1 id="二分图">5. 二分图</h1>
<ul>
<li><p>染色法 O(n+m) &gt; 对图中的点染色，深度和宽度优先均可。 &gt;
如果一个点出现两个颜色，则说明该图不是二分图</p></li>
<li><p>匈牙利算法 O(nm) &gt; 实际运行时间一般远小于O(nm) &gt;
只会找左边所有点所对应的边 &gt; st[N],
存储当前使用匹配时是否用到该点；在寻找下一轮时，需要更新为0</p></li>
</ul>
<p>复杂图论的题 &gt; 二分图： 棋盘覆盖 ## Error Seg F:
纯随机，然后利用minmax来求解 &gt;
当输入输出规模到达100万时一般才开始考虑用scanf(),而不用cin</p>
<p>检查：</p>
<p>初始化，代码逻辑，赋值等是否出错。
最需要注意的一件事就是尽量用空格来划分代码的逻辑</p>
<p>有问题怎么办？首先确信一部分代码一定是对的，不要怀疑</p>
<p>函数中的变量务必注意要初始化，否则是会出错的！！！</p>
<p>前面的题难点在思路上 图论难点在代码实现上</p>
<p>为之半个小时的调试，发现是一个变量给写错了，唉，就在for循环那里</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/09/19/algorithm-acwing-basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/09/19/algorithm-acwing-basic/" class="post-title-link" itemprop="url">基础算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 12:15:53" itemprop="dateCreated datePublished" datetime="2019-09-19T12:15:53+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>[toc]</p>
<h3 id="十大排序算法">十大排序算法</h3>
<h4 id="算法分类">算法分类</h4>
<p>十种常见排序算法可以分为两大类： 1.
比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn),
所以又称为非线性时间比较类排序 2.
非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，所以称为线性时间非比较类排序。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">void quick_sort(int q[], int l, int r)&#123;</span><br><span class="line">    if(l&gt;=r) return;  // 注意边界条件不可少</span><br><span class="line"></span><br><span class="line">    int x=q[l], i=l-1, j=r+1; // 或q[(1+r)/2], q[r]</span><br><span class="line">    // 这里赋初值是为了后面对称的情况</span><br><span class="line">    while(i&lt;j) &#123;</span><br><span class="line">        do i++; while(q[i]&lt;x);</span><br><span class="line">        do j--; while(q[i]&gt;x);</span><br><span class="line">        if(i&lt;j) swap(q[i],q[j]); // 注意这里要加一个判断</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    quick_sort(q, l, j);  // </span><br><span class="line">    quick_sort(q, j+1, r);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="归并">2. 归并</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">const int N=1e6+10;</span><br><span class="line">int n;</span><br><span class="line">int q[N], tmp[N];</span><br><span class="line"></span><br><span class="line">void merge_sort(int q[],int l, int r)&#123;</span><br><span class="line">    if(l&gt;=r) return;</span><br><span class="line">    int mid=l+r&gt;&gt;1;</span><br><span class="line">    merge(q, l, mid);</span><br><span class="line">    merge(q, mid+1,r);</span><br><span class="line"></span><br><span class="line">    int k=0, i=l, j=mid+1;</span><br><span class="line">    while(i&lt;=mid&amp;&amp;j&lt;=r)&#123;</span><br><span class="line">        if(q[i]&lt;q[j]) tmp[k++]=q[i++];</span><br><span class="line">        else tmp[k++]=q[j++];</span><br><span class="line">    &#125;</span><br><span class="line">    while(i&lt;=mid) tmp[k++]=q[i++];</span><br><span class="line">    while(j&lt;=r) tmp[k++]=q[j++];</span><br><span class="line"></span><br><span class="line">    for(i=l,j=0;i&lt;=r;i++,j++) q[i]=tmp[j];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    scanf(&quot;%d&quot;,&amp;n);</span><br><span class="line">    for(int i=0;i&lt;n;i++) scanf(&quot;%d&quot;,&amp;q[i]);</span><br><span class="line">    merge_sort(q,0,n-1);</span><br><span class="line">    for(int i=0;i&lt;n;i++) printf(&quot;%d&quot;, q[i]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="二分">3. 二分</h3>
<h4 id="整数二分">整数二分</h4>
<p>// 整数二分，有单调性一定可以二分；但是没有单调性也可以二分 //
在区间上定义上某种性质，将区间分为两部分，左边满足某种性质，右边不满足某种性质；二分法可以用来寻找不满足性质和满足性质的边界
？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">int bsearch_1(int l,int r)&#123;</span><br><span class="line">    while(l&lt;r)&#123;</span><br><span class="line">        int mid=l+r&gt;&gt;1; // 下取整</span><br><span class="line">        if(check(mid)) r=mid; // [l,mid]</span><br><span class="line">        else l=mid+1;</span><br><span class="line">    &#125;</span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int bsearch_2(int l, int r)&#123;</span><br><span class="line">    while(l&lt;r)&#123;</span><br><span class="line">        int mid=l+r+1&gt;&gt;1;</span><br><span class="line">        if(check(mid)) l=mid;</span><br><span class="line">        else r=mid-1;</span><br><span class="line">    &#125;</span><br><span class="line">    return l;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">如何判断死循环</span><br><span class="line">mid=l=r时，是否陷入死循环</span><br></pre></td></tr></table></figure>
<p>整数二分的问题一定可以用这个模板来解决。
每次二分，能找到一个答案的区间；当区间的长度是1时，即是解</p>
<p>二分能够包含解，然后用边界可以推出解。 &gt;
二分的退出条件一定是i=j吗？，这里知道快排的退出条件并不是l=r
对！！！！，二分一定能找到解，退出是i一定等于j</p>
<blockquote>
<p>性质的寻找， 如果a[mid] &lt; x,
怎么办？实际上就是看目标的数在a[mid]与x之间，比如靠近x最大的数，
此时l=mid; 如果是找x相等的左边界，在a[mid] 在a[mid] &gt; x, 比如 ???
有点模糊</p>
</blockquote>
<h4 id="浮点数二分simple">浮点数二分simple</h4>
<blockquote>
<p>这里就不用+1，-1 求平方根</p>
</blockquote>
<blockquote>
<p>这里务必注意左右范围的选取问题。</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;iostream&gt;</span><br><span class="line">using namespace std;</span><br><span class="line">int main()&#123;</span><br><span class="line">    double x;</span><br><span class="line">    cin&gt;&gt;x;</span><br><span class="line"></span><br><span class="line">    double l=0, r=x; // r=max(1,x);</span><br><span class="line">    while(r-l&gt;1e-8)&#123;  // 这里比要求的有效数字多2</span><br><span class="line">    // 这里用迭代次数来停止也可以，比如100，即除以2^100.</span><br><span class="line">        double mid=(1+r)/2;</span><br><span class="line">        if(mid*mid &gt;= x) r=mid;</span><br><span class="line">        else l=mid;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    printf(&quot;%lf\n&quot;,l); </span><br><span class="line">    // 这里l或者r</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="高精度">4. 高精度</h3>
<p>A+B： len(A)=1e6 A-B: A*a 1e6 a-10000 A/a</p>
<p>存储，倒序存储 在末位加上一位，下标为0的存个位。 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">A+B</span><br><span class="line">int a[N],b[N],sum[N];</span><br><span class="line">int cnt=0;</span><br><span class="line">for(int i=0,j=0;i&lt;n&amp;&amp;j&lt;m;i++,j++)&#123;</span><br><span class="line">    sum[i]=(a[i]+b[j]+cnt)%10;</span><br><span class="line">    cnt=(a[i]+b[j]+cnt)&gt;10?1:0;</span><br><span class="line">&#125;</span><br><span class="line">if(cnt) a[n+1]=1;</span><br><span class="line"></span><br><span class="line">A-B</span><br><span class="line">len(A)&gt;=len(B)</span><br></pre></td></tr></table></figure></p>
<p>如果A，B可能取正或负，那么一定可以转化为绝对值进行相加或者相减！</p>
<p>高精度减法 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">t=(t + A[i])/b;</span><br><span class="line">c.push_back(t%10);  //?</span><br><span class="line">t=A[i]%b;</span><br><span class="line"></span><br><span class="line">for (int i=A.size()-1;i&gt;=0;i--)</span><br><span class="line">t=10t+A[i];</span><br><span class="line">if((t/b)) c.push_back(t/b);  </span><br><span class="line">t %= b;</span><br><span class="line">t</span><br></pre></td></tr></table></figure></p>
<h3 id="前缀和和差分">5. 前缀和和差分</h3>
<ul>
<li>前缀和 a1, a2, a3,..., an S0, S1, S2,..., Sn a_{i}= S_{i}- S_{i-1}
S0=0; 当S0=0时，特别好定义边界。 前缀和 Si = Si-S0.</li>
</ul>
<p>因此，可以很容易地求片段和。 &gt;
很厉害！可以统一了，不用预处理了</p>
<blockquote>
<p>scanf比cin快一倍 ios::sync_with_stdio(false);
cin和标准输入输出不同步，提高cin读取速度；不能使用scanf</p>
</blockquote>
<p>区间和 区域和 (x1,y1), (x2,y2) Sx2y2-Sx1y2-Sx1y1+Sx1y1</p>
<ul>
<li>差分 前缀和的逆运算 &gt; 作用：可以在O(n)时间内B-&gt;A 区间[1,r]+c
&gt; b[l]+c, b[r+1]-c</li>
</ul>
<h3 id="双指针算法">6. 双指针算法</h3>
<p>两类： 一个数组两个指针；两个数组同时处理。 代码框架
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for(i=0,j=0;i&lt;n;i++)&#123;</span><br><span class="line">    while(j&lt;i &amp;&amp; check(i,j)) j++;</span><br><span class="line">    res=max(res, j-i+1);</span><br><span class="line">    // 每道题的具体逻辑</span><br><span class="line">&#125;</span><br><span class="line">O(n+m) </span><br><span class="line">这里i和j具有单调性，i和j只向一个方向发展。</span><br></pre></td></tr></table></figure></p>
<p>核心思想 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for(int i=0;i&lt;n;i++)</span><br><span class="line">    for(int j=0;j&lt;n;j++)</span><br><span class="line">        O(n^2)</span><br></pre></td></tr></table></figure> 将上面朴素算法优化到O(n), 运用了某种性质</p>
<p>维护两个窗口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">ad daa</span><br><span class="line">ad</span><br><span class="line">daa</span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;string.h&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int main()&#123;</span><br><span class="line">    char str[100];</span><br><span class="line">    gets(str);</span><br><span class="line">    int n=strlen(str);</span><br><span class="line">    </span><br><span class="line">    for(int i=0;i&lt;n;i++)&#123;</span><br><span class="line">        int j=i;</span><br><span class="line">        while(j&lt;n &amp;&amp; str[j]!=&#x27; &#x27;) j++; // i,j-1具体单词</span><br><span class="line">        for(int k=i;k&lt;j;k++)&#123;</span><br><span class="line">            cout&lt;&lt;str[k];</span><br><span class="line">        &#125;</span><br><span class="line">        cout&lt;&lt;endl;</span><br><span class="line">        i=j;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// 枚举起点和终点</span><br><span class="line">// 每个指针的位置都有意义</span><br><span class="line"> for(int i=0;i&lt;n;i++)</span><br><span class="line">    for(int j=0;j&lt;n;j++)</span><br><span class="line">        if(check(i,j))&#123;</span><br><span class="line">            res=max(res,i-j+1);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">// 双指针算法 O(n)</span><br><span class="line">for(i=0,j=0;i&lt;n;i++)&#123;</span><br><span class="line">    while(j&lt;i &amp;&amp; check(i,j)) j++;</span><br><span class="line">    res=max(res, j-i+1);</span><br><span class="line">    // 每道题的具体逻辑</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>特殊情况的举例</p>
<blockquote>
<p>数组元素的目标和为什么限制为一个解，因为如果是多个解时。对于 <code>1
1 1 1 1, 1 1 1 1 , 2</code>本身就要O(n*m),算法必然为O(n^2)的。 ???? ###
7. 位运算</p>
</blockquote>
<p>从个位开始算,(11111)_2 &gt; 先把第k位移到最后一位 &gt; n&gt;&gt;k
&amp; 1 看一下n的第k位是几？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for(int k=3;k&gt;=0;k--)</span><br><span class="line">    cout&lt;&lt;(n&gt;&gt;k&amp;1)&lt;&lt;endl;</span><br></pre></td></tr></table></figure>
<p>lowbit(x): 返回x的最后一位1 x&amp;(~x+1)= x&amp;-x</p>
<h3 id="离散化整数的离散化">8. 离散化，整数的离散化</h3>
<p>10^9 稀疏的值域，映射到从0开始的自然数 10^5
注意这里映射的话的思想实际上就是按照个数索引来映射的感觉。</p>
<p>a-&gt;b &gt; 1. a[]可能中有重复元素： 去重 &gt; 2.
如何算出a[i]离散化后的值：a是有序的，二分</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vector&lt;int&gt; alls;</span><br><span class="line">sort(alls.begin(), alls.end());</span><br><span class="line">alls.erase(unique(alls.begin(), alls.end()),alls.end()) // 去重</span><br><span class="line">&gt; ?这里unique返回的就是改变后的alls, 且alls后面的重复的元素都放在最后部分</span><br><span class="line"></span><br><span class="line">// 二分求出x对应的离散</span><br><span class="line">int find(int x)&#123; // 找第一个大于等于x的位置</span><br><span class="line">    int l=0, r=all.size()-1;</span><br><span class="line">    while(l&lt;r)&#123;</span><br><span class="line">        int mid=l+r&gt;&gt;1;</span><br><span class="line">        if(x&lt;=all[mid]) r=mid;</span><br><span class="line">        else l=mid+1;</span><br><span class="line">    &#125;</span><br><span class="line">    return r+1; // 这里是否加1都无所谓，这里从1开始映射；1,2,3,4,..,n</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意，一般的离散化的目标就是所有会用到的坐标 区间和 n+2m(l,r)</p>
<h3 id="合并区间">9. 合并区间</h3>
<ol type="1">
<li>按区间做变短排序</li>
<li>总共有三种情况 <code>ri&lt;ri+1, ri+1&lt;ri, ri&lt;li+1</code>
一般是贪心，优先对左右端点进行排序</li>
</ol>
<h3 id="其他">其他</h3>
<p>java输入输出比较慢</p>
<p>编译语言和即时编译 小数据 javascript,python快 大数据 go,c,c++快
javascript&gt;python&gt;go=C++&gt;java</p>
<h4 id="错误类型">错误类型</h4>
<ul>
<li><p>Segmentation Fault 基本情况是否没有考虑 数组是否越界
没有输入</p></li>
<li><p>Time Limit Exceeded<br />
输入输出可能都存在着问题</p></li>
</ul>
<p>关键：把自己想的算法能够实现出来</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yun-Pan Wang"
      src="/images/default-avatar.png">
  <p class="site-author-name" itemprop="name">Yun-Pan Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AugF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AugF" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangyp@smail.nju.edu.cn" title="E-Mail → mailto:wangyp@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/Joswxe" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;Joswxe" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://cn.bing.com/" title="https:&#x2F;&#x2F;cn.bing.com" rel="noopener" target="_blank">Bing</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.baidu.com/" title="https:&#x2F;&#x2F;www.baidu.com" rel="noopener" target="_blank">Baidu</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">[object Object]</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">466k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:04</span>
</div>!

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://augf.github.io/page/5/',]
      });
      });
  </script>



</body>
</html>
