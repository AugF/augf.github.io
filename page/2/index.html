<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"augf.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Life&#39;s Notes">
<meta property="og:url" content="https://augf.github.io/page/2/index.html">
<meta property="og:site_name" content="Life&#39;s Notes">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yun-Pan Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://augf.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Life's Notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Life's Notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">岁月数载，愿不负韶华</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/12/08/The-Minto-Pyramid-Princlple/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/08/The-Minto-Pyramid-Princlple/" class="post-title-link" itemprop="url">The Minto Pyramid Princlple</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-08 18:22:50" itemprop="dateCreated datePublished" datetime="2019-12-08T18:22:50+08:00">2019-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>537</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>金字塔结构</p>
<p>金字塔结构是什么？</p>
<p>金字塔有什么用？ 为什么？</p>
<p>想想一种场景，让我们记住一组东西。即输入，我们要如何记住呢？
人类的直觉就是将这些内容进行归纳分组。 &gt;
一次记忆一般超不过7个思想，4,5个即可</p>
<p>然后输出的时候，分层可以让人的记忆内容变少。</p>
<p>比如一组东西。 这里有3类物品。</p>
<p>自上而下，先传达结论。 任何其他顺序都可能造成无解。</p>
<p>人大脑的本质，没在接受一个东西时。如果不理解，就会有疑问，比如跟前面有什么联系。
&gt;
人的思维能力都是有限的，一部分思维能力用于识别和解毒读到的词语，一部分用于找出思想之间的关系，剩下的则要用于理解文中所表述的思想的含义。</p>
<p>一般来说，调查的问题是零散的，所以需要我们自下而上地思考，总结概括。
先将词分组，再抽取组与组之间的共性，从而抽取出主旨思想，单一思想。</p>
<p>金字塔结构有三种关系： 1.
纵向：文章中任一层次的思想必须是其下一层次思想的概括 2.
横向：每组的思想必须属于同一逻辑范畴 3.
横向：每组中的思想必须按照逻辑顺序组织。 - 演绎归纳 - 因果关系（时间） -
评论（空间） - 类别（程度）</p>
<p>金字塔每层的含义 1. 主题 2. 关键句 3. 较具体的思想（新的思想）</p>
<p>学习三个点： 1. 学什么 2. 知其然 3. 用其法</p>
<p>算法题的步骤： 1. 要求 2. 设计原则 3. 模型 4. 具体设计 5.
关于设计的验证</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/12/06/paper-read-graphsage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/06/paper-read-graphsage/" class="post-title-link" itemprop="url">paper-read-graphsage</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-06 15:22:24" itemprop="dateCreated datePublished" datetime="2019-12-06T15:22:24+08:00">2019-12-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn/" itemprop="url" rel="index"><span itemprop="name">gnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Overview:</p>
<p>stochastic generalization of graph convolutions, and it is useful for
massive, dynamic graphs that contain rich feature information.</p>
<p>看自身的PPT, 实际上关于k的提法，从1,2,...,k其他是对于每个节点。
比如说图上的每个节点，先分别做一跳的表示。
不断地一起共同的做，最终可以发现实际上所求的目标就最终表示是k跳来决定的。</p>
<p>关键表达式其实就只有那一个。</p>
<p>为什么inductive?
实际上感觉是在图上把问题给固定下来了，也就是说实际上，来一个性的点之后，直接按照之前的所有的函数和模式，学习新的表示即可。
&gt; 此时的表示是基于之前训练得到的新的表示? 还是旧的表示? &gt;
之前的学习是学习对应的函数和参数，所以说?</p>
<blockquote>
<p>那么对于未知节点的表示，实际上又是如何查找和运行的呢?</p>
</blockquote>
<p>hierarchical 层次</p>
<p>hierarchical pool: 层次遍历。 &gt; 如果将gnn适用于对整个图分类。</p>
<p>Characterize GNN's discriminative power</p>
<h2 id="paper">paper</h2>
<h3 id="dataset">dataset</h3>
<h4 id="ciatation-data">Ciatation data</h4>
<p>predicting paper subject categories. six biology-related fields for
2000-2005. node lable: six</p>
<p>n = 302421, fix_size = 9.15 undirected citation graph dataste.</p>
<p>train: 2000-2004 2005 test. 30% for validation</p>
<p>features: node degrees and processed the paper abstracts
300-dimensional word vectors.</p>
<h4 id="reddit-data">Reddit data</h4>
<p>which community different Reddit posts belong to.</p>
<p>users post and comment on content in different topical
communities.</p>
<p>node label: the community.</p>
<p>sampled 50 large communities. post-post graph.</p>
<blockquote>
<p>in the month of September, 2014.</p>
</blockquote>
<p>n=232965, fix_size=492 first 20 days for training and remaining for
testing (0.3 for validation)</p>
<p>features: 300-dimensional GloVe CommonCrawl word vectors.</p>
<p>each post concatenated 1. the average embedding of the post title. 2.
the average embedding of all the post' comments 3. the post's score. 4.
the number of comments made on th post</p>
<h4 id="protein-protein-interactions">Protein-protein interactions</h4>
<p>the task of generalizing across graphs. requires learning about node
roles rather than community structure.</p>
<p>protein roles, in terms of their cellular functions from gene
ontology. 我们未来了解节点的角色，即蛋白质的作用是什么, protein.
使用基因本体论为它们做标记。</p>
<p>节点与节点之间交互会构成人体的不同组织</p>
<p>various protein-protein interaction graphs.</p>
<p>node: protein graph: human tissue.</p>
<p>features: positional gene sets, motif gene sets and immuological
signatures labels: gene ontology sets</p>
<p>n=2373, fix_size = 28.8</p>
<h3 id="theoretical-analysis">5. Theoretical analysis</h3>
<p>为了探究GraphSage如何学习图结构，尽管它本质上可能是基于功能。
作为案例研究，我们考虑了graphsage是否可以学习预测节点的聚类稀疏，即节点1跳领域内闭合的三角形比例。
&gt; 反映在图上是连在一起的意思吗？
聚类系数是衡量节点本地淋雨聚类程度的一种流行度量，它是许多更复杂结构图案的基础。</p>
<p>我们可以证明算法1能够将聚类稀疏近似为任意精度</p>
<p>Theorem 1?: 公式表达</p>
<p>对于每个图，都有一个算法1的参数设置，这样，如果每个节点的特征都不同，则它可以将改图的聚类系数近似为人影精度。</p>
<p>即使从绝对连续的随机分布中采样节点特征输入，GraphSage也可以了解局部图的结构。</p>
<p>证明背后的基本思想是，
如果每个节点都有唯一的特征表示，则我们可以学习将节点映射到指标向量并识别节点领域。
定理1的证明依赖于池聚合器的某些属性，所以更优 ### Appendices</p>
<h4 id="algorithm-2">Algorithm 2</h4>
<p>首先第一步预处理， K轮： Bk=B B(k-1) = Bk 加上 u的点的邻居 B(k-2) =
B(k-1) 加上所有点的邻居 &gt; ? 反着来违反直觉，然后k=2是，即采样了S2,
又采样了S1*S2 2-hop neighbors. &gt; 解释说这里其实是采样过程而已</p>
<p>12-13 解释说运算肯定是只包括当前所需要的运算</p>
<h4 id="dataset-1">Dataset</h4>
<h4 id="deatails">Deatails</h4>
<ol type="1">
<li>Hyperparameter selection Random Walk: 50 random walks of length 5
from each node in order to obtain the pairs needed for the unsupervised
loss. Python, Perozzi[28]</li>
</ol>
<p>Logistic regression model SGDCClassifier from the scikit-learn Python
package[26], with default settings.</p>
<p>Hyperparameter selection DeepWalk: 0.01, 0.001, 0.0001 initial
learning rates. 2<em>1e-6, 2</em>1e-7, 2*1e-8 unsupervised model</p>
<p>big 1024, small 512</p>
<p>All models user rectified linear units All the unsupervised GraphSAGE
models and DeepWalk used 20 negative sample with context distributaion
smoothing over node degress using a smoothing parameter of 0.75.</p>
<p>rates: 0.2, 0.4, 0.8</p>
<p>batch sizes 512, batch size 64</p>
<ol start="2" type="1">
<li>Hardware 4 NVIDIA Titan X Pascal GPUS (12Gb of RAM at 10Gbps speed),
16 Intel Xeon CPUS(E5-2623 v4 @ 2.60GHz)</li>
</ol>
<p>3 days in a shred resource setting. &gt; 查一下这些硬件</p>
<p>Titan X GPU: 4-7 days full resources were dedicated</p>
<ol start="3" type="1">
<li><p>Notes on the DeepWalk implementation DeepWalk is also equivalent
to the node2vec model with p=q=1</p></li>
<li><p>Notes on neighborhood sampling: subsample edges so that no nodel
has degree large than 128. we only sample at most 25 neighbors per node.
downsampling allows us to store neighborhood information as dense
adjacency list, improves computational efficiency.</p></li>
</ol>
<p>Reddit data, downsampled the edges of the original graph as a
pre-processing step, since the original graph is extremely dense. ##
code</p>
<h3 id="tensorflow">tensorflow</h3>
<p>stochastic generalization of graph convolutions.</p>
<p>GraphSage now has better support on smaller, static gaphs, don't have
node features. 'identity features"</p>
<p>task of inductive generalization(generating embeddings for nodes that
were not present during training)</p>
<p>identity features: increase the runtime. potentially increase
performance(at the usual risk of overfitting)</p>
<p>GraphSage is intended for use on large graphs &gt;100000 nodes.</p>
<p>example_data: ppl</p>
<ol type="1">
<li>running the code if your benchmark does not require generalizing to
unseen data, --identity_dim ?flag to a value int ht range [64, 256] &gt;
目标是不需要inductive? 没错吧，那么不应该是也可以用到自身特征吗？ make
the model embed unique node ids as attributes. increase the runtime and
number of parameters , performance. &gt; node ids,
节点标号作为属性，什么鬼？<br />
set this flag and not try to pass dense one-hot vectors as featres(dut
to sparsity) &gt; 单独抽出来可以理解 the "dimension" of identity
features specifies how many parameter there are per node in the sparse
identity-feature lookup table. &gt;
这个维度是怎么展现的？不精确的？</li>
</ol>
<p>example_unsupervised.sh</p>
<p>set a samll max iteration， very near convergence ?</p>
<p>example_supervised.sh</p>
<p>for PPI data &gt; mulit-output?
除了节点编号，还要输出什么？懂了！，这里的意思是individual nodes to
belong to multiple classes. --sigmoid, defalut: one-hot</p>
<ol start="2" type="1">
<li>Input format --train_prefix: specifies the following data files.
X-G.json: a networkx-specified file decribing the input graph. Nodes
have 'val' and 'test' 表示是交叉验证集和测试集的一部分 X-id_map.json:
graph_node-ids映射到连续的整数 X-class_map.json: graph node ids map to
classes X-feat.npy: a numpy-stored array of node features. ordering
given by id 应该是对应的整数 X-walks.txt: a text file random walk
co-occurrences (one pair per line) &gt; 共同出现的节点标号</li>
</ol>
<p>run random walks. graphsage.utils run_walks</p>
<ol start="3" type="1">
<li>Model variants
<ul>
<li>graphsage_mean</li>
<li>graphsage_seq</li>
<li>graphsage_maxpol</li>
<li>graphsage_meanpool</li>
<li>gcn</li>
<li>n2v_an implementation of DeepWalk</li>
</ul></li>
<li>logging directory
<ul>
<li>--base_log_dir: default to the current directory;
<sunp/sunsup>--<data_prefix>/graphsage-<model_description>/</li>
<li>supervised model output F1 scores</li>
<li>unsupervied train emmeddings and store them。 val.npy, val.txt.
5-10Gb &gt; unsupervised 干什么？ 再看论文， use to the downstream
machine learning applications. <code>eval_scripts</code></li>
</ul></li>
</ol>
<h3 id="代码">代码</h3>
<ol type="1">
<li>tesorflow: 代码未看，速度快 python -m graphsage.supervised_train
--train_prefix ./example_data/ppi --model graphsage_mean --sigmoid</li>
</ol>
<p>python -m graphsage.unsupervised_train --train_prefix
./example_data/ppi --model graphsage_mean --max_total_steps 1000
--validate_iter 10</p>
<blockquote>
<p>in order to learn useful, predictive representations in a fully
unsupervised setting, use graph-based loss function.
因为是无监督，所以loss是自己定义出来的，不需要真实标记参与进来 修改PPT,
这里应该是无监督学习</p>
</blockquote>
<p>与GCN最大的区别，在于多了concat操作。</p>
<p>samp_neighs = [sample_neigh + set([node[i]])]</p>
<p>代码还有一个mock的bug未解决</p>
<p>注意到这里其实有好几个loss functions</p>
<ol start="2" type="1">
<li>pytorch: 代码已看，速度快 总结：总依据是原本的算法。 word
embedding特征, cora, pubmed引用数据集是这么做的！ 其他建议</li>
</ol>
<p>每轮是batch, 训练数据的读入。 256, 1024, 然后random.shuffle.
下次再参与训练。</p>
<p>在这里的特征使用了word embedding gcn: 81.5 4s, 训练方式是所有
graphsage: 0.874 两个进行对比，不同在哪里！</p>
<p>问题？ graphsage说将gcn改变为inductive, 就是这里进行改变吗？</p>
<p>有证明sample取不同的阈值，可以达到不同的精确度</p>
<h2 id="论文查找">论文查找</h2>
<p>论文： Hamilton W, Ying Z, Leskovec J (2017) Inductive Representation
Learning on Large Graphs</p>
<h3 id="parallel-215">parallel 215</h3>
<ol type="1">
<li><p>Neugraph</p></li>
<li><p>parallel computation of graph embedding: ! large graphs. framwork
for parallel computation of a graph embedding using a cluster of compute
nodes with resource constraints. propose a new way to evaluate the
quality of graph embeddings that is independent of a specific inference.
computation scales well, while largely maintaining the embedding
quality.</p></li>
<li><p>Accurate, Efficient and Scalable Graph Embedding &gt; a major
challenge is to reduce the complexity of layered GCNs and make them
parallelizable and scalable on very large graphs</p></li>
<li><p>Towards Efficient Large-Scale Graph Neural Network
Computing</p></li>
</ol>
<p>3篇文章是互不影响的关系</p>
<p>interesting</p>
<ol type="1">
<li><p>Deep Inductive Graph Representation Learning</p></li>
<li><p>large-Scale learnable graph convolutional networks &gt; subgraph
train</p></li>
<li><p>Generalizable Resource Allocation in Stream Processing via Deep
Reinforcement Learning &gt; RL</p></li>
<li><p>Deep Neural Representation Learning on Dynamic Graphs via
Self-Attention Networks &gt; 动态图</p></li>
</ol>
<h3 id="distributed">distributed</h3>
<h3 id="cluster-system">cluster system~~</h3>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/12/01/target-for-work/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/12/01/target-for-work/" class="post-title-link" itemprop="url">target for work</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-01 11:19:17" itemprop="dateCreated datePublished" datetime="2019-12-01T11:19:17+08:00">2019-12-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>63</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>需要找清楚自己的工作内容</p>
<p>自己必须学那些东西！</p>
<p>注意说一个事情，不要说着说着，想到另一个点，又转移到另一个点。</p>
<p>不要DFS, 要BFS</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/28/daily-note-11-28/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/28/daily-note-11-28/" class="post-title-link" itemprop="url">daily note 11-28</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-28 21:03:29" itemprop="dateCreated datePublished" datetime="2019-11-28T21:03:29+08:00">2019-11-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="a.-今日遗留任务">A. 今日遗留任务：</h2>
<ol type="1">
<li>查看GAT代码，思考是否能够直接用tensorflow multiplication来表示。
回看GGCN论文 https://github.com/Diego999/pyGAT/blob/master/layers.py
GAT论文还提出一个很有意思的点，就是tensorflow只能做batch类似的操作，不能做graph并行化</li>
</ol>
<ul>
<li><p>pytorch torch.mul(a,b): 元素相乘 torch.mm(a,b): 矩阵相乘 self.W =
(in_features, out_features) self.a = (2*out_fatures, 1) pytorch
检查成功</p></li>
<li><p>tensorflow 没有检查成功！ import tensorflow as tf node1 =
tf.Tensor node2 = tf.Tensor addr = node1 + node2 sess = tf.Session()
print(sess.run(addr))</p></li>
</ul>
<ol start="2" type="1">
<li>已经安装了eigen, C++数学计算库，计划在vscode中实现
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse/mat_mul_op.cc
https://blog.csdn.net/wilsonass/article/details/90754525
多线程，相乘的感觉，没有看到具体的算法 g++ -I /path/to/eigen/
my_program.cpp -o my_program</li>
</ol>
<p>C++ 模板程序也能理解，好像直接将这个放在对应的源文件目录下就可以
https://blog.svenhetin.com/c-ju-zhen-yun-suan-ku/</p>
<p>或许应该先了解稀疏矩阵的乘法是怎么？
https://www.cnblogs.com/wangkundentisy/p/9267764.html</p>
<p>进阶：思考如何进行</p>
<ol start="3" type="1">
<li><p>再思考tensorflow中能够做什么 &gt;
其实tensorflow和pytorch中所表示的都是矩阵的运算，不过所给的卷积操作，还是其他的
&gt; NeuGraph中提到Certain algorithms不能直接用Tensorflow multiplication
operations来表达是什么意思？ 实际上感觉就是矩阵操作。
所以说初步结论：就是tensorflow中没有直接的函数，即矩阵相乘的运算可以一步到位。
需要借助repeat, reshape的运算。 再运用乘法</p></li>
<li><p>阅读GraphSage论文，思考框架 Inductive 归纳 deuctive 演绎
tranductive 转导的</p></li>
</ol>
<p>transductive 特殊到特殊，测试数据也是未来出现的一部分 inductive
learning: 测试数据只用与训练，需要归纳到一般
https://www.zhihu.com/question/68275921</p>
<p>纠正一个错误：之前PPT上说inductive自己说错了。 应该是侧重于用途</p>
<p>感觉inductive像是一个某类问题的模板，比如排序 without task-specific
transductive像是一个具体的算法，求最大值？</p>
<p>tocheck 6. 重点：讨论AliGraph,
再细看AliGraph论文，以及后面的实验部分！
NeuGraph思考的是如何在GPU上进行加速
AliGraph是针对自己的平台的多个特性按需扩展 skip-gram论文？ word2vec
word2vec: 关键问题，如何对一个词进行表示？</p>
<p>power layer 原则，实际上的做法是</p>
<ol start="7" type="1">
<li><p>阅读how to read a paper, sparse</p></li>
<li><p>回顾NeuGraph
neugraph，重点：如何解决点划分，进而带来的每个块的负载均衡问题。
采用了两个策略：1</p></li>
</ol>
<h2 id="b.-现有框架的缺点">B. 现有框架的缺点：</h2>
<p>NeuGraph: 1. 图结构一旦确定，不能改变，不支持动态 2. 不支持GAT 3.
效果真的会比GraphSage好吗？ 看实验部分！！！ 未认真看</p>
<h2 id="paper-lists">1. Paper Lists</h2>
<h3 id="deep-learning-supported-dataflow-frameworks">1.1 Deep Learning
Supported Dataflow Frameworks</h3>
<p>TensorFlow[13], PyTorch[14], MXNet[15], CNTK[16]</p>
<h3 id="graph-processing-systems">1.2 Graph Processing Systems</h3>
<p>Pregel[1], GraphLab[2], PowerGraph[3], GraphX[4]</p>
<h3 id="graph-neural-networks">1.3 Graph Neural Networks</h3>
<p><strong>1. Survey</strong> A Comprehensive Survey on Graph Neural
Networks[25]</p>
<p><strong>2. Graph Convolution Networks</strong> 1stChebNet[26],
GGNN[27]</p>
<p><strong>3. Others</strong> CommNet[28]</p>
<h3 id="gnns-modeling-frameworks">1.4 GNNs Modeling Frameworks</h3>
<p>GraphSAGE[17], MPNN[18], GNBlock[19]</p>
<h3 id="gnns-parallel-systems">1.5 GNNs Parallel Systems</h3>
<p>NeuGraph[29], AliGraph[30]</p>
<h3 id="non-gnns-parallel-works">1.6 Non-GNNs Parallel Works</h3>
<p>AMPNet[23], BPT-CNN[24]</p>
<h3 id="optimizations">1.7 Optimizations</h3>
<p><strong>1. Graph Layout, Sequential Data Access, and Secondary
Storage</strong> GraphChi[5], Grace[6], FlashGraph[7], XStream[8],
Chaos[9]</p>
<p><strong>2. Ditributed Shared Memory</strong> Grappa[10]</p>
<p><strong>3. NUMA-Awareness, Scheduling, and Graph
Partitioning</strong> PowerLyra[11], BiGraph[12]</p>
<p><strong>4. Sparse Works</strong> Sparse Deep Neural Network Graph
Challenge[22]</p>
<h3 id="others">1.8 Others</h3>
<p><strong>1. Bridge the Gap between Graph and Traditional Machine
Learning Computation</strong> TuX2[20]</p>
<p><strong>2. Introduce the Vertex-Centric Programming Model into
Dynamic Neural Networks</strong> Cavs[21]</p>
<h3 id="aligraph">1.9 AliGraph</h3>
<h2 id="references">2. References</h2>
<p>[1] Elmagarmid, A. K., Agrawal, D., &amp; Association for Computing
Machinery. Special Interest Group on Management of Data. (n.d.). Pregel:
A System for Large-Scale Graph Processing. [2] Low, Y., Gonzalez, J.,
Kyrola, A., Bickson, D., Guestrin, C., &amp; Hellerstein, J. M. (2150).
Distributed GraphLab: A Framework for Machine Learning and Data Mining
in the Cloud. [3] Gonzalez, J. E., Low, Y., Gu, H., Bickson, D., &amp;
Guestrin, C. (n.d.). PowerGraph: Distributed Graph-Parallel Computation
on Natural Graphs (Vol. 12). [4] Flinn, J., Levy, H., ACM Special
Interest Group in Operating Systems., USENIX Association, &amp; ACM
Digital Library. (n.d.). GraphX: Graph Processing in a Distributed
Dataflow Framework. [5] Kyrola, A., Blelloch, G., &amp; Guestrin, C.
(n.d.). GraphChi: Large-Scale Graph Computation on Just a PC. [6]
Prabhakaran, V., Wu, M., &amp; Weng, X. (2012). Managing Large Graphs on
Multi-Cores With Graph Awareness Design for a Graph Management System.
USENIX Annual Technical Conference. [7] Zheng, D., Mhembere, D., Burns,
R., Vogelstein, J., Priebe, C. E., &amp; Szalay, A. S. (2014).
FlashGraph: Processing Billion-Node Graphs on an Array of Commodity
SSDs. [8] Roy, A., Mihailovic, I., &amp; Zwaenepoel, W. (2013).
X-Stream: Edge-centric graph processing using streaming partitions. SOSP
2013 - Proceedings of the 24th ACM Symposium on Operating Systems
Principles, 472–488. [9] Roy, A., Bindschaedler, L., Malicevic, J.,
&amp; Zwaenepoel, W. (2015). Chaos: Scale-out graph processing from
secondary storage. SOSP 2015 - Proceedings of the 25th ACM Symposium on
Operating Systems Principles, 410–424. [10] Nelson, J., Holt, B., Myers,
B., Briggs, P., Ceze, L., Kahan, S., &amp; Oskin, M. (2015).
Latency-Tolerant Software Distributed Shared Memory. USENIX Annual
Technical Conference (ATC) [11] Chen, R., Shi, J., Chen, Y., Zang, B.,
Guan, H., &amp; Chen, H. (2018). PowerLyra: Differentiated graph
computation and partitioning on skewed graphs. ACM Transactions on
Parallel Computing, 5(3). [12] Chen, R., Shi, J. X., Chen, H. B., &amp;
Zang, B. Y. (2015). Bipartite-Oriented Distributed Graph Partitioning
for Big Learning. Journal of Computer Science and Technology, 30(1),
20–29. [13] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean,
J., … Zheng, X. (2016). TensorFlow: A system for large-scale machine
learning. [14] PyTorch. http://pytorch.org, Retrieved January,2019. [15]
Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., … Alberta, M. U.
(n.d.). MXNet: A Flexible and Efficient Machine Learning Library for
Heterogeneous Distributed Systems. [16] Yu, D., Eversole, A., Seltzer,
M., Yao, K., Huang, Z., Guenter, B., … Slaney, M. (2015). An
Introduction to Computational Networks and the Computational Network
Toolkit. In Microsoft Technical Report. [17] Hamilton, W. L., Ying, R.,
&amp; Leskovec, J. (2017). Inductive representation learning on large
graphs. Advances in Neural Information Processing Systems,
2017-Decem(Nips), 1025–1035. [18] Gilmer, J., Schoenholz, S. S., Riley,
P. F., Vinyals, O., &amp; Dahl, G. E. (2017). Neural message passing for
quantum chemistry. 34th International Conference on Machine Learning,
ICML 2017, 3, 2053–2070. [19] Battaglia, P. W., Hamrick, J. B., Bapst,
V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., … Pascanu, R.
(2018). Relational inductive biases, deep learning, and graph networks.
1–40. [20] Xiao, W., Xue, J., Miao, Y., Li, Z., Chen, C., Wu, M., …
Zhou, L. (2017). Tux2: Distributed Graph Computation for Machine
Learning. Nsdi ’17, 669–682. [21] Xu, S., Zhang, H., Neubig, G., Dai,
W., Kim, J. K., Deng, Z., … Xing, E. P. (2018). Cavs: An Efficient
Runtime System for Dynamic Neural Networks. 2018 {USENIX} Annual
Technical Conference ({USENIX} {ATC} 18), 937–950. [22] Kepner, J.,
Alford, S., Gadepally, V., Jones, M., Milechin, L., Robinett, R., &amp;
Samsi, S. (n.d.). Sparse Deep Neural Network Graph Challenge. [23]
Gaunt, A. L., Johnson, M. A., Riechert, M., Tarlow, D., Tomioka, R.,
Vytiniotis, D., &amp; Webster, S. (2017). AMPNet: Asynchronous
Model-Parallel Training for Dynamic Neural Networks. [24] Chen, J., Li,
K., Member, S., Bilal, K., Zhou, X., Li, K., &amp; Yu, P. S. (n.d.). A
Bi-layered Parallel Training Architecture for Large-scale Convolutional
Neural Networks. [25] Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C.,
&amp; Yu, P. S. (2019). A Comprehensive Survey on Graph Neural Networks.
X(X), 1–22. [26] Kipf, T. N., &amp; Welling, M. (n.d.). SEMI-SUPERVISED
CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS. [27] Li, Y., Tarlow,
D., Brockschmidt, M., &amp; Zemel, R. (2015). Gated Graph Sequence
Neural Networks. (1), 1–20. [28] Sukhbaatar, S., Szlam, A., &amp;
Fergus, R. (2016). Learning multiagent communication with
backpropagation. Advances in Neural Information Processing Systems,
2252–2260. [29] Ma, L., Yang, Z., Miao, Y., Xue, J., Wu, M., Zhou, L.,
&amp; Dai, Y. (2019). NeuGraph: Parallel Deep Neural Network Computation
on Large Graphs. 2019 USENIX Annual Technical Conference (USENIX ATC
19), 443–458. [30] Zhu, R., Zhao, K., Yang, H., Lin, W., Zhou, C., Ai,
B., … Zhou, J. (2018). AliGraph: A comprehensive graph neural network
platform. Proceedings of the VLDB Endowment, 12(12), 2094–2105.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/21/gnn-parallel-AliGraph-paper-read/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/21/gnn-parallel-AliGraph-paper-read/" class="post-title-link" itemprop="url">gnn-parallel-AliGraph-paper-read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-21 10:38:17" itemprop="dateCreated datePublished" datetime="2019-11-21T10:38:17+08:00">2019-11-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn/" itemprop="url" rel="index"><span itemprop="name">gnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="引用论文列表">引用论文列表</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/13/gnn-parallel-NeuGraph-paper-read/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/13/gnn-parallel-NeuGraph-paper-read/" class="post-title-link" itemprop="url">gnn-parallel-NeuGraph-paper-read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-13 20:04:18" itemprop="dateCreated datePublished" datetime="2019-11-13T20:04:18+08:00">2019-11-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>120</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>NeuGraph: Parallel Deep Neural Network Computation on Large
Graphs</p>
<p>深度学习四大框架</p>
<p>tensorflow keras pytorch caffee2 mxnet cntk</p>
<p>deep graph library</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/03/torch-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/torch-study/" class="post-title-link" itemprop="url">torch-study</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-03 21:46:03" itemprop="dateCreated datePublished" datetime="2019-11-03T21:46:03+08:00">2019-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://blog.csdn.net/qq_31590831/article/details/90764240</p>
<h2 id="pytorch">pytorch</h2>
<p>https://www.cnblogs.com/denny402/p/7520063.html</p>
<p>一个完整的pipeline <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">class MyDataset(Dataset):</span><br><span class="line">    def __init__(self, txt, transform,):</span><br><span class="line">        self.imgs = </span><br><span class="line">        self.transform =</span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        pass</span><br><span class="line">    def __len__(self):</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">train_data = MyDataset(txt=&quot;.txt&quot;, transform)</span><br><span class="line">train_loader = DataLoader(dataset=, batch_size=, shuffle=True)</span><br><span class="line"></span><br><span class="line">class Net(torch.nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Conv2d(3, 32, 3, 1, 1),</span><br><span class="line">            torch.nn.ReLU(),</span><br><span class="line">            torch.nn.MaxPool2d(2)</span><br><span class="line">        )</span><br><span class="line">        self.conv2 = torch.nn.Sequential(</span><br><span class="line">            torch.nn.Conv2d(32, 64, 3, 1, 1),</span><br><span class="line">            torch.nn.ReLU(),</span><br><span class="line">            torch.nn.MaxPool2d(2)</span><br><span class="line">        )</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        conv1_out = self.conv1(x)</span><br><span class="line">        conv2_out = self.conv2(conv1_out)</span><br><span class="line">        conv3_out = self.conv3(conv2_out)</span><br><span class="line">        out = self.dense(conv3_out)</span><br><span class="line">        return out</span><br><span class="line"></span><br><span class="line">model = Net()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line">loss_func = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">for epoch in range(10):</span><br><span class="line">    print(&#x27;epoch &#123;&#125;&#x27;.format(epoch + 1))</span><br><span class="line">    # training</span><br><span class="line">    train_loss = 0</span><br><span class="line">    train_acc = 0</span><br><span class="line">    for batch_x, batch_y in train_loader:</span><br><span class="line">        batch_x, batch_y = Variable(batch_x), Variable(batch_y)</span><br><span class="line">        out = model(batch_x)</span><br><span class="line">        loss = loss_func(out, batch_y)</span><br><span class="line">        train_loss += loss.data[0]</span><br><span class="line">        pred = torch.max(out, 1)[1]</span><br><span class="line">        train_correct = (pred == batch_y).sum()</span><br><span class="line">        train_acc += train_correct.data[0]</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    print(&#x27;Train Loss: &#123;:.6f&#125;, Acc: &#123;:.6f&#125;&#x27;.format(train_loss / (len(</span><br><span class="line">        train_data)), train_acc / (len(train_data))))</span><br></pre></td></tr></table></figure></p>
<p>net.zero_grad(): optimizer.zero_grad():
在下一次计算梯度之前清空梯度</p>
<p>optimizer.step(): 进行下一步更新</p>
<p>查看grad
https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#variablehttp://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#variable</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">y = 2 * x + 2 #y.data, y.grad, y.grad_func</span><br><span class="line">out = y.sum()</span><br><span class="line">out.backward()</span><br><span class="line">x.grad # [[2, 2], [2, 2]]</span><br></pre></td></tr></table></figure>
<p>单看Variable和Tensor没什么区别，个人理解对于Variable可能能保证计算过程一直是grad</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/03/errors-in-c/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/03/errors-in-c/" class="post-title-link" itemprop="url">errors in c++</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-03 10:56:04" itemprop="dateCreated datePublished" datetime="2019-11-03T10:56:04+08:00">2019-11-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="bugs">bugs</h2>
<ol type="1">
<li><p>Segmentation Falut 删代码法, 段错误，溢出；
就是for循环没写对的问题 深搜里面爆栈了，进入了死循环</p></li>
<li><p>Complic Error</p></li>
<li><p>Memory Limit Exceeded 内存超限 &gt; 发现是i &lt; a.size() ||
b.size() 这里的操作出错</p></li>
</ol>
<p>图论问题TLE很大的一个问题可能就是邻接表忘记初始化了memset(h),
还有一个就是无向图M双倍，不要忘了</p>
<ol start="4" type="1">
<li><p>结果是多个一模一样的值，多半是数组越界的问题。就是输入数据的问题</p></li>
<li><p>Float Point Exception 表示除0错误</p></li>
</ol>
<h2 id="其他">其他</h2>
<p>为什么用static? 防止每次调用函数时重新分配内存，效率会更高</p>
<p>https://zhuanlan.zhihu.com/p/57512786</p>
<p>如果对于邻接链表的结构出现 Time Limited 检查h[N]忘记了初始化为-1</p>
<p>如果大部分数据已经过了，1,2个数据没过，说明边界问题没处理好 ##
else</p>
<p>csp考试准备</p>
<ol type="1">
<li>编译环境准备</li>
<li>时间熟悉，考场熟悉</li>
<li>题型熟悉</li>
<li>考试资料熟悉
<ol type="1">
<li>stl, 算法</li>
<li>acwing复习课</li>
</ol></li>
</ol>
<p>重定向 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line">#include &lt;fstream&gt; </span><br><span class="line">#include &lt;cstring&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">int n, m;</span><br><span class="line">int w[10][10];</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">	ifstream in(&quot;in.txt&quot;);</span><br><span class="line">	ofstream out(&quot;out.txt&quot;);</span><br><span class="line">	streambuf *cinbuf = cin.rdbuf();</span><br><span class="line">	streambuf *coutbuf = cout.rdbuf();</span><br><span class="line">	</span><br><span class="line">	cin.rdbuf(in.rdbuf());</span><br><span class="line">	cout.rdbuf(out.rdbuf());</span><br><span class="line">	</span><br><span class="line">	cin &gt;&gt; n &gt;&gt; m;</span><br><span class="line">	</span><br><span class="line">	for (int i = 1; i &lt;= n; i ++)</span><br><span class="line">		for (int j = 1; j &lt;= m; j ++)</span><br><span class="line">			cin &gt;&gt; w[i][j];</span><br><span class="line">			</span><br><span class="line">	for (int i = 1; i &lt;= n; i ++) &#123;</span><br><span class="line">		for (int j = 1; j &lt;= m; j ++)</span><br><span class="line">			cout &lt;&lt; w[i][j] &lt;&lt; &#x27; &#x27;;</span><br><span class="line">		cout &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	cin.rdbuf(cinbuf);</span><br><span class="line">	cout.rdbuf(coutbuf);</span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="c-新特性">c++ 新特性</h2>
<p>dev c++ 5.4.0</p>
<p>tools -&gt; complier options -&gt; add following commands when
calling the complier: -std=c++11</p>
<p>新特性 for (auto x: nums) cout &lt;&lt; x &lt;&lt; endl;</p>
<p>设置autosave</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">str常见操作</span><br><span class="line"></span><br><span class="line">string -&gt; char[]</span><br><span class="line">char *c;</span><br><span class="line">string s = &quot;1234&quot;;</span><br><span class="line">c = s.c_str(); // c_str()返回的是指针</span><br><span class="line"></span><br><span class="line">char c[20];</span><br><span class="line">string s &quot;1234&quot;;</span><br><span class="line">strcpy(c, s.c_str());</span><br><span class="line"></span><br><span class="line">char[] -&gt; str</span><br><span class="line">char b[8]=&#123;&#x27;a&#x27;, &#x27;b&#x27;&#125;;</span><br><span class="line">string str(b); //()一般是初始化</span><br><span class="line"></span><br><span class="line">str += substr</span><br></pre></td></tr></table></figure>
<p>注意除了，初始化不能进行{}赋值</p>
<p>c++: replace(start_pos, len, replace_str) // 指定长度
replce(line.beigin(), line.begin()+6, replace_str); //
迭代器从开始到结束</p>
<p>特别地，需要注意关于字符串这部分的内容</p>
<p>思考下stdlib.h下有没有好用的一些东西 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int atoi(const char *str) 将字符串转换为整数</span><br><span class="line">double atof(const char *str)</span><br></pre></td></tr></table></figure></p>
<p>刷题经常用的库 algorithm - swap, sort, count(f,f+n,value),
reverse()</p>
<p>cstring stoi, stof</p>
<p>int.to_string</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/01/tools-pygraphviz-study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/01/tools-pygraphviz-study/" class="post-title-link" itemprop="url">linux</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-01 13:28:11" itemprop="dateCreated datePublished" datetime="2019-11-01T13:28:11+08:00">2019-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.9k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>2 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>参考 1. <a target="_blank" rel="noopener" href="http://graphviz.org/doc/info/attrs.html">Node, Edge
and Graph Attributes of Pygraphviz</a> 2. <a
target="_blank" rel="noopener" href="https://www.cnblogs.com/liang1101/p/7641984.html">graphviz
程序生成多种类型图表详解</a></p>
<h2 id="i-一个完整的pipline">I 一个完整的pipline</h2>
<ol type="1">
<li>install <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install python-dev graphviz libgraphviz-dev pkg-config</span><br></pre></td></tr></table></figure></li>
<li>初始化
<ul>
<li>.dot文件 <code>dot hello.dot -T png -o hello.png</code>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">strict digraph&#123;</span><br><span class="line">    graph [bb=&quot;-64.382, -82.544, 57.743, 78.934&quot;,</span><br><span class="line">        center=True,</span><br><span class="line">        label=wyp_label];</span><br><span class="line">    node [label=&quot;\n&quot;];</span><br><span class="line">    edge [label=&quot;\n&quot;]</span><br><span class="line">    1   [height=0.5,</span><br><span class="line">        pos=&quot;, &quot;]</span><br><span class="line">    2   []</span><br><span class="line">    1 -&gt; 2 [pos=&quot;&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>dict <code>d = &#123;'1': &#123;'2': None&#125;, '2': &#123;'1': None, '3':None&#125;,
'3':&#123;'2': None&#125;&#125;  A=pgv.AGraph(d)</code></li>
<li>图形 &gt; man <cmd>?
<ul>
<li>dot 明确的方向性</li>
<li>neato 缺乏反向性</li>
<li>twopi 放射性</li>
<li>circo 环形布局</li>
</ul></li>
<li>pipline <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import pygraphviz as pgv</span><br><span class="line"># 1. init a graph</span><br><span class="line">G = pgv.AGraph(strict=, directed=) # default: strict-True</span><br><span class="line"></span><br><span class="line"># 2. set the global property</span><br><span class="line">G.graph_attr[&#x27;&#x27;] = &#x27;&#x27;</span><br><span class="line">G.node_attr[&#x27;&#x27;]=&#x27;&#x27;</span><br><span class="line">G.edge_attr[&#x27;&#x27;]=&#x27;&#x27;</span><br><span class="line"></span><br><span class="line"># 3. add node and edge</span><br><span class="line">G.add_node_from(list1)</span><br><span class="line">G.add_node(&#x27;a&#x27;, color=&#x27;red&#x27;)</span><br><span class="line">G.add_edge(&#x27;b&#x27;, &#x27;c&#x27;, color=&#x27;blue&#x27;)</span><br><span class="line"></span><br><span class="line"># change node or edge attributes</span><br><span class="line">n = G.get_node(&#x27;f&#x27;)</span><br><span class="line">n.attr[&#x27;shape&#x27;]=&#x27;box&#x27;</span><br><span class="line">e = G.get_edge(&#x27;b&#x27;, &#x27;c&#x27;)</span><br><span class="line">e.attr[&#x27;color&#x27;]=&#x27;green&#x27;</span><br><span class="line"></span><br><span class="line"># 4. set layout: dot(direction), twopi(away), circo(circle)</span><br><span class="line">G.layout(lay)</span><br><span class="line"></span><br><span class="line"># 5. save fig</span><br><span class="line">G.draw(file_path)</span><br></pre></td></tr></table></figure></li>
<li>属性
<ul>
<li>node
<ul>
<li>color</li>
<li>comment: ??</li>
<li>fillcolor</li>
<li>fontname: font family Times-Roman</li>
<li>fontsize: 14</li>
<li>label</li>
<li>shape</li>
</ul></li>
<li>edge
<ul>
<li>arrowhead: normal</li>
<li>arrowsize: 1.0</li>
<li>arrowtail</li>
<li>color</li>
<li>comment</li>
<li>dir: forward, back, both</li>
<li>fontcolor: black; fontname: ; fontsize</li>
<li>headlabel: label near head of edge</li>
<li>label: edge label</li>
<li>labelfontcolor:; labelfontname:;</li>
</ul></li>
<li>graph
<ul>
<li>bgcolor:</li>
<li>center false</li>
<li>comment:</li>
<li>label &gt; 有意思的是支持html语言 &gt;
http://graphviz.org/doc/info/shapes.html#record &gt;
https://www.cnblogs.com/zhishaofei/p/4033175.html</li>
</ul></li>
</ul></li>
<li>其他的
<ul>
<li>转换为pvg <code>h=A.handle;
C=pgv.AGraph(h);A.draw("a.svg");</code></li>
<li>load and save dot file <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">G = pgv.AGraph(dot_file)</span><br><span class="line">G.write(dot_file)</span><br></pre></td></tr></table></figure></li>
<li>string &gt; s = G.string()</li>
</ul></li>
</ul></li>
</ol>
<p>output-format https://graphviz.gitlab.io/_pages/doc/info/output.html
## 待做</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://augf.github.io/2019/11/01/gnn-parallel-ggsnn-code-realize/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/11/01/gnn-parallel-ggsnn-code-realize/" class="post-title-link" itemprop="url">gnn-parallel-ggsnn-code-realize</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-01 09:29:13" itemprop="dateCreated datePublished" datetime="2019-11-01T09:29:13+08:00">2019-11-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn/" itemprop="url" rel="index"><span itemprop="name">gnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="待做">待做</h2>
<ol type="1">
<li><p>表示复杂的梯度计算图 &gt; 根据计算图来表示，计算图有两种类型</p>
<ul>
<li>节点表示操作，边表示数据，如Tensorflow</li>
<li>节点表示数据，边表示操作，如计算图
https://blog.csdn.net/zxl55/article/details/83537144</li>
</ul></li>
<li><p>将所有复杂的公式转变为矩阵操作，然后直接使用Tensorflow类似的想法，不过感觉上可以做得更具体点</p></li>
<li><p>剥离输入输出，模型照抄 &gt;
为什么远程登录是用到Anaconda中的python</p></li>
</ol>
<h5 id="一些常量说明">一些常量说明</h5>
<ul>
<li>H(t-1): 上一轮的状态</li>
<li>H(t): 最终状态</li>
<li>Hz(t): 当前计算的状态</li>
<li>Wa: 初始化对H(t-1)做的操作</li>
<li>Wz, Wh, Wr: Z(t), Hz(t), R(t)关于Av(t-1)的权重</li>
<li>Uz, Uh, Ur: Z(t), Hz(t), R(t)关于H(t-1)的权重</li>
<li><span class="math inline">\(Av=[A_{in}, A_{out}]\)</span></li>
</ul>
<h5 id="图上的操作">图上的操作</h5>
<p><span class="math inline">\(Av(t-1) = A_{v:} (H(t-1)Wa)\)</span>
##### GRU单元 <span class="math inline">\(Z(t) = \delta (Av(t-1)Wz +
H(t-1)Uz)\)</span> <span class="math inline">\(R(t) = sigmoid(Av(t-1)Wr
+ H(t-1)Ur)\)</span> <span class="math inline">\(Hzt(t) = tanh(Av(t-1)Wh
+ (R(t) \odot H(t-1))Uh)\)</span> <span class="math inline">\(H(t-1) =
(1-Z(t))\odot H(t-1) + Z(t) \odot Hzt(t)\)</span></p>
<h5 id="原论文公式">原论文公式</h5>
<p>$<em>v^{t} = </em>{v:}^T [<em>1^{(t-1)T} ... </em>{|V|}^{(t-1)T}]^{T}
+ $ <span class="math inline">\(\mathbf{z}_v^t = \mathbf{\delta} (
\mathbf{W}^z \mathbf{a}_v^{t} + \mathbf{U}^z
\mathbf{h}_v^{t-1})\)</span> <span class="math inline">\(\mathbf{r}_v^t
= \delta ( \mathbf{W}^r \mathbf{a}_v^{t} + \mathbf{U}^r
\mathbf{h}_v^{t-1})\)</span> <span class="math inline">\(\mathbf{h}_v^t
= tanh ( \mathbf{W} \mathbf{a}_v^{t} + \mathbf{U} ( \mathbf{r}_v^t \odot
\mathbf{h}_v^{t-1}))\)</span> <span class="math inline">\(\mathbf{h}_v^t
= (1 - \mathbf{z}_v^t) \odot \mathbf{h}_v^{t-1} + \mathbf{z}_v^t \odot
\mathbf{h}_v^t\)</span></p>
<h5 id="代码-pyg">代码 pyg</h5>
<p><span class="math inline">\(\mathbf{h}_j^{0} = \mathbf{x}_i
\mathbf{\parallel} 0\)</span> <span
class="math inline">\(\mathbf{m}_i^{l+1} = \sum_{j \in \mathcal{N}(i)}
\mathbf{\Theta} \mathbf{h}_j^{l}\)</span> <span
class="math inline">\(\mathbf{z}_i^{l+1} = \mathbf{\delta} (
\mathbf{W}^z \mathbf{m}_i^{l+1} + \mathbf{U}^z
\mathbf{h}_i^{l})\)</span> <span
class="math inline">\(\mathbf{r}_i^{l+1} = \delta ( \mathbf{W}^r
\mathbf{m}_i^{l+1}+ \mathbf{U}^r \mathbf{h}_i^{l})\)</span> <span
class="math inline">\(\mathbf{h}_i^{l+1} = tanh ( \mathbf{W}
\mathbf{m}_i^{l+1} + \mathbf{U} ( \mathbf{r}_i^{l+1} \odot
\mathbf{h}_i^{l})))\)</span> <span
class="math inline">\(\mathbf{h}_i^{l+1} = (1 - \mathbf{z}_i^{l+1})
\odot \mathbf{h}_i^l + \mathbf{z}_i^{l+1} \odot
\mathbf{h}_i^{l+1}\)</span></p>
<p>$^{l+1} = ^{l} $ <span class="math inline">\(\mathbf{Z}^{l+1} =
\mathbf{\delta} (\mathbf{M}^{l+1} \mathbf{W}^z + \mathbf{H}^{l}
\mathbf{U}^z)\)</span> <span class="math inline">\(\mathbf{R}^{l+1} =
\delta ( \mathbf{M}^{l+1} \mathbf{W}^r + \mathbf{H}^{l}
\mathbf{U}^r)\)</span> <span class="math inline">\(\mathbf{H}_t^{l+1} =
tanh ( \mathbf{M}^{l+1} \mathbf{W} + (\mathbf{R}^{l+1} \odot
\mathbf{H}^{l})\mathbf{U}))\)</span> <span
class="math inline">\(\mathbf{H}^{l+1} = (1 - \mathbf{Z}^{l+1}) \odot
\mathbf{H}^l + \mathbf{Z}^{l+1} \odot \mathbf{H}_t^{l+1}\)</span></p>
<p>以算<span
class="math inline">\(\mathbf{W}^r\)</span>的权重矩阵梯度为例，我们需要将上面公式拆分为以下小公式，
$_1 = ^{l+1} ^r $ <span class="math inline">\(\mathbf{T}_2 =
\mathbf{T}_1 + \mathbf{H}^{l} \mathbf{U}^r\)</span> <span
class="math inline">\(\mathbf{R}^{l+1} = \mathbf{\delta}
(\mathbf{T}_2)\)</span> $_3 = ^{l+1} ^l $ <span
class="math inline">\(\mathbf{T}_4 = \mathbf{T}_3 \mathbf{U}\)</span>
<span class="math inline">\(\mathbf{T}_5 = \mathbf{M}^{l+1} \mathbf{W} +
\mathbf{T}_4\)</span> <span class="math inline">\(\mathbf{H}_t^{l+1} =
tanh(\mathbf{T}_5)\)</span> <span class="math inline">\(\mathbf{T}_6 =
\mathbf{Z}^{l+1} \odot \mathbf{H}_t^{l+1}\)</span> <span
class="math inline">\(\mathbf{H}^{l+1} = (1 - \mathbf{Z}^{l+1}) \odot
\mathbf{H}^l + \mathbf{T}_6\)</span></p>
<p>从而，反向传播，通过链式法则，<span
class="math inline">\(\mathbf{W}_r\)</span>的梯度就为： <span
class="math inline">\(\frac{\partial loss} {\partial \mathbf{W}_r} =
\frac {\partial loss}{\partial\mathbf{H}^{l+1}} \cdot \frac
{\partial\mathbf{H}^{l+1}} {\partial\mathbf{T}^6} \cdot \frac {\partial
\mathbf{T}_6}{\partial\mathbf{H}_t^{l+1}} \cdot \frac
{\partial\mathbf{H}_t^{l+1}}{\partial\mathbf{T}_5} \cdot \frac
{\partial\mathbf{T}_5}{\partial\mathbf{T}_4} \cdot \frac
{\partial\mathbf{T}_4}{\partial\mathbf{T}_3} \cdot \frac
{\partial\mathbf{T}_3}{\partial\mathbf{R}^{l+1}} \cdot \frac
{\partial\mathbf{R}^{l+1}}{\partial\mathbf{T}_2} \cdot \frac
{\partial\mathbf{T}_2}{\partial\mathbf{T}_1} \cdot \frac
{\partial\mathbf{T}_1}{\partial\mathbf{W}_r}\)</span> ## 问题总结</p>
<ol type="1">
<li>RuntimeWarning: overflow encountered in exp &gt;
https://www.cnblogs.com/zhhy236400/p/9873322.html</li>
</ol>
<p>实现说明
由于该算法初始输入要求输入维度小于隐藏层的维度，所以在实现时，在Input
Layer后进行了特征到隐向量维度的转换，在Prediction Layer前进行</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yun-Pan Wang"
      src="/images/default-avatar.png">
  <p class="site-author-name" itemprop="name">Yun-Pan Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AugF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AugF" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangyp@smail.nju.edu.cn" title="E-Mail → mailto:wangyp@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/Joswxe" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;Joswxe" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://cn.bing.com/" title="https:&#x2F;&#x2F;cn.bing.com" rel="noopener" target="_blank">Bing</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.baidu.com/" title="https:&#x2F;&#x2F;www.baidu.com" rel="noopener" target="_blank">Baidu</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">[object Object]</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">466k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:04</span>
</div>!

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://augf.github.io/page/2/',]
      });
      });
  </script>



</body>
</html>
