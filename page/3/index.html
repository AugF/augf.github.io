<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ypwang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Life&#39;s Notes">
<meta property="og:url" content="https://ypwang.github.io/page/3/index.html">
<meta property="og:site_name" content="Life&#39;s Notes">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Yun-Pan Wang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://ypwang.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Life's Notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Life's Notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">岁月数载，愿不负韶华</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/gnn-parallel-GRU-9a9ca13e91f9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/gnn-parallel-GRU-9a9ca13e91f9/" class="post-title-link" itemprop="url">gnn-parallel-GRU</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-26 08:57:59" itemprop="dateCreated datePublished" datetime="2019-10-26T08:57:59+08:00">2019-10-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn/" itemprop="url" rel="index"><span itemprop="name">gnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>3 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>2015 ## 1. Abstract 机器翻译。 encoder + decoder encoder: a
fixed-lenght representation decoder: a correct translation from this
representation</p>
<p>RNN Encoder-Decoder gated recusive rconvolutional neural network</p>
<p>the neural machine translation performs relatively well on short
sentences without unknown words. but degrades rapidly as the lengthe of
the sentence and the number of unknown words.</p>
<p>gated recursive convolutional networks learns a grammatical
struture</p>
<h2 id="introduction">2. Introduction</h2>
<p>SMT仅仅记忆一小部分内容 GRU可以记忆500MB内容</p>
<p>没有工作来分析这些模型的属性以及表现</p>
<p>understand the properties and behavior</p>
<p>grConv is able to learn, without supervision, a kind of syntactic
struture over the sourece language. &gt; rnn, cnn paper</p>
<p>不懂概率的形式 &gt; K是class labels 数量</p>
<p>p(x): joint distribution</p>
<hr />
<p>以上抛弃</p>
<p>Learning Phrase Representations using RNN Encoder–Decoder for
Statistical Machine Translation</p>
<p>其实所谓的概率模型<span class="math inline">\(p(x_1, x_2, ...,
x_n)\)</span>，
其实是链式法则，直接看最后一层即可，这也就是RNN实际的含义。</p>
<p>RNN其实的目标是 maximize the conditional log-likelihood.</p>
<hr />
<p>LSTM</p>
<ol type="1">
<li>the problem, with conventional "Back-Propagation Through TIme).
<ul>
<li>blow up</li>
<li>vanish</li>
</ul></li>
</ol>
<p>直接已经有的知识那么用</p>
<p>为什么RNN不能记忆知识？ 反向传播时，&lt;1.0的数引起梯度消失，
&gt;1.0的数引起梯度爆炸</p>
<p>LSTM: 输入、忘记、输出</p>
<p>主线剧情由输入分线进行控制</p>
<p>输入的内容会写入主线，然后忘记的内容会影响到输入的内容</p>
<p>RNN到底是什么？</p>
<p>network对于sequence序列问题解决的很好 上一个状态和当前状态</p>
<p>lstm使得其中cell的运算变得复杂</p>
<p>https://www.youtube.com/watch?v=EC3SvfW0Z_A</p>
<p>https://blog.csdn.net/FlyingLittlePig/article/details/72229041 &gt;
非常好，有代码实现部分</p>
<p>https://www.yunaitong.cn/understanding-lstm-networks.html &gt;
这篇也有意思</p>
<p>https://www.youtube.com/watch?v=8HyCNIVRbSU &gt; 好像有乘积符号</p>
<blockquote>
<p>传统RNN: <span class="math display">\[h = tanh(W_x \cdot x_t + W_h
\cdot h_{t-1} + b)\]</span></p>
</blockquote>
<blockquote>
<p>LSTM: 1. forget gate layer <span class="math inline">\(f_t =
\delta(W_i \cdot [h_{t-1}, x_t] + b_i)\)</span> 2. input gate layer $
i_t = (W_i + b_i) $ $ = tanh(W_C + b_C) $ 3. the current state $ C_t =
f_t C_{t-1} + i_t $ &gt; <span class="math inline">\(f_t,
i_t\)</span>分别表示forget gate的权重， input gate所占的权重 &gt;
根据维度进行猜想，那么这里实际上就不是元素的乘法了？？？ 4. output layer
$ o_t = (W_o + b_o)$ $ h_t = o_t tanh(C_t)$ 5. predict $ y_t =
softmax(W_y h_t + b_t）$</p>
</blockquote>
<blockquote>
<p>GRU: <span class="math inline">\(z_t = \delta (W_z \cdot [h_{t-1},
x_t])\)</span> <span class="math inline">\(r_t = \delta(W_r \cdot
[h_{t-1}, x_t])\)</span> <span class="math inline">\(\tilde{h}_t =
tanh(W \cdot [r_t \odot h_{t-1}, x_t])\)</span> <span
class="math inline">\(h_t = (1-z_t) \odot h_{t-1} + z_t \odot
\tilde{h}_t\)</span> &gt; LSTM的变体，主要的改变在于， forget gate <span
class="math inline">\(f_t\)</span>和input gate <span
class="math inline">\(i_t\)</span>用了一个update gate <span
class="math inline">\(z_t\)</span>来替代. &gt; 然后， current
state的变化再加了一层， <span class="math inline">\(\tilde{h}_t = tanh(W
\cdot [r_t \odot h_{t-1}, x_t])\)</span>, 这里引入了一个<span
class="math inline">\(r_t\)</span>叫做reset gate. &gt;
其次就是没有了output layer层，直接输出 计算开销更小</p>
</blockquote>
<blockquote>
<p>参考 1. https://zhuanlan.zhihu.com/p/32481747 2.
https://www.yunaitong.cn/understanding-lstm-networks.html 3.
https://blog.csdn.net/FlyingLittlePig/article/details/72229041 4.
https://www.youtube.com/watch?v=EC3SvfW0Z_A</p>
</blockquote>
<p>GGNN propgagation model</p>
<ol type="1">
<li><p>初始化 <span class="math inline">\(h_v^{(1)} = [x_v^T,
0]^T\)</span></p></li>
<li><p>图操作 <span class="math inline">\(a_v^{(t)}= A_{v:}^T
[h_1^{(t-1)T} ... h_{|V|}^{(t-1)T}]^T + b\)</span></p></li>
<li><p>GRU <span class="math inline">\(z_v^t = \delta (W^z a_v^{(t)} +
U^z h_v^{(t-1)})\)</span> <span class="math inline">\(r_v^t = \delta
(W^r a_v^{(t)} + U^r h_v^{(t-1)})\)</span> <span
class="math inline">\(\hat{h}_v^{(t)} = tanh(W a_v^{(t)} + U (r_v^t
\odot h_v^{(t-1)}))\)</span> <span class="math inline">\(h_v^{(t)} = (1
- z_v^t) \odot h_v^{(t-1)} + z_v^t \odot
\hat{h}_v^{(t)}\)</span></p></li>
</ol>
<h2 id="new">new</h2>
<ol type="1">
<li>初始化 <span class="math inline">\(H^{(0)} = [X_v, P]\)</span> <span
class="math inline">\(A_v = [A_{in}, A_{out}]\)</span> <br></li>
<li>图操作 <span class="math inline">\(T^{(t-1)} = A_v (H^{(t-1)}
W^{A})\)</span> <br></li>
<li>节点操作 $H^{(t)} = $ <font color='red'><span
class="math inline">\(\delta(T^{(t-1)}W^{z_a})\)</span></font> <span
class="math inline">\(\odot\)</span> <font color='red'><span
class="math inline">\(tanh(T^{(t-1)}W_{h_a})\)</span></font> $ + $
<font color='red'><span
class="math inline">\(\delta(T^{(t-1)}W^{z_a})\)</span></font> <span
class="math inline">\(\odot\)</span> <font color='red'><span
class="math inline">\(tanh\{\delta[(T^{t-1)}W^{r_a} \odot
H^{(t-1)})W^h]\}\)</span></font><span class="math inline">\(+\)</span>
<font color='red'>$ (T<sup>{(t-1)}W</sup>{z_a})$</font> $  tanh{+
H^{(t-1)}} + $ <font color='red'>$
tanh(T<sup>{(t-1)}W</sup>{h_a})$</font> $  (H<sup>{t-1}W</sup>{z_h}) + $
<font color='red'>$tanh{} $</font> $ (H<sup>{(t-1)}W</sup>{z_a}) +\
(H<sup>{(t-1)}W</sup>{z_h}) tanh{- H^{(t-1)} } + H^{(t-1)}$</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/tools-latex-MathJax-b0283ec1aa12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/tools-latex-MathJax-b0283ec1aa12/" class="post-title-link" itemprop="url">tools-latex-MathJax</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-14 10:34:04" itemprop="dateCreated datePublished" datetime="2019-10-14T10:34:04+08:00">2019-10-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/latex/" itemprop="url" rel="index"><span itemprop="name">latex</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://colobu.com/2014/08/17/MathJax-quick-reference/</p>
<p>MathJax是一个JavaScript引擎，用来显示网络上的数学公式。它支持大部分主流的浏览器，对大部分用户而言它不需要安装，既没有插件需要下载也没有软件需要安装，属于Apache。MathJax使用网络字体去产生高质量的排版，使其在所有分辨率都可缩放和显示，这远比包含公式的图片更有效得多。使用MathJax显示数学公式是基于文本的，而非图片。他可以被搜索引擎使用，这意味着方程式和页面上的文字一样是可以被搜索的。MathJax允许页面作者使用Tex、
LaTex符号和MathML或者AsciiMath去书写公式。
转化为MathML格式你可以赋值粘贴它们到其他程序中。
MathJax是模块化的，所以它仅仅在需要时才加载它的组件，同时也可以被扩展以实现更多功能。MathJax同时也是高度可配置的，允许作者做出更适宜网站自身的定义。</p>
<p><a
target="_blank" rel="noopener" href="https://colobu.com/2014/08/17/MathJax-quick-reference/">MathJax中文文档</a></p>
<ol type="1">
<li><p>希腊字母</p>
<ul>
<li>lower <code>\alpha, \beta, \gamma, \delta, \epsilon(\varepsilon),
\zeta, \eta, \theta(\vartheta), \iota, \kappa, \lambda, \mu, \xi, o,
\pi, \rho(\varrho), \sigma, \tau, \upsilon, \phi(\varphi), \chi, \psi,
\omega</code> <span class="math display">\[\alpha, \beta, \gamma,
\delta, \epsilon(\varepsilon), \zeta, \eta, \theta(\vartheta), \iota,
\kappa, \lambda, \mu, \xi, o, \pi, \rho(\varrho), \sigma, \tau,
\upsilon, \phi(\varphi), \chi, \psi, \omega\]</span></li>
<li>upper <code>A, B, \Gamma, \Delta, E, Z, H, \Theta, I, K, \Lambda,
M(N), \Xi, O, \Pi, P, \Sigma, T, \Upsilon, \Phi, X, \Psi, \Omega</code>
<span class="math display">\[A, B, \Gamma, \Delata, E, Z, H, \Theta, I,
K, \Lambda, M(N), \Xi, O, \Pi, P, \Sigma, T, \Upsilon, \Phi, X, \Psi,
\Omega\]</span></li>
</ul></li>
<li><p>字体</p>
<ul>
<li><code>\mathbb, \Bbb</code>黑板体 <span
class="math inline">\(\mathbb{Z}, \Bbb{Z}\)</span></li>
<li><code>\mathbf</code>粗体 <span
class="math inline">\(\mathbf{Z}\)</span></li>
<li><code>\mathtt</code>打印体 <span
class="math inline">\(\mathtt{Z}\)</span></li>
<li><code>\mathrm</code>罗马体 <span
class="math inline">\(\mathrm{Z}\)</span></li>
<li><code>\mathcal</code> <span
class="math inline">\(\mathcal{Z}\)</span></li>
<li><code>\mathscr</code> <span
class="math inline">\(\mathscr{Z}\)</span></li>
<li><code>\mathfrak</code> <span
class="math inline">\(\mathfrak{Z}\)</span></li>
</ul></li>
<li><p>一大堆符号 &gt; https://pic.plover.com/MISC/symbols.pdf</p>
<ul>
<li>常用</li>
</ul></li>
<li><p>中途空格 <code>\quad, \qquad</code></p></li>
<li><p><code>\hat, \widehat, \bar, \overline, \vec, \overrightarrow,
\dot, \ddot</code> &gt; <span class="math inline">\(\hat{x},
\widehat{xy}, \bar{x}, \overline{xyz}, \vec{x}, \overrightarrow{xyz},
\dot{x}, \ddot{x}\)</span></p></li>
<li><p>矩阵 <code>\begin&#123;matrix&#125; ... \end&#123;matrix&#125;</code></p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/gnn-parallel-ggsnn-code-read-41f35521ee50/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/gnn-parallel-ggsnn-code-read-41f35521ee50/" class="post-title-link" itemprop="url">gnn-parallel-ggsnn-code-read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-10 21:16:00" itemprop="dateCreated datePublished" datetime="2019-10-10T21:16:00+08:00">2019-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="torch-lua">1. torch lua</h2>
<p>th test.lua to test all the modules in the ggnn and run libraries</p>
<ol type="1">
<li>go into "babi/data", run'bash get_10_fold_data.sh" to get 10 folds
of bAbI data for 5 tasks (4, 15, 16, 18, 19) and do some
preprocessing</li>
<li>go into 'babi/data/extra_seq_tasks', run'bash
generate_10_fold_data.sh' to get 10 folds of data for the two extra
sequence tasks</li>
<li>go back to 'babi/' and use 'run_experiments.py' to run the
GGNN/GGS-NN</li>
<li>Use 'run_rnn_baselines.py babi18 lstm' runs LSTM on bAbI task 18 for
all 10 folds of data</li>
</ol>
<h3 id="目录结构">目录结构</h3>
<ul>
<li>run_rnn_baselines babi18 lstm</li>
<li>run_experiments / bash generate_10_fold_data.sh</li>
<li>get_10_fold_data.sh: do some preprocessing预处理</li>
</ul>
<h3 id="实际执行">实际执行</h3>
<ul>
<li>babi4
<ul>
<li>run_q4()
<ul>
<li>babi_train.lua
<ul>
<li>paras
<ul>
<li>nsteps: number of propagation iterations</li>
<li>learnrate: learning rate 0.01</li>
<li>momentum: ? for adam ？</li>
<li>mb: minibatch size</li>
<li>maxiters: maximum number of weight updates 100</li>
<li>printafter: save checkpoint after this amount of weight updates</li>
<li>saveafter: save checkpoint after this amount of weight updates
100</li>
<li>optim: adam</li>
<li>statedim: dimensionality of the node representations</li>
<li>evaltrain: evaluate on training set during training if set to 1</li>
<li>nthreads: set the number of threads to use with this process</li>
<li>ntrain: number of training instances 50</li>
<li>nval: number of validation instances 50</li>
<li>annotationdim: dimensionality of the node annotations
节点annotations的维度</li>
<li>outputdir: output directory</li>
<li>mode: {selectnode, classifygraph, seqclass, shareprop_seqclass}
selectnode</li>
<li>datafile: should contain lists of edges and questions in standard
format</li>
<li>seed: random seed. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">preprocess其实就是edges上的属性，点的属性</span><br><span class="line">color = require &#x27;trepl.colorize&#x27;</span><br><span class="line">babi_data = require &#x27;babi_data&#x27;</span><br><span class="line">eval_util = require &#x27;eval_util&#x27;</span><br><span class="line">ggnn = require &#x27;,,/ggnn&#x27;</span><br><span class="line">1. prepare data</span><br><span class="line">math.randomseed(opt.seed)</span><br><span class="line">torch.manualSeed(opt.seed)</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ul></li>
</ul></li>
<li>eval_q4()
<ul>
<li>babi_eval.lua</li>
</ul></li>
</ul></li>
</ul>
<p>todo: 看具体是怎么做的?</p>
<h3 id="数据预处理">数据预处理</h3>
<pre><code>python rnn_preprocess.py processed_$fold/train/$&#123;i&#125;_graphs.txt processed_$fold/rnn/train/$&#123;i&#125;_tokens.txt --mode graph --nval 50</code></pre>
<h3 id="babi使用">bAbi使用</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">babi-tasks PathFinding --path-length 3 --decoys 1</span><br><span class="line">babi-tasks Size --steps 3</span><br><span class="line"></span><br><span class="line"> ./babi-tasks $i 1000 --symbolic true &gt; symbolic_$fold/train/$&#123;i&#125;.txt</span><br><span class="line"></span><br><span class="line">--symbolic true </span><br></pre></td></tr></table></figure>
<h2 id="pytorch">2. pytorch</h2>
<p>论文中总共4个数据集 task 4 task 15 task 16 task 18 只有0.3386</p>
<ul>
<li>main.py
<ul>
<li>grid(opt)
<ul>
<li>main(opt)
<ul>
<li>run(opt)
<ul>
<li>net = GGNN (model.py)</li>
<li>train_loss = train(epoch, train_dataloader, net, criterion,
optimitzer, opt)</li>
<li>test_loss, numerator, denomitor = test(test_dataloader, net,
criterion, optimizer, opt)</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>model.py
<ul>
<li>GGNN
<ul>
<li>incoming and outgoing edge embedding</li>
<li>propogation model</li>
<li>output model</li>
</ul></li>
</ul></li>
</ul>
<p>task4 基本数据集 - edge_types - labels.txt: true or false -
node_ids.txt: - question_types.txt: - graphs.txt - n1 e6 n2 - ? n1 n2 2
数据主要从graphs.txt中提取 graphs.txt中的数据 n_edges_type 2 n_tasks 4
问题的类型 n_node 4</p>
<p>item[1] = {task_type, _, task_output} annotation = np.zeros([n_nodes,
n_annotation_dim]) # [4, 1] annotation[target][0] = 1 &gt;
这里的annotation是怎么设置的？ 这样设置有什么用？</p>
<p>task_data_list[task_type-1].append([edge_list,annotation,task_output])
&gt; 每次只使用一种类型的task_id进行训练</p>
<p>一些提前设置的参数 -D: denmensions 50 -H: hidden layer size 50</p>
<p>然后进行变换的参数是 bAbIDataloder(train_dataset, batch_size,
shuffle, num_workers) batch_size = 1, num_workers = 2</p>
<h3 id="pytorch数据处理的通路">pytorch数据处理的通路</h3>
<p>基于task4</p>
<ol type="1">
<li>原始含义-&gt;graphs.txt
<ul>
<li>单条数据 &gt; 1 1 2: node_1 e_type node_2 fact &gt; 3 1 1 fact &gt;
? 1 1 2: ? qustion_node qustion_type(&lt;=edge_type)
question_target</li>
<li>根据quetsion_type, 每次指示某个task_type进行运行GGNN模型</li>
<li>统计数据
<ul>
<li>n_edges_type 2</li>
<li>n_node 4</li>
<li>n_tasks 4</li>
</ul></li>
<li>返回结果
<ul>
<li>edges: (node_1, edges, node_2)</li>
<li>annotation: [n_nodes, n_annotation_dim] &gt;
这里n_annotation_dim由用户自己指定，这里为1 &gt; question_node对应位为1
&gt; 这里值得注意 n_annotation_dim=2, 结果为<code>[[1, 0, 0, 0], [0, 0,
0, 0]]</code>, 只指示<code>[question_node - 1][0]</code>为1</li>
<li>target: question_target &gt;</li>
</ul></li>
</ul></li>
<li>dataset -&gt; dataloader(batch_size=1, shuffle=True, num_workers=2):
&gt; 猜想是转换为邻接矩阵，或者其他？ 介绍的是 产生multi-process
iterable
<ul>
<li>dataset <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> &lt;class &#x27;list&#x27;&gt;: [[[1, 1, 2], [3, 1, 1]], array([[1.],</span><br><span class="line">[0.],</span><br><span class="line">[0.],</span><br><span class="line">[0.]]), 2]</span><br><span class="line">&lt;class &#x27;list&#x27;&gt;: [[[3, 1, 2], [1, 1, 3]], array([[0.],</span><br><span class="line">[0.],</span><br><span class="line">[1.],</span><br><span class="line">[0.]]), 2]</span><br></pre></td></tr></table></figure></li>
<li>dataloader &gt; num_samples 15??? <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[[0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],</span><br><span class="line"> [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line"> [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],</span><br><span class="line"> [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]</span><br><span class="line"> // [1, 4, 16]</span><br><span class="line"> [[[1.],</span><br><span class="line"> [0.],</span><br><span class="line"> [0.],</span><br><span class="line"> [0.]]]</span><br><span class="line"> </span><br><span class="line"> [1]</span><br></pre></td></tr></table></figure>
<ul>
<li>adj_matrix: (1, 4, 16): [A_in, A_out]</li>
<li>annotation (1, 4, 1) &gt; 这里前面的1是与那个有关</li>
<li>target: 按索引开始排列，-1, 从0开始 (1,)</li>
</ul></li>
<li>train.py对代码使用
<ul>
<li>padding: (n_nodes, n_nodes, opt.state_dim - opt.annotation_dim) (1,
4, 3) &gt; len(): 取的一直都是第一位 &gt; ? 有什么用 &gt; opt.state_dim:
论文中有提到的关于每个节点要设置多少个的东西 &gt;
所以这个是关于前面的补充？？</li>
<li>init_input = (annotation, padding)</li>
<li>output = net(init_input, annotation, adj_matrix) &gt; shape: [1, 4],
估计里面存储的是softmax值
<ul>
<li>GGNN mode: Select node???
<ul>
<li>forward(prop_state, annotation, A)
<ul>
<li>prop_state: (1,4,3)</li>
<li>annotation: (1,4,1)</li>
<li>A: (1, 4, 16) &gt; A的猜测对了，但是in_states,
out_states还没检查</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
<li>main.py中相关参数
<ul>
<li>opt.state_dim: default 4</li>
<li>opt.n_steps: propogation: 4</li>
</ul></li>
</ul></li>
</ol>
<p>pytorch用法小结 torch.cat((x, x, x), 1) torch.stack((x, x, x), 2)
进行数据的封装 transpose(x, 0, 1) 进行转置 &gt;
https://zhuanlan.zhihu.com/p/64551412 &gt; view(1,6)
按照所想的内容进行填充 &gt; view(-1, 1, 2):
如果为-1表示这个位置由其他位置的数字来推断 nn.Linear(): 权重自己给定 ##
torch</p>
<ol type="1">
<li>ggnn.NodeSelectionGGNN(state_dim, annotation_dim, prop_net_h_sizes,
output_net_h_sizes, n_edge_types)
<ul>
<li>state_dim = opt.statedim 4</li>
<li>annotation: opt 1</li>
<li>prop_net_h_sizes: {}</li>
<li>output_net_h_sizes: {state_dim} 4 &gt; 输出的每个节点的大小</li>
</ul></li>
</ol>
<h2 id="something-new">something new</h2>
<ol type="1">
<li>学习方式的问题</li>
</ol>
<p>测试集和训练集分开的。 测试集和训练集</p>
<p>测试集上是全部关于一个图的局部的训练数据。</p>
<p>所以实际上对于每个图的获取方式都不一样</p>
<p>训练数据是原本图上的一部分</p>
<ol start="2" type="1">
<li>edge-type训练还是question_type训练</li>
</ol>
<p>edge-type不对，这里是边类型直接进行处理了</p>
<p>question_id 进行训练</p>
<p>这里question_id实际上是边的type类型，跟之前的理解是一样的</p>
<ol start="3" type="1">
<li><p>sequence 观测点的想法: 对的，刚刚看了下论文，发现确实是这样的！
应该是中间的标注能够知道。</p></li>
<li><p>是否有初始维度之说。 对的，没错！！！</p></li>
<li><p>训练模式 &gt;
这又回到了传统的训练方式，不过是借助了图的模型而已。还是进行一个图形一个图形地进行训练</p></li>
<li><p>跟原始的有什么区别，原始的一大串进行考虑，结果发现怎么根据边来传播，发现这样做是最成功的
## 思考</p></li>
<li><p>普通的机器学习 vs 图神经网络</p></li>
</ol>
<p>普通机器学习过程即学习一个函数，然后有很多数据都是关于这个函数的。
然后，用一个数据会产生loss, 然后根据loss进行反向传播，更新梯度。
然后，不断地用多组数据不停地进行训练。</p>
<p>图神经网络 一部分图的数据，一部分图的标签来模拟常见操作。</p>
<p>不对是一组数据进行学习的过程。</p>
<ol start="2" type="1">
<li>导数更新
对于传统的机器学习。导数更新实际上是由公式算出来的导数，然后导数更新公式上赋值上一堆权重。问题就是如何进行更新。</li>
</ol>
<p>对于一般情况来说，如果是一个训练数据直接使用一个数据进行训练即可。如果是多个训练数据，则loss来进行更新</p>
<ol start="3" type="1">
<li>分析 相比于传统的图数据处理方式，GNN,
GG-NN等模型可以有效地利用图的拓扑结构信息。</li>
</ol>
<p>这些模型在每个传播时间步都需要展开所有的节点，因而这些模型也可以用于处理各种图（包括有向图、无向图、有环图、无环图等）。</p>
<p>而且有以下问题： &gt; 1. 在每个时间步都要展开图中所有节点，效率降低；
&gt; 2. GG-NN将传播时间固定，可能只会得到有效的信息。</p>
<ol start="4" type="1">
<li>总结 &gt;
实际是每步即根据出度和入度矩阵进行标签传播，然后每一步都是在图上，从一个节点走到另外的节点。但是这样图上的信息传输也因此受到了约束。
&gt;
而且只是对特定的点给予信息，这意味着只能这些点的信息进行向前传播</li>
</ol>
<h2 id="总结">10.28总结</h2>
<p>对GGNN总结主要如下： 所谓的A_in,
实际上就是每个节点新一轮的表示是按照当前边的in边，将in边的节点的值进行加和，从而得到最终的结果。
&gt; 详细可见矩阵相乘的运算</p>
<p>关于训练集和验证集、测试集的思考：
经过实践发现，其实训练集和验证集是一直用的。 然后</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/" class="post-title-link" itemprop="url">gnn-parallel-ggsnn-paper-read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-10 21:16:00" itemprop="dateCreated datePublished" datetime="2019-10-10T21:16:00+08:00">2019-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="摘要">摘要</h2>
<p>图结构广泛出现在化学、自然语言语义、社交网络和知识结构中。
在这篇工作中，我们学习了关于图结构输入的特征学习方法。
我们开始的起点是2009年的GNN的工作，我们使用gated recurrent
units和现代化优化手段，并且扩展到输出序列。
结果是灵活的并且广泛对于神经网络，结果相对于纯粹基于序列的模型具有很好的归纳偏差?，尤其是图结构的模型。(LSTM)
&gt; https://zhuanlan.zhihu.com/p/38740843,
归纳偏差简单的理解就是模型的偏好是什么？
我们描述了模型在bAbI任务上的能力和图的算法学习任务。接下来，我们展示了程序验证中的问题的应用效果，其中子图可以抽象为数据结构</p>
<h2 id="introduction">1. Introduction</h2>
<p>很多现实的图结构任务。
我们想在学习任务中将图作为输入。标准的方法对于问题来说包括对于一个输入图的工程自定义特征，
graph kernel. 方法用来定义图的特征通过在图上random walks，
更多的靠近我们目标的是先从图中进行学习特征中能包括GNN, spectral
networks以及在图足迹分类，化学分子表示上的方法</p>
<p>我们的主要贡献在于关于GNN在输出sequences上。之前在特征学习的工作主要集中在产生图的单个输出，比如grah-level分类任务，但是很多关于图的许多问题需要输出sequences。
例子包括在图上的路径，具有理想节点的枚举，以及图分类任务的序列。比如， a
start and end node. 我们不确定现有的工作是否能够很好的解决这个问题
我们的动机来自于程序验证，需要输出logical formulas，
我们把它形式化为一个序列化输出问题。
第二个贡献是GNN是一个广阔的神经网络模型可以应用再现阶段的很多领域</p>
<p>这里有两种关于图的特征学习的设定： 1. 学习关于输入图的表示 2.
学习在产生序列输出的interal state的表示</p>
<p>其中，1主要在GNN的工作里面。我们对这个框架做了调整，比如使用modern
practices 有关Recurrent Neural Networks.
2是我们期望从图结构问题中输出的，这不仅仅是individual classfication.
因此，这要的挑战就在于如何在图中学习特征，并且encode
已经产生的partial输出序列（比如path so far if outputting a
path），以及还需要被产生的（the remaining path）。 我们同时也展示了GNN
framework can be adapted to these settings,
从而获得一个新的关于基于图的神经网络模型，GGS-NNs</p>
<p>我们阐述了这个模型在bAbI tasks和图算法，在学习模型的能力上。
然后我们展示关于程序验证的应用。
当需要试图验证内存安全是，一个核心的问题是如何发现关于程序中运用的关于数据结构的数学描述。
我们 phrased这个机器学习任务，在我们需要map from a set of input graphs,
代表运用的内存，用来获取关于是咧的数据结构的logical descripton.
另外有一篇论文依赖于大量的手写工程的特征，我们展示了可以使用GGS-NN来替代该系统。</p>
<h2 id="graph-neural-networks">2. Graph Neural Networks</h2>
<p>在这段中，我们review GNNs, 并且介绍了整个过程用到的notation and
conecpts</p>
<p>GNNs 是一个普遍的神经网络结构根据图来定义， G=(V, E), 其中v为Nodes,
e为edges. 我们主要集中在directed graph上，所以(v, v')表示一个directed
edge, 但是我们发现这个框架可以easily to adapyed to undirected graphs,
see Scarselli et al.(2009). The node vector(node representation or node
embedding) for node v 是<span class="math inline">\(h_v\)</span>,
图也可以包含关于摸个节点的标签, edge labels <span
class="math inline">\(h_S\)</span>中S是a set of nodes <span
class="math inline">\(l_S\)</span>中S是a set of edges <span
class="math inline">\(IN(v)\)</span>表示v节点的祖先 <span
class="math inline">\(OUT(v)\)</span>表示v节点的后继 <span
class="math inline">\(NBR(v)=IN(v) U OUT(V)\)</span>表示v的邻居 <span
class="math inline">\(Co(v)\)</span> 所有经过顶点v的边</p>
<p>GNNs将graphs map到outputs通过两步： 1. propagation
step计算每个节点的表示 2. <span class="math inline">\(o_v =
g(\mathbf{h}_v, l_v)\)</span>
从node的表示和对应的labels中学习，来获取关于每个节点的一个输出。
在对于g的notation，
我们留下了关于参数的隐式的依赖，然后我们继续做这件事。 this
system是differentiable from end-to-end,
所以所有参数都可以通过gradient-based optimization学习</p>
<h3 id="propagation-model">2.1 Propagation Model</h3>
<p>在这里是关于一个迭代过程传播节点的表示。初始化节点的表示<span
class="math inline">\(h_v\)</span>可以设置为arbitary
values(任意的值)，然后node
representation根据下面的节点表示进行更新，直到convergence.</p>
<p><span class="math display">\[h_v^{(t)} = f^*(l_v, l_{Co_{(v)}},
l_{NBR(v)}, h_{NBR(v)}^{(t-1)})\]</span>
很多变量被提及，所以Scarselli建议decompsing函数f为一个关于每个对应的变得相加</p>
<p><span class="math display">\[f(l_v, l_{(v&#39;,v), l_{v&#39;},
h_{v&#39;}^{(t)}}) = A h_{v&#39;}^{(t-1)}+b\]</span></p>
<h3 id="output-model-and-learning">2.2 Output Model and Learning</h3>
<p>输出模型定义在每个节点上， g函数是可微的，可以maps到输出。
这仅仅是一个linear or neural network mapping.
在这个模型中每个节点将获得一个最终的表示。
如果是为了处理graph-level分类任务，他们的建议是使用一个dummy 'supper
node, which is connected to all other nodes by a special type of edge."
学习过程 ALmeida-Pineda算法已经完成
然后基于梯度的计算直到收敛解决最终问题。 评价： 1. advantage:
不需要存储中间状态来计算梯度。 2. disadvantage: 参数必须首先的，
所以传播步骤是一个收缩map。这需要来确保收敛，因为它可能限制模型的表达能力。</p>
<p>鼓励使用一个关于1-norm的Jacobian作为惩罚项。 Appendix
A是一个例子，给出了收缩银蛇的直觉难以在图中长时间传播信息</p>
<h2 id="gated-graph-neural-networks">3. Gated Graph Neural Networks</h2>
<p>我们提出了GG-NNs, 适用于non-sequential puputs.
关于GNNs的最大调整是我们使用了Gated Recurrent Units，并且unroll(展开)
一个循环的重复的神经步，使用backpropagation 来计算梯度。
这需要更多的内存相比于GNN算法，但是不需要限制参数来确保收敛convergence.
我们同时扩展了隐藏层的表示和输出模型</p>
<h3 id="node-annatations">3.1 Node Annatations</h3>
<p>在GNNs中， 这里没有明显的点来初始化节点的表示因为收缩图map
constraint来确保，者使得我们不需要与其他的节点label作为额外的输出。为了分辨这些节点用作输入和之前介绍的节点。
node annotations 并且使用向量x来denote这些表示。
为了产生他们怎么用，考一个简单的任务，训练图神经网络来预测接地那t如何到达节点s在一个给定的图上。在这个问题中有来哥哥问题相关的
节点，s 和t， [1,0] [0,1]. 在可到达的例子中，
可简单来看春波模型如何来传播节点的annotation(注解)，使得它们的第一位变为1.
output step
classifier可以轻易地辨别nodel是如何从s到达t的通过查看安歇节点有非零的实体在前两维上
&gt; 这个过程好像标签传播</p>
<h3 id="propagation-model-前向传播模型">3.2 Propagation Model
前向传播模型</h3>
<p><img data-src="gnn-parallel-ggsnn-paper-read/ggnn-equations.png" /></p>
<p>MatrixA决定了节点在图的communicate中如何进行交流。
稀疏的结构和参数在A中图图1所示。 eq 1是初始化步骤，copy node
annotions到隐藏层的第一位，并把剩余维度清零。 eq
2是在不同节点间通过出度和入度的边来春波参数，这些参数独立于边的种类和方向。
<span class="math inline">\(a_v^{(t)}\)</span> &gt; ?
输入x是什么？是对应的邻接矩阵信息吗? 注意维度，这里是二维的</p>
<p>剩余的就是GRU的部分 https://zhuanlan.zhihu.com/p/32481747</p>
<p>https://zh.d2l.ai/chapter_recurrent-neural-networks/gru.html</p>
<p><span class="math inline">\(z_t\)</span>用于门控的更新， <span
class="math inline">\(r_t\)</span>用于门控的重置</p>
<p><span class="math inline">\(h^{(t-1)} = h^{(t-1)} . r\)</span> &gt;
理解，<span class="math inline">\(H_t\)</span>是更新门，
最终结果是对上一时间步的隐藏层和当前时间步的候选隐藏状态<span
class="math inline">\(H_t\)</span>做组合 <span class="math inline">\(Z_t
* H_{t-1}\)</span> 表示对上一层的遗忘，忘记上一层的某些东西 <span
class="math inline">\((1-Z_t) *
H_{t}\)</span>表示对当前候选隐藏层的状态进行记忆。 这里z是门控喜好，
门控信号越接近1表示记忆得越多，越接近0表示遗忘得越多？？ &gt;
需要看乘法是怎么做的？</p>
<blockquote>
<p>不要管z和r怎么看，都是有w的超参数，实际上就进行理解为z是用来学习分配更新的，r是用来产生中间隐藏候选状态的中间变量</p>
</blockquote>
<p>结果表示GRU-like 前向传播步骤更有效果</p>
<h3 id="output-models">3.3 Output Models</h3>
<p>这里有one-step outputs，我们可以用来产生各种的情形。首先GG-NNs node
selection任务为门戈节点的输出进行打分，并且因公softmax在node 分数上 &gt;
意思是node selection其实它会中间输出表示，按理说维度应该和类别一样。
&gt; 其实这里就是中间观察的结果</p>
<p>对于graph-level outputs,我们的定义如下</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/eq7.png" /></p>
<blockquote>
<p>这里<span class="math inline">\(\delta\)</span>使用的是soft
attention机制? 来决定是否哪些节点与当前的tgraph-level任务相关。
i和j是神经网络。使用<span
class="math inline">\(h_v^{(T)}\)</span>和<span
class="math inline">\(x(v)\)</span>$作为input和ouputs的real-valued
vectors. tanh可以被替代
这里说的就是如何产生中间观察结果，记得之前有学习过一个模型！ 不理解？ ##
4. Gated Graph Sequence Neural Networks 在这里GGSNNs</p>
</blockquote>
<p>对于k-th 输出步骤，我们描述了node annotations as <span
class="math inline">\(X^{(k)}\)</span>, 我们用了两个 GG-NNs <span
class="math inline">\(F_o^k\)</span>和<span
class="math inline">\(F_x^k\)</span>来从<span
class="math inline">\(X\)</span>中预测o. 以及<span
class="math inline">\(F_X\)</span>用来从<span
class="math inline">\(X_k\)</span>预测<span
class="math inline">\(X_{k+1}\)</span>,
两个都是包括了一个前向模型和输出模型。在前向模型中，我们使用了节点向量作为t层的前向步骤和k层的输出步骤。
在之前的每一步，我们设置 &gt; 这两个模型一个输出x， 一个用来输出o；
在之前我们把<span
class="math inline">\(H^{k-1}\)</span>初始化为0-extending的<span
class="math inline">\(X_{k}\)</span>.
这种简单的变体是能够很快用来训练和评估的，在很多case种，整个模型可以达到相同的表现。但是在某些case中，两个的表现效果不一样，所以这种变体不能work得很好
&gt; ? 为什么 我们介绍了node annotation ouput用来从H预测X。
这个预测对于每个节点都可以很简单地用一个神经网络j来连接h和x作为输入和输出，从而得到一个真实的分数
&gt; ?</p>
<p>这里有两种关于GGS-NNs的设定： specifying all intermediate annotation
Xk, 或者训练所有模型，仅仅给X1, graphs and target sequence.
前者可以提高表现力，但我们有邻居只是在知道节点的信息在哪些 &gt;
取得节点达摩鞋特征。 第二种更为普遍</p>
<ol type="1">
<li>Sequence outputs with observed annotations 考虑任务来做整个，
图的序列预测，每个预测只是包括图的一笑部分。为了确定每个部分都有一个输出，每个节点只有一位就过来，表示我们已经解释了。
&gt; 子图训练？？</li>
</ol>
<p>在某些设定中，少量的annotations足够来预测。我们可以扩展多个模型，这些模型都有注释地可做
&gt; one idea, 可以用不同的邻域，来结合多个相同模型！！！
还有模型的组合</p>
<p>单步domain预测，这实际上和所有是一样的 2. Sequence outputs with
latent annotations 一般地，当中间不可用时，作为隐藏层，后向传播训练</p>
<h2 id="explanatory-applications">5. Explanatory Applications</h2>
<h3 id="babi-tasks">5.1 bAbI Tasks</h3>
<p>这里我们简述了如何使用GGS-NNs. 我们主要集中在bAbI任务上。</p>
<p>bAbI tasks是Facebook提出的一个AI任务集，
第一组发布的bAbI包括20个任务。若要查看这些任务的详细信息， 请移步<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05698">bAbI tasks</a>.
在bAbI数据集中，每一篇文本对应于一个故事。本文将每个故事映射成了一个简单的图（节点对应于实体，边对应于关系）。
每个问题(eval)由问题类型（类似于谓词，如"has_fear")与一些参数（对应于图中一个或多个节点）构成。当任务具有多种问题类型时，需要对每种类型都训练一个独立的模型。在本文中，仅处理类型未二元一阶谓词的问题。</p>
<p>例如，对于问题“eval E&gt;A true”, 那么问题的类型为"&gt;",
图中的节点E将会被（初始化）标注为<span class="math inline">\(x_E = [1,
0]^T\)</span>, 节点A将会被标注为<span class="math inline">\(x_A=[0,
1]^T\)</span>, 其他节点将会被标注为<span class="math inline">\([0,
0]^T\)</span></p>
<h4 id="单输出任务">单输出任务</h4>
<p>单输出任务实验基于bAbI的四个任务： Task4: Two Argument
Relations(选用D=4)； Task15: Basic Deduction(D=5); Task16: Basic
Induction(D=6); Task 18: Size Reasoning(D=3) &gt; D是什么？</p>
<p>在bAbI任务集的四个任务上的实验结果为（括号内为使用的训练样本数，下同）</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/bAbI_res.png" /></p>
<h4 id="序列输出任务">序列输出任务</h4>
<p>序列输出任务基于bAbI Task19: Path Finding(D=6). 其实验结果如下：</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/bAbI_res2.png" /></p>
<h3 id="learning-graph-algorithms">5.2 Learning Graph Algorithms</h3>
<h2 id="程序验证问题-ggs-nns">6 程序验证问题 GGS-NNs</h2>
<p>在GGS-NNs的工作主要来自于实际program verification. 在automatic
program verification中最关键的一步就是program invariants的推论，
它用来近似评估在一次execution中一些程序状态是否可达。
发现数据结构中的不变量是一个open problem. 在这里，C函数为例
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">node* concat(node* a, node* b) &#123;</span><br><span class="line">    if (a==NULL)  return b;</span><br><span class="line">    node* cur=a;</span><br><span class="line">    while (cur.next != NULL)</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    cur-&gt;next = b;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 为了证明这个问题two lists
a和b确实是concatnates连接的，所有指针取消引用都是合法的。我们需要characterize程序在每一次迭代的循环中的help。
在这里我们用了separation logic(分隔的逻辑符号)， 它使用inductive
predicates归纳谓词来描述, 抽象数据结构。 举个例子，
一个分段的list定义为todo1, todo2
意味着x指向一个内存区域，它包括了数据结构有两个域， val和next，
它们的值保存在v和n中， * connective是合并的布尔连接法，
但是额外需要它操作指导"separate" parts of the heap. ls(cur, NULL)
于是这cur要么是null,要么指向heap中的两个值v、n, 这里n用ls描述。
公式todo3 是一个循环的不变式。
使用它，我们可以证明，不会失败，因为不会引用一块未分配的内存地址。
这个函数进一步使用Hoare-style验证方式的确连接了两个lists.
关于整个流程最困难的地方在于产生描述数据结构的公式。在这里我们使用到机器学习。在给定的程序中，我们跑了很多次并且提炼了内存的状态（重新表示为graph)在相关程序位置，接着预测了分离逻辑公式。静态的程序分析工具可以检查一个候选的公式是否是充足地去证明它需要的属性。
&gt; 这里考虑的是使用Machine Learning来描述data struture,
然后预测相关的separation logic formula??? 怎么能保证正确性啊？
惊奇！！！</p>
<blockquote>
<p>Input这里边上的权重0表示什么意思？ ### 6.1 Formalization</p>
</blockquote>
<ol type="1">
<li>Representing heap state as a graph:<br />
作为输入，我们考虑有向图，可能是循环图来表示一个程序的heap.
这些图可以自动地构建程序的对状态。
每个图的节点都对应着内存中一块内存地址，在这块地址中，v0, ...,
vk已经存储。 图的边反映了指针值，即v具有标记为0, ..., k
分别指向了节点v0, ..., vk &gt; ?? node 和 edge 还需要再理解。
实际上只指向一块地址，但是node可能多个地方对这块地址有引用？
地址一块，一块链接存储，像链表一样，每个节点都对应独特的一块内存空间？
&gt; 边的属性表示什么意思？</li>
</ol>
<p>A subset of nodes are labeled as corresponding to program
variables.</p>
<p>一个输入图的例子as "Input" in Fig.3. In it, 节点展示在节点里面。node
id(i.e., memory address), 边的labels表示特殊的域？，e.g. 0对应于previous
section中的下一个指针。 对于binary trees, 这里有两种类型的指针left,
right分别指向树节点的left和right节点。 2. Output Representation
我们的目标是数学表示heap的形状。在我们的模型中，我们限制了分离限制的语法版，
formulas的形式是，<span class="math inline">\(\exists x_1, ..., x_n, a_1
* ... * a_m\)</span>， 这里每个atomic formula <span
class="math inline">\(a_i\)</span>或者ls(x, y), tree(x), 或者none(x)(no
data struture at x).
存在量词是用于给描述形状所需的堆节点命名，而不给一个程序的变量命名。
举例， 为了描述“人手清单”（a list that ends in a cycle）, 第一个list
element需要被描述。 在separation logic, 被表示为 <span
class="math inline">\(\exists t. ls(x, t) * ls(t, t)\)</span></p>
<ol start="3" type="1">
<li>Data
我们为了这个问题产生合成的（标记的）数据集。自此，我们还修复了一些谓词，比如ls和tree(扩展名可以考虑双向链标的列表段，多树)以及它们的递归定义。然后，我们列举了实例化分离逻辑公式我们的谓词使用的程序变量集
&gt; 程序变量集，变量v, address</li>
</ol>
<p>最后，对于每个公式，我们列举满足该公式的堆图，结果是一个数据集包括我们heap
graphs and associated formulas that are used by our learning
procedures.</p>
<h3 id="formulation-as-ggs-nns">6.2 Formulation As GGS-NNs</h3>
<p>通过数据迭代产生的过程来获得节点的中间表示。所以，我们训练一个不变量GGS-NN使用观察的annotations(observed
at training time) &gt; 这里关注的点是什么？为什么要这么做</p>
<p>使用un-obeserved GGSNN
variant来做end-to-end学。这个过程将会使得separation logic formula拆分为
a sequence of steps.
我们首先决定了来宣称了哪个必要的变量，如果可以的话，选择对应的节点来表示变量。<code>??</code>
一旦我们有了宣称的必要的变量，我们迭代所有变量的名字，然后产生一个separation
logic formula以当前变量所对应的的节点未根来描述数据结构。
完整的算法如下所示。</p>
<p>我们使用了三个明显的node annotation部分，namely is-named(heap node
labeled by program variable or declared existentially quantified
variable(量化的))， 被命名的， active(cf. algorithm) 和
is-explained(heap node is part of data struture already predicted).
初始化的节点标记可以直接从input graph中计算得到。
is-named是对于程序变量的编辑。 active和is-explained总是off.</p>
<p>评论的行在扩展中都使用的是GG-NN。 Alg.1 是一个GGS-NN model的instance.
一个简要的关于算法运行的一开始的算法在Fig3,
每一个斗鱼算法的每一行相关</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/alg1.png" /> &gt; Separation
logic formula prediction procedure <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Input:  Heap graph G with named program variables.</span><br><span class="line">X: compute initial labels from G    // 做了些什么</span><br><span class="line">H: initialize node vectors by 0-extending X</span><br><span class="line"></span><br><span class="line">while exists quantifier needed do // 当需要存在量词的时候  Graph-level Classification</span><br><span class="line">    t: fresh variable name</span><br><span class="line">    v: pick node    // Node selection.</span><br><span class="line">    X: turn on &quot;is-named&quot; for in X</span><br><span class="line">    print &quot;exists t&quot;  // 不懂什么意思</span><br><span class="line">end while</span><br><span class="line"></span><br><span class="line">for node v_l with abel &quot;is-named&quot; in X do</span><br><span class="line">    H: intialize node vectors, turn on &quot;active&quot; label for v_l in X</span><br><span class="line">    pred: pick data structure predicate // Graph-level  Graph-level Classification 好抽象， 随便来抽取数据结构还是怎么会使</span><br><span class="line">    if pred = ls then</span><br><span class="line">        l_&#123;end&#125;: pick list end node  // Node selection</span><br><span class="line">        print &quot;ls(l, l_&#123;end&#125;)  // 这里的print实际上就是输出上面想要分析的表达式</span><br><span class="line">    else </span><br><span class="line">        print &quot;pred(l)*&quot;  // 没有tree等其他情况，等于说</span><br><span class="line">    end if</span><br><span class="line">    X: update node annoatations in X  // Node Annotation</span><br><span class="line">end for</span><br></pre></td></tr></table></figure> &gt;
所以是这里每一步的评论都会用到GGS-NN模型？ &gt; is-named, active,
is-explained等于说是变量的三个状态，
于是heap中的变量的状态就是？，不过依然看不懂fig3</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/fig3.png" /></p>
<h3 id="model-setup-details">6.3 Model Setup Details</h3>
<p>我么使用了full GGS-NN model， <span
class="math inline">\(F_o^k\)</span> 和 <span
class="math inline">\(F_X^k\)</span>有着separate propagation models.
对于所有GG-NN成分，在GGS-NN pipeline中的， 我们展开传播过程为10 time
steps. GGS-NNs与step(|)(决定是否更多存在两次变量需要声明) 和 step(||)
(辨别哪些节点需要声明为存在量词)， 通过使用D=16维的节点表示。 对于all
other GGS-NN成分， D=8使用，Adam优化器用来优化，模型被训练在20
graphs的minibatches， 优化直到训练错误率非常低。
对于图级别的分类任务，我们认为平衡地分类，每一小皮量中甚至都有even
number of examples. 所有的GGS-NN成分包含了少于5k paras, 并且 no
overfitting被观察到，在训练过程中。</p>
<h3 id="batch-predication-details">6.4 Batch Predication Details</h3>
<p>在实际中，一系哦啊部分heap graphs将会被给与input and a single output
formula is expected to describe, 并且与所有的input graphs一直。
不同的heap graphs可以是程序执行过程中不同状态点，
或者运行在相同程序的不同输入。 我们称这个"batch prediction" setup
与描述在主要的paper中的single graph prediction相互对比。</p>
<p>为了得到batch predictions, 我们运行一次GGS-NN对于each graph同时的。
For each prediction step, 所有GGS-NNs的outputs at the step across the
batch of graph被聚集。</p>
<p>对于node selection outputs, 普遍的命名为variables link
nodes在不同的图上，它是聚集prediction in a
batch的关键。我们计算particular named variable t as <span
class="math inline">\(o_t = \sum_g o^g_{V_g(t)}\)</span>, 其中<span
class="math inline">\(V_g(t)\)</span>maps 变量t到图中的一个节点。 <span
class="math inline">\(o^g_{V_g(t)}\)</span>是named variable t in graph
g中的 output score. 当使用一个softmax对于所有names using <span
class="math inline">\(o_t\)</span> as scores, 这个与计算<span
class="math inline">\(p(toselect=t)=II_g
p_g(toselect=V_g(t))\)</span>相同。</p>
<p>对于graph-level 分类输出，我们增加一个特别的类的scores across the
batch of graphs., 或者相等的计算<span
class="math inline">\(p(class=k)=II_gp_g(class=k)\)</span>, Node
annoation outputs作为不同的图在每个图中独立地被更新，有着完全不同的set
of nodes. 然后，当算法尝试更新annotation of one named variable, the node
相关联的所有节点也被更新。在训练中， 所有标记 intermediate
steps对于我们是可获得的，从data generation process, 所以training
process能够在一次的降解到single output single graph training.
一个更complex的情景允许nested（嵌套的）数据结构(list of lists)被讨论。
我们也成功地扩展了GGS-NN model到这个例子，更多的细节在Appendix C.</p>
<h3 id="experiments">6.5 Experiments</h3>
<p>在这篇文章中，我们产生了327 formulas的数据集，包含了三个程序变量，
498 graphs per formula, 产生了大约160,000 formula/heap graph
combinations.
为了评估，我们分隔数据到训练集、验证集和测试集按6:2:2的比例(测试集不会用在训练集上)。我们测试了准确率，是否formula预测在测试时间在逻辑上等于基本事实
等价通过规范化来近似公式的名称和顺序，然后进行比较以求完全相等。</p>
<p>我么比较了我们的GGS-NN 模型和Brockschmidt中提到的方法。
更早的方法把每一个验证不都对待为一个标准的分类任务，并且要求complex,
manual, problem-specific feature engineering,
来达到89.11%的准确率。作为对比，我们的新模型no feature
engineering和少量的domain knowledge来达到了89.96%的准确率。</p>
<p>一个heap graph 的例子和对应的logic formula从我们GGS-NN
model中发现的如fig4所示。 <img
src="gnn-parallel-ggsnn-paper-read/fig4.png" /> &gt;
比较了两个命名变量arg1, arg2. one isolated NULL node, nodel 1.
所有的边指向NULL没有进行显示。
边上的数字暗示了不同的边的类型。我们的GGS-NN模型成功地找到了正确的formul</p>
<p>这个例子还包括了nested data structures和the batching
extension在之前的section中提到的。</p>
<p>我们还成功的用我们的模型在程序验证的framework,
提供所需将定理证明给定理证明这，以证明列表操作集合的正确性比如插入排序等算法。
一下table
4展示了一组基准列表操作程序和由GGS-NN模型找到的分离逻辑公式不等式，分别是在验证框架中成功使用以证明相应程序的正确性。</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/table4.png" />
一个未来的扩展已经展示成功的证明了更复杂的程序比如排序程序，以及各种l其他list操作的程序。</p>
<h2 id="相关工作">7. 相关工作</h2>
<p>最相关的工作是GNNs, 我们之前讨论过。
Micheli建议了另一个紧密相关的模型，它不同于GNNs，主要在输出上。
GNNs被应用在许多领域，但是他们没有广阔地在ICLR社区传播。我么的部分目的publicize
GNNs是一个useful and interesting neural network variant.</p>
<p>一个比喻可以秒速我们从GNNs到GG-NNs的掉成， the work of Domke and
Stoyanov在strutured prediction setting.
这里的信念传播（这里必须运行在能够convergence,
以取得好的梯度）被替代为truncated(截断的)信念传播更新。然后这个模型被训练，所以truncated
iteration produce good results在一些迭代次数后。同样地，RNN扩展到了Tree
LSTM也相似与我们使用GRU来更新GG-NNs而不是使用标准的GNN，目的在于提高长时间的信息在graph
struture中的前向传播。 the general
idea表现在paper中的是组装特定问题的神经网络学习成分已经有一个很长的历史，可以追溯到the
work of Hinton， 用神经网络来根据家族数结构预测人与人之间的关系。 Graph
kernel能够被用于一系列的kernel-based learning tasks使用graph-strutured
inputs， 但是我们不知道learn the kernel 和outputs sequences.
Perozzi转化graph为sequences通过following random walks在图上，学习node
embedding 使用sequence-based的方法。 Sperduti map graphs to graph
vectors 然后output neural
network来分类这里有多种模型使用同样的节点表示的前向传播在图结构上。
他们的工作和GNNS的不同点在于卷积和recurrent networks.
Duvenaud认为convolutional想在图上的操作，建立一个可学习的，可微的关于graph
feature的变体。
LUsci转换认为的无向图为一定数量的不同的DAGs，然后向内传播节点表示每个根，训练一组模型。在以上所有内容上，重点都放在了一步式问题上。</p>
<p>GNNs和我们扩展具有滋镇网络的许多相同的理想属性。使用节点选择输出层，可以选择输入中的及诶按作为输出。
有两个区别：
首先，在GNN中，图结构是显式的，这使得模型通用性较低，但可以提供更强的泛化能力；第二，指针网络要求每个节点都有属性（比如,
空间中的位置），而GNN可以表示已定义的节点仅通过他们在图形中的位置即可，这使它们在不同维度上更具有通用性。
GGS-NN与软对齐和注意力名有关。有两个方面，第一，eg7中，等式的图形表示，
使用上下文将注意力集中在哪些节点对当前决策很重要；第二，节点程序验证例子中的注释跟踪已注释的哪些节点，因此，它地公馆一种明确的机制来确保输入中的每个节点都已经使用在产生输出的顺序上。</p>
<h2 id="discussion">8. Discussion</h2>
<ol type="1">
<li>What is being learned?
有启发的去思考GG-NNs能学到什么。为此，我们可以在bAbI任务的方式之间类比,
bAbI task15将会通过logical
formulation得到解决。作为一个例子，考虑右边的一个例子中需要回答的行。</li>
</ol>
<p>为了做逻辑推断，我们需要的不仅仅是关于故事中事实的逻辑编码，而且，关于世界的背景知识也编码为一个inference
rules</p>
<p>我们对任务的编码简化了将故事解析为图形形式的过程，但没有提供任何背景知识，可以将GG-NN模型视为学习此方法的结果存储在神经网络权重中。</p>
<ol start="2" type="1">
<li>Discussion
论文的结果表明，GGS-NN在整个过程中都具有理想的感应偏差，一系列具有固有图结构的问题，我们相信在很多问题，在更多情况下，GGS-NN将很有用。但是，有一些限制需要克服这些障碍，使其广泛地应用。我们前面提到的两个显示是bAbI任务翻译未包含输入的时间顺序或三进制或更高解关系。我们可以想象一下接触这些限制的可能性，例如将一个座位会议论文在ICLR2016上发布的一系列GG-NN,
每个边缘有一个GG-NN,
代表更高阶的关系，作为因子图。一个更大的挑战是如何处理结构化的输入表示形式。例如，在bAbI任务中，最好不要使用输入的符号形式。
一种可能的方法是在我们的GGS-NN中合并结构较少的输入和潜在向量。但是，需要进行实验以找到解决这些问题的最佳方法
## Analysis</li>
</ol>
<p>相比于传统的图数据处理方式，GNN、
GG-NN、GGS-NN等模型可以有效地利用图的拓扑结构信息，这也是为什么GGS-NN可以在Path
Finding任务中取得优异效果的原因。另外，由于这些模型在每个实践部都需要展开所有的节点，因而这些模型也可以用于处理各种图（包括有向图、无向图、有环图、无环图等）。但是，这同样也带来了一些问题：
1.
由于在每个实践部都需要展开所有的节点，每个节点还需要使用D维向量进行表示；当图很大且向量表示很大时，模型的效率问题就会变得很重要
2.
在GNN中需要保证凸的整体映射是一个压缩映射，这显然就减小了该模型所能建模的问题空间???(好像的要求时对于寻找压缩映射问题而言的)。为了解决这一问题，GG-NN将传播时间步固定为T,
虽然不需要保证收敛，但是，图上的信息传输却也因此受到了约束(???，信息的传输与时间步有关，因为终止条件不再是收敛，而是具体的时间步)。例如，在可达性预测任务中，在两个时间步后节点1的信息才可以到达节点4.虽然本文中采取了一定的措施来缓解这一问题（即不仅定义了沿边方向的转移举证，还定义了与边相反方向的转移矩阵）。但是只要T的大小有限，节点之间的信息就只能沿着路径传播T步，而不能像GNN那样到模型收敛才停止。换句话说，GG-NN实际上是以损失图中较长路径信息的代价换取了模型可建模的问题空间。</p>
<h2 id="参考文献">参考文献</h2>
<p>https://zhuanlan.zhihu.com/p/28170197</p>
<p>jianshu.com/p/40362662014a
https://github.com/microsoft/tf-gnn-samples</p>
<p>cnblogs.com/wacc/p/5341670.html
https://zhuanlan.zhihu.com/p/38051458</p>
<h2 id="阅读思考">阅读思考</h2>
<p>有空再阅读一遍，通过阅读发现，这里的图神经网络实际上node-selection,以及graph-level,
都是graph-level级别的任务，即得到每个节点的表示，然后由节点进行预测。</p>
<p>该论文和上篇semi-gnn形成鲜明对比 1. 上篇论文是inductive learning,
这篇是transactive learning. 2. 上篇是node-level
分类任务，这篇是graph-level任务，即node-selection.</p>
<p>好像是为了偷懒，所以才用三维，把对应的给空着</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/linux-keyboard-using-e7578316642f/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/linux-keyboard-using-e7578316642f/" class="post-title-link" itemprop="url">linux keyboard using</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-09 06:29:04" itemprop="dateCreated datePublished" datetime="2019-10-09T06:29:04+08:00">2019-10-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>161</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>快捷键： hot key</p>
<p>https://www.cnblogs.com/EasonJim/p/7119214.html</p>
<p>如何查看？ 在桌面空白位置长按Win</p>
<p>如何屏幕最左化 ctrl+win+-&gt;<br />
&gt; 以下是针对Ubuntu的快捷键，测试没有效果。
进一步，通常可以在系统设置-&gt; 键盘-&gt;
快捷键中对相应的设置进行查看</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/lua-learning-b6b3f852624d/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/lua-learning-b6b3f852624d/" class="post-title-link" itemprop="url">lua learning</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-02 23:58:53" itemprop="dateCreated datePublished" datetime="2019-10-02T23:58:53+08:00">2019-10-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>1.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://www.runoob.com/lua/lua-miscellaneous-operator.html</p>
<ol type="1">
<li>data struture:
<ul>
<li>nil</li>
<li>boolean</li>
<li>number: 双精度的实浮点数</li>
<li>string</li>
<li>funcion: 由C或lua编写的函数</li>
<li>userdata: 任意存储在变量中的C数据解雇</li>
<li>thread: 表示执行的独立线路，用于执行协同程序</li>
<li>table: 关联数据 &gt; type() 查看类型</li>
</ul></li>
<li>variable
<ul>
<li>global</li>
<li>local</li>
<li>表中的索引 A.b, A[b], gettable_event(t, i)</li>
</ul></li>
<li>loop
<ol type="1">
<li>while (true)  do  执行体  end</li>
<li>do () while() end</li>
<li>for
<ul>
<li>数值 &gt; for var=exp1, exp2, exp3 do  执行体  end</li>
<li>泛型循环 相当于foreach &gt; for i, v in ipairs(a) do  print(i, v)
 end &gt; for k, v in pairs(table) do  print(k, v)  end</li>
</ul></li>
</ol></li>
<li>if: process control
<ul>
<li>if(cond) then  true-exp  end</li>
<li>if(cond) then  true-exp  else  false-exp  end</li>
<li>if(cond1) the  true1-exp  elseif(cond2)  true2-exp  else  else-exp
 end</li>
</ul></li>
<li>function
<ul>
<li>定义 &gt; <code>&lt;scope&gt; function &lt;function-name&gt; (arg1,
arg2, ..., argn) \ &lt;function-body&gt; \ return
result_params_comma_separated \ end</code></li>
<li>usage
<ul>
<li>add(...) 三点表示可变参数</li>
</ul></li>
</ul></li>
<li>运算符
<ul>
<li>+, -, *, /, %, ^, -</li>
<li>==, ~=, &gt;, &gt;=, &lt;=</li>
<li>and, or, not</li>
<li>a..b, #a</li>
</ul></li>
<li>string &gt; 以下均是静态方法
<ul>
<li>uppper</li>
<li>lower</li>
<li>gsub(mainstr, findstr, replacestr, num)</li>
<li>find(str, substr, [init, [end]]): index</li>
<li>reverse</li>
<li>format("the value is:%d", 4)
<ul>
<li>%c, %d, %f, %s</li>
<li>[符号][占位符][对齐标志-左对齐][宽度数值][小数位数] &gt; more:
https://www.runoob.com/lua/lua-strings.html</li>
</ul></li>
<li>char(arg): convert other type to char type to concat</li>
<li>byte(arg)</li>
<li>len(str)</li>
<li>rep(str, n): 返回n个拷贝</li>
<li>gmatch(str, re_pattern): return iterator</li>
<li>match(str, re_pattern): return the first match</li>
</ul></li>
</ol>
<p>? lua have not go to definition??</p>
<p>th TREPL都充满了方便的特性: 1. tab不全 2. 历史 history 3. 打印 print
4. eval() 自动打印 5. 自助:? 6. shell命令: $ ls</p>
<p>torch中文网学习 https://ptorch.com/docs/2/five-simple-examples</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/gnn-parallel-communication-neural-network-read-4a90a35cefcb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/gnn-parallel-communication-neural-network-read-4a90a35cefcb/" class="post-title-link" itemprop="url">gnn-parallel-communication_neural_network-read</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-30 20:46:40" itemprop="dateCreated datePublished" datetime="2019-09-30T20:46:40+08:00">2019-09-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/cnn/" itemprop="url" rel="index"><span itemprop="name">cnn</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>5k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="abstract">Abstract</h2>
<p>很多任务在Ai中都需要多个智能体之间的交互。特别地，这些相互交互的协议往往是提前手动特化好的，不能随着训练而改变。
在这篇文章， 提出一个简单的神经模型 CommNet,
用于全合作的任务之间的连续的交流
这个模型包含多个智能体，而且他们之间的交流信息可以依据policy来进行学习得到。
我们应用这个模型到一些列的任务中，使他们自安静相互学习，最终提高他们在非交流智能体和baselines中的表现。
在某些例子中，最好的效果甚至可以根据智能体所产生的语言来做出好的策略 ##
1. Introduction
Communication是智能体一个很basic的方面，特别是在真实环境中的复杂任务。但是大多都有着有限的能力limited
capabilities或者对环境的不足的可见性visibility。
实际中的例子包括电梯控制，sensor networks,
Communication是一个很重要的因素在robot
soccer中。在某些特别的环境中，Communication交流甚至比智能体个人Individual的表现更为重要。然而，现有Reinforcement
learning中， specification and format of Communication 是
pre-determined.</p>
<p>在这篇work中，我们提出了一种模型，在做出actions之前可以通过agents之间的cooperating来进行学习。Each
agent 被一个deep feed-forward network控制, 通过一个continuous vector.
通过这个channel, 他们可以获得来自于其他智能体的summed
transmissions。然而，每个agent传递的信息不是a-prioir,而是学习得到的。
因为学习是continuous, 所以模型可以通过back-propagation来进行训练，
所以他们可以与标准的single agent RL algorithms或者supervised
learning方法相结合。
这个模型是简单而且多才多艺的，它允许应用于只能partial visibility of the
environment,
智能体可以学习task-specific交流通过他们之间的表现。更进一步，
模型可以动态变化， the number and type of agents.</p>
<p>我们考虑设置了J个agents, 所有的一起合作去最大化reward R在某些环境中。
我们做了一个assumption, 每个agent获得R独立于contribution. 这就意味着，
each agent有他们各自的控制器或者可以把他们看作a pieces of a larger
model一大个训练的模型。
从另一个观点来看，我们的控制器将每个agent的input映射到他们的actions. ?
input, 每个agent参加在a subset of units?<br />
一个特别的在layer之间的结构就是 a. 实例化通信的channe b.
传播agent的state</p>
<p>我们扩展了这个模型在一系列任务上，在某些方面， supervision
用来提供每一步动作， 偶尔地给别人提供? In the former case,
对于每个agent多续联的控制器， backpropagation the error signal 通过
connectivity structure，确保智能体学习到如何communication?
(学习函数)来最大化目标。 In the latter case,
RL会用于一个额外的步骤来提供训练信号，在每个时间步里?
等于说训练数据是通过它提供的?</p>
<h2 id="communication-model">2. Communication Model</h2>
<p>我们表述了一个模型用来计算 the distribution over actions <span
class="math inline">\(p(\mathbf{a}(t)|\mathbf{s}(t),
\theta)\)</span>即在给定时间步t下的行动的概率分布。 <span
class="math inline">\(s_j\)</span>作为<span
class="math inline">\(j_{th}\)</span>的agents在环境中获取的state. ? ==
input? yes controller 's input 是所有的state拼接而成的向量<span
class="math inline">\(\mathbf{s}={s_1, s_2, ..., s_J}\)</span>,
output是all actions的拼接 <span class="math inline">\(\mathbf{a} = {a_1,
a_2, ..., a_J}\)</span> &gt;
?controller每个agent都有一个，然后output是所有的，输出的是直接概率吗？
distribution又是如何得到的</p>
<p><span class="math inline">\(\Phi\)</span>即a single controller
包括individual controllers for each agents, 和communication对于agents.
&gt; ? ### 2.1 Controller Struture <span
class="math inline">\(\Phi\)</span>是有modules<span
class="math inline">\(f^i\)</span>得到的， <span class="math inline">\(i
\in \{0,...,K\}\)</span>, K是沟通的时间步</p>
<p><img
src="gnn-parallel-communication-neural-network-read/eq1&amp;2.png" /></p>
<p>每个<span class="math inline">\(f_i\)</span>对agent j有两个输入<span
class="math inline">\(h_j\)</span>和<span
class="math inline">\(c_j\)</span> &gt; 注意到的<span
class="math inline">\(c_j\)</span>这里是收集的不包含自身</p>
<p><span class="math inline">\(h^{i+1} = \delta(T^i h^i)\)</span> &gt;
这里考虑到了进一步将权重进行简化 &gt; 需要说明的是这里<span
class="math inline">\(T^i\)</span>具有permutation invariant,
即排列不变性 &gt; 这里<span
class="math inline">\(c^{i+1}\)</span>允许其他的形式,
所以agent的数量可以发生改变</p>
<p>另外 1. first layer: encoder funciton, problem
dependent问题独立性相关的。 &gt; <span class="math inline">\(h^{0} =
r(s_j)\)</span>, 一般地， <span class="math inline">\(c^0 = 0\)</span>
2. end layer: decoder funciton <span class="math inline">\(a_j =
q(h_j^K)\)</span></p>
<ol start="3" type="1">
<li>the whole model <img
src="gnn-parallel-communication-neural-network-read/fig1_whole_model.png" />
&gt; 解释： <span class="math inline">\(\Phi\)</span>是整个过程？,
这个应该不是单个吧？ 灰色阴影是agent, 还是agent group?
<ul>
<li>用了所有agent的状态? 需要吗？</li>
<li>使用等式1和2</li>
<li>根据decoder, 采样出所有的agents的行动</li>
</ul></li>
</ol>
<h3 id="model-extensions">2.2 Model Extensions</h3>
<ol type="1">
<li>Local Connectivity: 在<span
class="math inline">\(c^i\)</span>计算中， 可以选择特别数量的来做</li>
<li>Skip Connection: <span class="math inline">\(h^{i+1} = f^i(h^i, c^i,
h^0)\)</span>, 在f中添加其他T的h的印象</li>
<li>Temporal Recurrence: 一直循环某一个<span
class="math inline">\(f^i\)</span>。 ## 3. Related Work</li>
</ol>
<h2 id="experiments">4. Experiments</h2>
<h3 id="baselines">4.1 Baselines</h3>
<ol type="1">
<li><p>独立的控制器： 每个控制器都是独立的一部分，它们之间没有任何交流。
在这里直接认为<span class="math inline">\(\mathbf{a} = (\Phi(s_1),
\Phi_(s_2), ..., \Phi_(s_J))\)</span>完全是独立的。
这样的优点其实在于分子性和灵活性，可以很好地选择进入或者退出</p></li>
<li><p>全连接的控制器： 认为<span
class="math inline">\(\Phi\)</span>是一个全连接层的神经网络,
softmax使用multiple output softmax heads?.
简单的理解就是所有的agent全进行参与</p></li>
<li><p>离散的交流： 一个agents可替代的communicate方式就是通过discrete
symbols（感觉就是一些信号量）。 因为<span
class="math inline">\(\Phi\)</span>包含了离散的操作，所以不可微分，RL可以用来训练。
特别地，一个agent可以在每个communication step产生离散的信号。
但是，如果这里有internal 时间段，我们可以直接应用policy gradient?
如何做， 见下文 <span class="math inline">\(w_j^i ~
softmax(Dh_j^i)\)</span>,
这里D是模型的参数。在我们广播的操作中，在下一个<span
class="math inline">\(c^{i+1}\)</span>的运算即变成了离散的操作之间与，或关系。</p></li>
</ol>
<h3 id="simple-demonstration-with-a-lever-pulling-task">4.2 Simple
Demonstration with a Lever Pulling Task</h3>
<p>我们设计一个游戏，需要agent之间相互communicate来win.
这个包括了m个levers(杠杆？ 有什么用？), a pool of N agents. 在each
round中， 从N个agents中随机地取m个agents, 他们每个必须选择一个lever to
pull(?), 同时与其他m-1个agents进行交互，在每轮结束时(每轮都选？) &gt;
每一轮都选m个agent, 每个agent每轮确定一个lever. 每个lever如何初始化？
这个游戏的目标是让它们每个都选择一个不同的lever?, 这是目标？ &gt;
lever是什么？ 同时，所有的agents按照获取的不同的levers的比列来获得奖励。
每个agent可以看到自己的身份, <span class="math inline">\(s_j =
j\)</span> &gt; 所以这是一个间谍游戏，level也是混在其中？？</p>
<p>我们扩展这个游戏m=5, N=500. 使用CommNet拥有两个交流步K=2和skip
connections from (4)来进行实验。 1. encoder是一个N entries of
128D的查找表 2. <span
class="math inline">\(f^i\)</span>是一个两层的神经网络，以ReLU非线性的结构，
<span class="math inline">\(f(h^i, c^i, h^0)\)</span>, 输出128D的向量。
3. decoder是一个linear layer plus softmax, 产生关于m layer的一个分布
&gt; decoder就是<span class="math inline">\(p(a|s, \theta)\)</span>,
这里行动action用来选择lever 4. sample来决定使用哪一个</p>
<p>同时与Indepent controller进行对比，所有结构相同，只有c为0.
我们同时扩展了两组实验Supervised和RL - Supervised: 使用排序agents IDs,
每个agent pull the lever根据当前m agents中的相对顺序 - RL:
另一篇论文</p>
<p><img data-src="gnn-parallel-communication-neural-network-read\res.png" />
&gt; m=5, 500 trials, 5000 batches of size 64 during traning &gt;
metrics = # distinct / # levers</p>
<h3 id="multi-turn-games">4.3 Multi-turn Games</h3>
<p>两项任务： 1.
控制通过交通路口的汽车进入，最大化流量的同时，最大程度减小碰撞 2.
控制多个代理击败敌人</p>
<p>使用前馈神经网络MLP, <span
class="math inline">\(f^i\)</span>是一个single layer network, K=2
communication steps. 对于RNN module, 我们需要一个<span
class="math inline">\(f^i\)</span>, 使用共享权重。 最后对<span
class="math inline">\(f^t\)</span>使用LSTM, 在所有的模块中，hidden layer
is 50, MLP使用skip connections. 训练300 epochs, 每一轮进行100 weight
updates 使用RMSProp在mini-batch有288论的训练。 在训练中会经过~8.6M</p>
<p>a few days #### 4.3.1 Traffic Junction</p>
<p><img
src="gnn-parallel-communication-neural-network-read/fig2_traffic_junction.png" /></p>
<blockquote>
<p>left: 观察到车时，可能有的三种行进路线 middle: 有限的视野，
不能看见己方机器人，目标是攻击对方机器人 right:
当视野扩大时，实验对比。</p>
</blockquote>
<p>包含了4-way junction on 14*14 grid.
在每个时间步里，以某个概率p在四个方向放入车辆，车辆的总共数量为10.
每个car在有限的视野下，选择两种操作，前进或者不动。</p>
<p>当两个车位置重叠时即发生碰撞，此时reward=-10.
不影响以其他方式模拟？</p>
<p>为了缓解交通事故，会给每个时间步安排一个<span
class="math inline">\(Tr_{time}\)</span>, 于是可以得到总共的时间步:</p>
<p><img
src="gnn-parallel-communication-neural-network-read/eq_traffic_junction.png" /></p>
<p>each car {n, l, r}, 视野范围为3*3, 所以维度为 <span
class="math inline">\(3^3 * n * l * r\)</span></p>
<p><img
src="gnn-parallel-communication-neural-network-read/table2.png" /></p>
<p>将f()使用不同的结构时所带来的失败概率上的问题 #### 4.3.2 Analysis of
Communication Communication的分析</p>
<p><img
src="gnn-parallel-communication-neural-network-read/fig3.png" /></p>
<blockquote>
<p>left: 在traffic
junctiontask中，两个主要的成分的交流向量c在多轮运行中的结果。</p>
</blockquote>
<h4 id="combat-task">4.3.3 Combat Task</h4>
<p>对抗任务，15<em>15， m=5 agents, 5</em>5 初始化位置</p>
<p>视野3*3, 电脑方可以共享位置。</p>
<p>攻击一下就减1，总共生命值3， 只要在视野内即可攻击。</p>
<p>失败条件： 所有agents生命值为0， 达到最终轮数</p>
<p>？</p>
<h3 id="babi-tasks">4.4 bAbI Tasks</h3>
<p>读一段故事回答问题。</p>
<h2 id="discussion-and-future-work">5. Discussion and Future Work</h2>
<p>MARL在多种任务中表现超过没有交流，
全连接，模型利用离散交流信息的模型，我们去的了很好的效果。</p>
<p>另一方面，可扩展的 1. 异构agent types 2. large numbers of agents.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/tensorflow-variable-63af8c1f1dbc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/tensorflow-variable-63af8c1f1dbc/" class="post-title-link" itemprop="url">tensorflow variable</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-24 16:23:33" itemprop="dateCreated datePublished" datetime="2019-09-24T16:23:33+08:00">2019-09-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>880</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>https://blog.csdn.net/shenxiaolu1984/article/details/52815641 &gt;
tensorflow变量相关</p>
<p>https://blog.csdn.net/huqinweI987/article/details/82771521 &gt;
tensorflow自动训练简介和选择训练</p>
<h2 id="概览">概览</h2>
<p>tf.Optimizer只优化tf.GraphKeys.TRAINABLE_VARIABLES变量</p>
<p>init=tf.initialize_all_variables() sess.run(init) //
初始化之后Variable中的值生成完毕，不再变化</p>
<p>v = tf.all_variables() // 查看所有tf.GraphKeys.VARIABLES v =
tf.get_collection(tf.GraphKeys.VARIABLES)</p>
<h2 id="各类variable">各类Variable</h2>
<ul>
<li>获取训练的变量 方式1
<ul>
<li>tf.all_variables() &gt; 直接在某种状态下进行运行即可</li>
<li>tf.trainable_variables()</li>
</ul></li>
<li>获取训练的变量 方式2
<ul>
<li>tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</li>
<li>优化特定的变量: optimizer.minimize(cost, var_list)</li>
</ul></li>
</ul>
<p>sess: how to get a default sess</p>
<p>tf.gradients() &gt; 需要注意的是grad_weights这里返回得到的是list</p>
<p>使用一个未知的函数时，请务必详细地查看参数</p>
<p>想想tensorflow, 真的是厉害， 把模型具体给抽象出来</p>
<p>做事之前先想清楚，比如说对这段时间科研进行总结的话</p>
<p>一件事情不要测试过多，过于复杂</p>
<p>首要目标应该是把事情做完。</p>
<p>而且遇到挫折，不要扔，可以暂时转移，应该是想如何解决问题</p>
<p>ctrl+shift+f 全局搜索</p>
<p>一定要精确度到最细</p>
<p>而且要把测试代码单独拿出来，没做一次改变进行一次测试</p>
<p>git 使用中扩展思路总结: 1.
代码在当前版本，然后进行两部分分别测试，那么可以考虑创建两个分支 2.
代码提交的版本最好是能够运行的</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/linux-fbd371a191e1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/linux-fbd371a191e1/" class="post-title-link" itemprop="url">linux</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 13:28:11" itemprop="dateCreated datePublished" datetime="2019-09-19T13:28:11+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4.3k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="强大的常用命令-find-grep">强大的常用命令: find, grep</h2>
<p>https://www.cnblogs.com/skynet/archive/2010/12/25/1916873.html</p>
<p>find主要用于查找文件 grep(global search regular expression and print
out the line):
强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。</p>
<h3 id="find">1. find</h3>
<p>man: find [-H] [-L] [-P] [-D debugs] [-Olevel] [path]
[experssion]</p>
<p>find [path] [experssion] - path: find命令所查找的目录路径 -
experssion: expression可以分为"-options [-print -exec -ok...]" -
-options, 制定find命令下的常用选项 - -print,
find命令将匹配的文件输出到标准输出 - -exec,
find命令对匹配的文件执行该参数所给出的shell命令</p>
<p>常用选项以及实例 &gt;
可以不用加-print也可以正常输出，那么-print到底有啥用呢 &gt;
在本地测试时发现一个问题，bash test.sh, ok, 但是 ./test.sh permission
denied, 不知道是怎么回事 - -name: 按照文件名查找文件 - find /dir -name
filename 在/dir目录及其子目录下面查找名字为filename的文件 - find . -name
"<em>.c"
在当前目录及其子目录下查找任何扩展名为"c"的文件（即支持正则表达式） -
-perm 按照文件权限来查找文件<br />
- find . -perm 755 -print
在当前目录下查找文件权限为755的文件，即文件拥有者可以读、写、执行，其他用户可以读、执行的文件
- -prune
使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略
- find /apps -path "apps/bin" -prune -o -name "a.txt" -print
在/apps目录下查找a.txt，但不希望在/apps/bin目录下查找 &gt;
对的！不过必须要-print, -o才行，位置也不能错，todo, -o,
-print到底是什么作用啊？ - -user: 根据文件拥有者来查找文件 - find ~
-user sam -print 在$HOME目录中查找文件拥有者为sam的文件 - -mtime -n +n:
根据文件的更改时间来查找文件，-n表示文件更改时间在距现在n天以内，+n
表示文件更改时间距离n天以前 - find / -mtime -5 -print
在系统根目录下查找更改时间在5日以内的文件 - find /var/adm -mtime +3
-print 在/var/adm目录下查找更改时间在3日以前的文件 - type:
查找某一类型的文件 - 类型 &gt; b块设备文件， d目录， c字符设备文件，
p管道文件, l符号链接文件，f普通文件 - find /etc -type d -print
在/etc目录下查找所有的目录 - find . !-type d -print
在当前目录下查找除目录以外的所有类型的文件 - find /etc -type l -print -
-size n[c]: 查找文件长度为n块的文件，带有c时表示文件长度以字节计，不常用
- find . -size +10000c -print +表示大于 - find /home/apache -size 100c
print 正好 - find . -size -10 -print
在当前目录下查找长度不超过10块的文件（一块等于100字节） - -depth:
在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找 - find /
-name "CON.FILE" -depth -print
它将首先匹配所有的文件然后再进入子目录中查找 - -mount:
查找文件时不跨越文件系统mount点 - find . -name "</em>.XC" -mount -print
从当前目录查找位于本文件系统文件名以XC结尾的文件</p>
<h3 id="总结">总结</h3>
<ol type="1">
<li>最常用
<ul>
<li>查找文件, 可过滤一些文件，？？是否可以用正则表达式 -name
通配多种文件</li>
<li>时间、目录
<ul>
<li>-mtime 过滤时间</li>
<li>-prune 过滤目录</li>
</ul></li>
<li>大小、类型
<ul>
<li>-size</li>
<li>-type</li>
</ul></li>
</ul></li>
<li>高级
<ul>
<li>控制权限
<ul>
<li>-perm 过滤文件自身权限</li>
<li>-user 过滤拥有者的权限，</li>
</ul></li>
<li>访问顺序
<ul>
<li>-depth</li>
</ul></li>
<li>挂载点
<ul>
<li>-mount</li>
</ul></li>
</ul></li>
</ol>
<h3 id="grep">2. grep</h3>
<p>grep [options] pattern [file..] grep [options] [-e pattern] [-f file]
[file ...] ## 作业相关</p>
<p>jobs 查看正在进行的job ps 查看正在进行的进程</p>
<p>jobs 挂起 kill -stop PID kill %job_num kill pid &gt;
先经过ps查看一下</p>
<h2 id="查看端口">查看端口</h2>
<ol type="1">
<li><p>linux netstat -lnp | grep 8080 ps -aef | grep tomcat</p></li>
<li><p>windows</p></li>
</ol>
<p>netstat -ano | findstr 8080 taskkill /F /pid 1088</p>
<h2 id="ssh使用">ssh使用</h2>
<ol type="1">
<li>查看ip地址， ifconfig</li>
<li>sudo apt install openssh-server</li>
</ol>
<h2 id="apt-install">apt install</h2>
<p>https://www.cnblogs.com/hanxing/p/3996103.html</p>
<h3 id="安装位置">1. 安装位置</h3>
<p>/var/cache/apt/archieve 软件的安装缓存 sudo apt-get autoclean
只删除低版本的deb包 sudo apt-get clean 全部删除 &gt;
为了以后安装系统方便，可以将这些deb包保存在其他地方</p>
<p>一般的deb包都安装在/usr 或 /usr/share /usr/local中。
自己下载的压缩包或编译的包，有些可以选择安装目录， 一般放在/usr/local.
有时也放在/opt中。</p>
<p>如果想知道具体位置， dpkg -L XXX.deb &gt; eg: dpkg -L firefox</p>
<p>如果想知道apt-get install 安装的软件 dpkg -S softwarenmae | grep
cnf$</p>
<ol type="1">
<li>dpkg -L<br />
</li>
<li>dpkg -S apt-get install &gt; whereis 查看命令位置</li>
<li>/usr, /usr/share, /usr/local, /opt</li>
</ol>
<p>!!!! usr/share correct!!!</p>
<h3 id="apt-get">2. apt-get</h3>
<p>apt-get: advanced packaging tools &gt; apt-get需要root,
apt只需要当前用户</p>
<p><code>apt-get &lt;command&gt; [&lt;option&gt;] pkg1 [pkg2..]</code>
1. command: - update
重新获取软件包列表，很多时候软件安装不上就要先进行update一下 - upgrade
进行更新 - install, remove - autoremove: 自动移除全部不使用的软件包 -
purget 移除软件包和配置文件诶见 - source 下载源码档案 - autoclean 2.
args - -h 帮助文件 - -q 输出到日志 无进展指示 - -qq 不输出信息，错误除外
- -d 仅下载， 不安装或解压归档文件 3. 常用实例 - apt-cache search
pkg</p>
<h2 id="add-a-path-to-path">3. add a path to PATH</h2>
<p>~/bin: some distributions automatically put in your PATH if it
exists</p>
<ol type="1">
<li>add a path to PATH: <code>PATH=$PATH:~/opt/bin</code>
<ul>
<li>way1: 需要重启
<ul>
<li>~/.profile 当前用户 &gt; profile 侧面，轮廓</li>
<li>/etc/profile 所有用户</li>
</ul></li>
<li>way2: ~/.bashrc
<ul>
<li>vim ~/.bashrc</li>
<li>source ~/.bashrc</li>
</ul></li>
</ul></li>
<li>check the path
<ul>
<li>echo $PATH 检查环境变量 &gt; echo 重复，反射； 回应 ## 4. snap
？？？ 什么东西 add /snap/bin to $PATH</li>
</ul></li>
</ol>
<p>then can directly use</p>
<h2 id="lua环境安装">5. lua环境安装</h2>
<ol type="1">
<li>sudo apt-get install lua5.3</li>
<li>sudo apt-get install luarocks &gt; luarocks install gnuplot: install
torch first</li>
<li>git clone torch &gt; bash install-deps &gt; lua5.2 torch7 &gt; th
快捷命令</li>
</ol>
<h2 id="todo">todo</h2>
<ol type="1">
<li><p>proxy problem: 开机手动设置</p></li>
<li><p>查看后台进程</p>
<ul>
<li>ps</li>
<li>service &gt; chkconfig --list &gt; no find!!!</li>
</ul></li>
<li><p>uname -a: 查看当前版本信息</p></li>
<li><p>chrome 快速定位到搜索栏: ctrl+L, alt+D</p></li>
<li><p>最小化所有窗口 win + d ？ 最小化当前窗口， 终端重启一个</p></li>
<li><p>to add deepin some memory</p></li>
<li><p>mv 使用正则表达式</p></li>
<li><p>关于硬盘：rename没有必要</p></li>
<li><p>linux关于安装系统的命令</p>
<ul>
<li>ln命令： 功能是为某一个文件在另一个位置建议一个同步的链接
<ul>
<li>例子
<ul>
<li>ln -s link1 link2request to https://registry.npmjs.org/hexo-cli
failed, reason: Client network socket disconnected before secure TLS
connection was established &gt; 为link1文件创建软链接link2,
如果link1丢失，link2将会失效</li>
</ul></li>
</ul></li>
</ul></li>
<li><p>百度网盘使用有问题，aria2使用频率不高，放弃</p></li>
<li><p>安装linux版的hexo &gt;
配置文件一般是有全局配置文件的，在～/.npmrc这些内容之下</p></li>
<li><p>vscode &gt; 配置总是让输入user和passwd, git config --global
credential.helper store</p></li>
<li><p>lantern &gt; what is proxy IP? 是否会影响其他的安装</p></li>
<li><p>在linux上安装pip <code>sudo apt-get install
python-pip(python2)</code></p></li>
<li><p><code>scp</code> 加-r, 上传，下载目录；
不加-r上传，下载文件</p></li>
<li><p><code>sudo apt-get install fim</code>: 展示图片，测试git
bash不行; 终端显示ok</p></li>
<li><p>linux远程登录windows todo &gt; 基于rdesktop: 但是好像不是命令行
https://blog.csdn.net/u011054333/article/details/79905102</p>
<ul>
<li>设置允许远程连接到计算机 &gt;
https://jingyan.baidu.com/article/b0b63dbf321c224a49307062.html
<ul>
<li>scp不行 5s内</li>
</ul></li>
</ul></li>
<li><p>查看某一端口是否被占用 &gt; lsof -i:2323; netstat -tunpl | grep
2323</p></li>
<li><p>查看是否有网 &gt; netstat -ntl 查看端口是否在监听</p></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/vim-03f7c94da709/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/vim-03f7c94da709/" class="post-title-link" itemprop="url">vim</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-09-19 13:25:19" itemprop="dateCreated datePublished" datetime="2019-09-19T13:25:19+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/" itemprop="url" rel="index"><span itemprop="name">tools</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tools/vim/" itemprop="url" rel="index"><span itemprop="name">vim</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>659</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="vim">vim</h1>
<h2 id="光标移动">光标移动</h2>
<ol start="0" type="1">
<li><p>整篇文章 gg: 开头 G: 末尾 123+gg: 跳转到指定行</p></li>
<li><p>屏幕 H屏幕顶行，J合并两行 L屏幕底行 M屏幕中间行 ctr+f pagedown
ctrl+b pageup</p></li>
<li><p>行列上下 左、下、上、右 h, j,k,l</p></li>
<li><p>单词 w:开头, 向后 b:开头，向前 e:末尾，向后</p></li>
<li><p>缩进 &gt;&gt; 缩进 &lt;&lt;</p></li>
<li><p>行前，行尾 行前，行尾 ^, $</p></li>
<li><p>段前，段后 {段前 }段后: 跳过开头 ( ): 会经过开头行 ## 选择 v:
行选择 ctrl+v: 列选择</p></li>
</ol>
<h2 id="修改">修改</h2>
<ol type="1">
<li>插入 i: 当前字母前插入 I: 当前行前插入</li>
</ol>
<p>a: 当前字母后插入 A: 当前字母后插入</p>
<p>o: 下一行插入 O: 上一行插入</p>
<ol start="2" type="1">
<li><p>复制 y: 复制选中内容 yy: 复制当前行 123+yy:
向下复制多少行</p></li>
<li><p>剪切 无，=复制+删除</p></li>
<li><p>删除 y: 复制选中内容 yy: 复制当前行 123+yy:
向下复制多少行</p></li>
<li><p>粘贴 y: 复制选中内容 yy: 复制当前行 123+yy:
向下复制多少行</p></li>
</ol>
<h2 id="操作撤销和恢复">操作撤销和恢复</h2>
<p>u: 撤销操作undo ctr+r 重做redo</p>
<h2 id="全局查找替换">全局查找、替换</h2>
<ol type="1">
<li><p>查找 /word 回车<br />
N查看上一处 n查看下一处</p></li>
<li><p>替换 :{作用范围}s/{目标}/{替换}/{替换标志}</p></li>
</ol>
<ul>
<li><p>作用范围 全局范围<code>:%s/</code> 当前行s 选区
<code>:'&lt;,'&gt;/</code>
当前行<code>.</code>与接下来的两行<code>+2</code>:
<code>:.,+2s</code></p></li>
<li><p>替换标志 g: global替换所有的出现 空：从光标位置开始替换 gc:
需要进行确认</p></li>
<li><p>块选择 v 光标移动过的位置直接进行选择。可以操作 ctrl+v 列选择
y复制， p粘贴</p></li>
</ul>
<p>该模式下移动光标的感觉跟查看下一样</p>
<p>参考 https://harttle.land/2016/07/18/intro-to-regexp.html &gt;
正则表达式这篇写得很好！！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yun-Pan Wang"
      src="/images/default-avatar.png">
  <p class="site-author-name" itemprop="name">Yun-Pan Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AugF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AugF" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangyp@smail.nju.edu.cn" title="E-Mail → mailto:wangyp@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/Joswxe" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;Joswxe" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://cn.bing.com/" title="https:&#x2F;&#x2F;cn.bing.com" rel="noopener" target="_blank">Bing</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.baidu.com/" title="https:&#x2F;&#x2F;www.baidu.com" rel="noopener" target="_blank">Baidu</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">[object Object]</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">466k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:04</span>
</div>!

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://ypwang.github.io/page/3/',]
      });
      });
  </script>



</body>
</html>
