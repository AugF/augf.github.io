<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ypwang.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="摘要 图结构广泛出现在化学、自然语言语义、社交网络和知识结构中。 在这篇工作中，我们学习了关于图结构输入的特征学习方法。 我们开始的起点是2009年的GNN的工作，我们使用gated recurrent units和现代化优化手段，并且扩展到输出序列。 结果是灵活的并且广泛对于神经网络，结果相对于纯粹基于序列的模型具有很好的归纳偏差?，尤其是图结构的模型。(LSTM) &gt; htt">
<meta property="og:type" content="article">
<meta property="og:title" content="gnn-parallel-ggsnn-paper-read">
<meta property="og:url" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/index.html">
<meta property="og:site_name" content="Life&#39;s Notes">
<meta property="og:description" content="摘要 图结构广泛出现在化学、自然语言语义、社交网络和知识结构中。 在这篇工作中，我们学习了关于图结构输入的特征学习方法。 我们开始的起点是2009年的GNN的工作，我们使用gated recurrent units和现代化优化手段，并且扩展到输出序列。 结果是灵活的并且广泛对于神经网络，结果相对于纯粹基于序列的模型具有很好的归纳偏差?，尤其是图结构的模型。(LSTM) &gt; htt">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/ggnn-equations.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/eq7.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/bAbI_res.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/bAbI_res2.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/alg1.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/fig3.png">
<meta property="og:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/table4.png">
<meta property="article:published_time" content="2019-10-10T13:16:00.000Z">
<meta property="article:modified_time" content="2021-05-27T01:05:35.000Z">
<meta property="article:author" content="Yun-Pan Wang">
<meta property="article:tag" content="gnn-parallel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/gnn-parallel-ggsnn-paper-read/ggnn-equations.png">

<link rel="canonical" href="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>gnn-parallel-ggsnn-paper-read | Life's Notes</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Life's Notes</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">岁月数载，愿不负韶华</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://ypwang.github.io/2019/gnn-parallel-ggsnn-paper-read-d28823966db5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/default-avatar.png">
      <meta itemprop="name" content="Yun-Pan Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Life's Notes">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          gnn-parallel-ggsnn-paper-read
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-10-10 21:16:00" itemprop="dateCreated datePublished" datetime="2019-10-10T21:16:00+08:00">2019-10-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-05-27 09:05:35" itemprop="dateModified" datetime="2021-05-27T09:05:35+08:00">2021-05-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/gnn-parallel/" itemprop="url" rel="index"><span itemprop="name">gnn-parallel</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>13 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="摘要">摘要</h2>
<p>图结构广泛出现在化学、自然语言语义、社交网络和知识结构中。
在这篇工作中，我们学习了关于图结构输入的特征学习方法。
我们开始的起点是2009年的GNN的工作，我们使用gated recurrent
units和现代化优化手段，并且扩展到输出序列。
结果是灵活的并且广泛对于神经网络，结果相对于纯粹基于序列的模型具有很好的归纳偏差?，尤其是图结构的模型。(LSTM)
&gt; https://zhuanlan.zhihu.com/p/38740843,
归纳偏差简单的理解就是模型的偏好是什么？
我们描述了模型在bAbI任务上的能力和图的算法学习任务。接下来，我们展示了程序验证中的问题的应用效果，其中子图可以抽象为数据结构</p>
<h2 id="introduction">1. Introduction</h2>
<p>很多现实的图结构任务。
我们想在学习任务中将图作为输入。标准的方法对于问题来说包括对于一个输入图的工程自定义特征，
graph kernel. 方法用来定义图的特征通过在图上random walks，
更多的靠近我们目标的是先从图中进行学习特征中能包括GNN, spectral
networks以及在图足迹分类，化学分子表示上的方法</p>
<p>我们的主要贡献在于关于GNN在输出sequences上。之前在特征学习的工作主要集中在产生图的单个输出，比如grah-level分类任务，但是很多关于图的许多问题需要输出sequences。
例子包括在图上的路径，具有理想节点的枚举，以及图分类任务的序列。比如， a
start and end node. 我们不确定现有的工作是否能够很好的解决这个问题
我们的动机来自于程序验证，需要输出logical formulas，
我们把它形式化为一个序列化输出问题。
第二个贡献是GNN是一个广阔的神经网络模型可以应用再现阶段的很多领域</p>
<p>这里有两种关于图的特征学习的设定： 1. 学习关于输入图的表示 2.
学习在产生序列输出的interal state的表示</p>
<p>其中，1主要在GNN的工作里面。我们对这个框架做了调整，比如使用modern
practices 有关Recurrent Neural Networks.
2是我们期望从图结构问题中输出的，这不仅仅是individual classfication.
因此，这要的挑战就在于如何在图中学习特征，并且encode
已经产生的partial输出序列（比如path so far if outputting a
path），以及还需要被产生的（the remaining path）。 我们同时也展示了GNN
framework can be adapted to these settings,
从而获得一个新的关于基于图的神经网络模型，GGS-NNs</p>
<p>我们阐述了这个模型在bAbI tasks和图算法，在学习模型的能力上。
然后我们展示关于程序验证的应用。
当需要试图验证内存安全是，一个核心的问题是如何发现关于程序中运用的关于数据结构的数学描述。
我们 phrased这个机器学习任务，在我们需要map from a set of input graphs,
代表运用的内存，用来获取关于是咧的数据结构的logical descripton.
另外有一篇论文依赖于大量的手写工程的特征，我们展示了可以使用GGS-NN来替代该系统。</p>
<h2 id="graph-neural-networks">2. Graph Neural Networks</h2>
<p>在这段中，我们review GNNs, 并且介绍了整个过程用到的notation and
conecpts</p>
<p>GNNs 是一个普遍的神经网络结构根据图来定义， G=(V, E), 其中v为Nodes,
e为edges. 我们主要集中在directed graph上，所以(v, v')表示一个directed
edge, 但是我们发现这个框架可以easily to adapyed to undirected graphs,
see Scarselli et al.(2009). The node vector(node representation or node
embedding) for node v 是<span class="math inline">\(h_v\)</span>,
图也可以包含关于摸个节点的标签, edge labels <span
class="math inline">\(h_S\)</span>中S是a set of nodes <span
class="math inline">\(l_S\)</span>中S是a set of edges <span
class="math inline">\(IN(v)\)</span>表示v节点的祖先 <span
class="math inline">\(OUT(v)\)</span>表示v节点的后继 <span
class="math inline">\(NBR(v)=IN(v) U OUT(V)\)</span>表示v的邻居 <span
class="math inline">\(Co(v)\)</span> 所有经过顶点v的边</p>
<p>GNNs将graphs map到outputs通过两步： 1. propagation
step计算每个节点的表示 2. <span class="math inline">\(o_v =
g(\mathbf{h}_v, l_v)\)</span>
从node的表示和对应的labels中学习，来获取关于每个节点的一个输出。
在对于g的notation，
我们留下了关于参数的隐式的依赖，然后我们继续做这件事。 this
system是differentiable from end-to-end,
所以所有参数都可以通过gradient-based optimization学习</p>
<h3 id="propagation-model">2.1 Propagation Model</h3>
<p>在这里是关于一个迭代过程传播节点的表示。初始化节点的表示<span
class="math inline">\(h_v\)</span>可以设置为arbitary
values(任意的值)，然后node
representation根据下面的节点表示进行更新，直到convergence.</p>
<p><span class="math display">\[h_v^{(t)} = f^*(l_v, l_{Co_{(v)}},
l_{NBR(v)}, h_{NBR(v)}^{(t-1)})\]</span>
很多变量被提及，所以Scarselli建议decompsing函数f为一个关于每个对应的变得相加</p>
<p><span class="math display">\[f(l_v, l_{(v&#39;,v), l_{v&#39;},
h_{v&#39;}^{(t)}}) = A h_{v&#39;}^{(t-1)}+b\]</span></p>
<h3 id="output-model-and-learning">2.2 Output Model and Learning</h3>
<p>输出模型定义在每个节点上， g函数是可微的，可以maps到输出。
这仅仅是一个linear or neural network mapping.
在这个模型中每个节点将获得一个最终的表示。
如果是为了处理graph-level分类任务，他们的建议是使用一个dummy 'supper
node, which is connected to all other nodes by a special type of edge."
学习过程 ALmeida-Pineda算法已经完成
然后基于梯度的计算直到收敛解决最终问题。 评价： 1. advantage:
不需要存储中间状态来计算梯度。 2. disadvantage: 参数必须首先的，
所以传播步骤是一个收缩map。这需要来确保收敛，因为它可能限制模型的表达能力。</p>
<p>鼓励使用一个关于1-norm的Jacobian作为惩罚项。 Appendix
A是一个例子，给出了收缩银蛇的直觉难以在图中长时间传播信息</p>
<h2 id="gated-graph-neural-networks">3. Gated Graph Neural Networks</h2>
<p>我们提出了GG-NNs, 适用于non-sequential puputs.
关于GNNs的最大调整是我们使用了Gated Recurrent Units，并且unroll(展开)
一个循环的重复的神经步，使用backpropagation 来计算梯度。
这需要更多的内存相比于GNN算法，但是不需要限制参数来确保收敛convergence.
我们同时扩展了隐藏层的表示和输出模型</p>
<h3 id="node-annatations">3.1 Node Annatations</h3>
<p>在GNNs中， 这里没有明显的点来初始化节点的表示因为收缩图map
constraint来确保，者使得我们不需要与其他的节点label作为额外的输出。为了分辨这些节点用作输入和之前介绍的节点。
node annotations 并且使用向量x来denote这些表示。
为了产生他们怎么用，考一个简单的任务，训练图神经网络来预测接地那t如何到达节点s在一个给定的图上。在这个问题中有来哥哥问题相关的
节点，s 和t， [1,0] [0,1]. 在可到达的例子中，
可简单来看春波模型如何来传播节点的annotation(注解)，使得它们的第一位变为1.
output step
classifier可以轻易地辨别nodel是如何从s到达t的通过查看安歇节点有非零的实体在前两维上
&gt; 这个过程好像标签传播</p>
<h3 id="propagation-model-前向传播模型">3.2 Propagation Model
前向传播模型</h3>
<p><img data-src="gnn-parallel-ggsnn-paper-read/ggnn-equations.png" /></p>
<p>MatrixA决定了节点在图的communicate中如何进行交流。
稀疏的结构和参数在A中图图1所示。 eq 1是初始化步骤，copy node
annotions到隐藏层的第一位，并把剩余维度清零。 eq
2是在不同节点间通过出度和入度的边来春波参数，这些参数独立于边的种类和方向。
<span class="math inline">\(a_v^{(t)}\)</span> &gt; ?
输入x是什么？是对应的邻接矩阵信息吗? 注意维度，这里是二维的</p>
<p>剩余的就是GRU的部分 https://zhuanlan.zhihu.com/p/32481747</p>
<p>https://zh.d2l.ai/chapter_recurrent-neural-networks/gru.html</p>
<p><span class="math inline">\(z_t\)</span>用于门控的更新， <span
class="math inline">\(r_t\)</span>用于门控的重置</p>
<p><span class="math inline">\(h^{(t-1)} = h^{(t-1)} . r\)</span> &gt;
理解，<span class="math inline">\(H_t\)</span>是更新门，
最终结果是对上一时间步的隐藏层和当前时间步的候选隐藏状态<span
class="math inline">\(H_t\)</span>做组合 <span class="math inline">\(Z_t
* H_{t-1}\)</span> 表示对上一层的遗忘，忘记上一层的某些东西 <span
class="math inline">\((1-Z_t) *
H_{t}\)</span>表示对当前候选隐藏层的状态进行记忆。 这里z是门控喜好，
门控信号越接近1表示记忆得越多，越接近0表示遗忘得越多？？ &gt;
需要看乘法是怎么做的？</p>
<blockquote>
<p>不要管z和r怎么看，都是有w的超参数，实际上就进行理解为z是用来学习分配更新的，r是用来产生中间隐藏候选状态的中间变量</p>
</blockquote>
<p>结果表示GRU-like 前向传播步骤更有效果</p>
<h3 id="output-models">3.3 Output Models</h3>
<p>这里有one-step outputs，我们可以用来产生各种的情形。首先GG-NNs node
selection任务为门戈节点的输出进行打分，并且因公softmax在node 分数上 &gt;
意思是node selection其实它会中间输出表示，按理说维度应该和类别一样。
&gt; 其实这里就是中间观察的结果</p>
<p>对于graph-level outputs,我们的定义如下</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/eq7.png" /></p>
<blockquote>
<p>这里<span class="math inline">\(\delta\)</span>使用的是soft
attention机制? 来决定是否哪些节点与当前的tgraph-level任务相关。
i和j是神经网络。使用<span
class="math inline">\(h_v^{(T)}\)</span>和<span
class="math inline">\(x(v)\)</span>$作为input和ouputs的real-valued
vectors. tanh可以被替代
这里说的就是如何产生中间观察结果，记得之前有学习过一个模型！ 不理解？ ##
4. Gated Graph Sequence Neural Networks 在这里GGSNNs</p>
</blockquote>
<p>对于k-th 输出步骤，我们描述了node annotations as <span
class="math inline">\(X^{(k)}\)</span>, 我们用了两个 GG-NNs <span
class="math inline">\(F_o^k\)</span>和<span
class="math inline">\(F_x^k\)</span>来从<span
class="math inline">\(X\)</span>中预测o. 以及<span
class="math inline">\(F_X\)</span>用来从<span
class="math inline">\(X_k\)</span>预测<span
class="math inline">\(X_{k+1}\)</span>,
两个都是包括了一个前向模型和输出模型。在前向模型中，我们使用了节点向量作为t层的前向步骤和k层的输出步骤。
在之前的每一步，我们设置 &gt; 这两个模型一个输出x， 一个用来输出o；
在之前我们把<span
class="math inline">\(H^{k-1}\)</span>初始化为0-extending的<span
class="math inline">\(X_{k}\)</span>.
这种简单的变体是能够很快用来训练和评估的，在很多case种，整个模型可以达到相同的表现。但是在某些case中，两个的表现效果不一样，所以这种变体不能work得很好
&gt; ? 为什么 我们介绍了node annotation ouput用来从H预测X。
这个预测对于每个节点都可以很简单地用一个神经网络j来连接h和x作为输入和输出，从而得到一个真实的分数
&gt; ?</p>
<p>这里有两种关于GGS-NNs的设定： specifying all intermediate annotation
Xk, 或者训练所有模型，仅仅给X1, graphs and target sequence.
前者可以提高表现力，但我们有邻居只是在知道节点的信息在哪些 &gt;
取得节点达摩鞋特征。 第二种更为普遍</p>
<ol type="1">
<li>Sequence outputs with observed annotations 考虑任务来做整个，
图的序列预测，每个预测只是包括图的一笑部分。为了确定每个部分都有一个输出，每个节点只有一位就过来，表示我们已经解释了。
&gt; 子图训练？？</li>
</ol>
<p>在某些设定中，少量的annotations足够来预测。我们可以扩展多个模型，这些模型都有注释地可做
&gt; one idea, 可以用不同的邻域，来结合多个相同模型！！！
还有模型的组合</p>
<p>单步domain预测，这实际上和所有是一样的 2. Sequence outputs with
latent annotations 一般地，当中间不可用时，作为隐藏层，后向传播训练</p>
<h2 id="explanatory-applications">5. Explanatory Applications</h2>
<h3 id="babi-tasks">5.1 bAbI Tasks</h3>
<p>这里我们简述了如何使用GGS-NNs. 我们主要集中在bAbI任务上。</p>
<p>bAbI tasks是Facebook提出的一个AI任务集，
第一组发布的bAbI包括20个任务。若要查看这些任务的详细信息， 请移步<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.05698">bAbI tasks</a>.
在bAbI数据集中，每一篇文本对应于一个故事。本文将每个故事映射成了一个简单的图（节点对应于实体，边对应于关系）。
每个问题(eval)由问题类型（类似于谓词，如"has_fear")与一些参数（对应于图中一个或多个节点）构成。当任务具有多种问题类型时，需要对每种类型都训练一个独立的模型。在本文中，仅处理类型未二元一阶谓词的问题。</p>
<p>例如，对于问题“eval E&gt;A true”, 那么问题的类型为"&gt;",
图中的节点E将会被（初始化）标注为<span class="math inline">\(x_E = [1,
0]^T\)</span>, 节点A将会被标注为<span class="math inline">\(x_A=[0,
1]^T\)</span>, 其他节点将会被标注为<span class="math inline">\([0,
0]^T\)</span></p>
<h4 id="单输出任务">单输出任务</h4>
<p>单输出任务实验基于bAbI的四个任务： Task4: Two Argument
Relations(选用D=4)； Task15: Basic Deduction(D=5); Task16: Basic
Induction(D=6); Task 18: Size Reasoning(D=3) &gt; D是什么？</p>
<p>在bAbI任务集的四个任务上的实验结果为（括号内为使用的训练样本数，下同）</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/bAbI_res.png" /></p>
<h4 id="序列输出任务">序列输出任务</h4>
<p>序列输出任务基于bAbI Task19: Path Finding(D=6). 其实验结果如下：</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/bAbI_res2.png" /></p>
<h3 id="learning-graph-algorithms">5.2 Learning Graph Algorithms</h3>
<h2 id="程序验证问题-ggs-nns">6 程序验证问题 GGS-NNs</h2>
<p>在GGS-NNs的工作主要来自于实际program verification. 在automatic
program verification中最关键的一步就是program invariants的推论，
它用来近似评估在一次execution中一些程序状态是否可达。
发现数据结构中的不变量是一个open problem. 在这里，C函数为例
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">node* concat(node* a, node* b) &#123;</span><br><span class="line">    if (a==NULL)  return b;</span><br><span class="line">    node* cur=a;</span><br><span class="line">    while (cur.next != NULL)</span><br><span class="line">        cur = cur-&gt;next;</span><br><span class="line">    cur-&gt;next = b;</span><br><span class="line">    return a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 为了证明这个问题two lists
a和b确实是concatnates连接的，所有指针取消引用都是合法的。我们需要characterize程序在每一次迭代的循环中的help。
在这里我们用了separation logic(分隔的逻辑符号)， 它使用inductive
predicates归纳谓词来描述, 抽象数据结构。 举个例子，
一个分段的list定义为todo1, todo2
意味着x指向一个内存区域，它包括了数据结构有两个域， val和next，
它们的值保存在v和n中， * connective是合并的布尔连接法，
但是额外需要它操作指导"separate" parts of the heap. ls(cur, NULL)
于是这cur要么是null,要么指向heap中的两个值v、n, 这里n用ls描述。
公式todo3 是一个循环的不变式。
使用它，我们可以证明，不会失败，因为不会引用一块未分配的内存地址。
这个函数进一步使用Hoare-style验证方式的确连接了两个lists.
关于整个流程最困难的地方在于产生描述数据结构的公式。在这里我们使用到机器学习。在给定的程序中，我们跑了很多次并且提炼了内存的状态（重新表示为graph)在相关程序位置，接着预测了分离逻辑公式。静态的程序分析工具可以检查一个候选的公式是否是充足地去证明它需要的属性。
&gt; 这里考虑的是使用Machine Learning来描述data struture,
然后预测相关的separation logic formula??? 怎么能保证正确性啊？
惊奇！！！</p>
<blockquote>
<p>Input这里边上的权重0表示什么意思？ ### 6.1 Formalization</p>
</blockquote>
<ol type="1">
<li>Representing heap state as a graph:<br />
作为输入，我们考虑有向图，可能是循环图来表示一个程序的heap.
这些图可以自动地构建程序的对状态。
每个图的节点都对应着内存中一块内存地址，在这块地址中，v0, ...,
vk已经存储。 图的边反映了指针值，即v具有标记为0, ..., k
分别指向了节点v0, ..., vk &gt; ?? node 和 edge 还需要再理解。
实际上只指向一块地址，但是node可能多个地方对这块地址有引用？
地址一块，一块链接存储，像链表一样，每个节点都对应独特的一块内存空间？
&gt; 边的属性表示什么意思？</li>
</ol>
<p>A subset of nodes are labeled as corresponding to program
variables.</p>
<p>一个输入图的例子as "Input" in Fig.3. In it, 节点展示在节点里面。node
id(i.e., memory address), 边的labels表示特殊的域？，e.g. 0对应于previous
section中的下一个指针。 对于binary trees, 这里有两种类型的指针left,
right分别指向树节点的left和right节点。 2. Output Representation
我们的目标是数学表示heap的形状。在我们的模型中，我们限制了分离限制的语法版，
formulas的形式是，<span class="math inline">\(\exists x_1, ..., x_n, a_1
* ... * a_m\)</span>， 这里每个atomic formula <span
class="math inline">\(a_i\)</span>或者ls(x, y), tree(x), 或者none(x)(no
data struture at x).
存在量词是用于给描述形状所需的堆节点命名，而不给一个程序的变量命名。
举例， 为了描述“人手清单”（a list that ends in a cycle）, 第一个list
element需要被描述。 在separation logic, 被表示为 <span
class="math inline">\(\exists t. ls(x, t) * ls(t, t)\)</span></p>
<ol start="3" type="1">
<li>Data
我们为了这个问题产生合成的（标记的）数据集。自此，我们还修复了一些谓词，比如ls和tree(扩展名可以考虑双向链标的列表段，多树)以及它们的递归定义。然后，我们列举了实例化分离逻辑公式我们的谓词使用的程序变量集
&gt; 程序变量集，变量v, address</li>
</ol>
<p>最后，对于每个公式，我们列举满足该公式的堆图，结果是一个数据集包括我们heap
graphs and associated formulas that are used by our learning
procedures.</p>
<h3 id="formulation-as-ggs-nns">6.2 Formulation As GGS-NNs</h3>
<p>通过数据迭代产生的过程来获得节点的中间表示。所以，我们训练一个不变量GGS-NN使用观察的annotations(observed
at training time) &gt; 这里关注的点是什么？为什么要这么做</p>
<p>使用un-obeserved GGSNN
variant来做end-to-end学。这个过程将会使得separation logic formula拆分为
a sequence of steps.
我们首先决定了来宣称了哪个必要的变量，如果可以的话，选择对应的节点来表示变量。<code>??</code>
一旦我们有了宣称的必要的变量，我们迭代所有变量的名字，然后产生一个separation
logic formula以当前变量所对应的的节点未根来描述数据结构。
完整的算法如下所示。</p>
<p>我们使用了三个明显的node annotation部分，namely is-named(heap node
labeled by program variable or declared existentially quantified
variable(量化的))， 被命名的， active(cf. algorithm) 和
is-explained(heap node is part of data struture already predicted).
初始化的节点标记可以直接从input graph中计算得到。
is-named是对于程序变量的编辑。 active和is-explained总是off.</p>
<p>评论的行在扩展中都使用的是GG-NN。 Alg.1 是一个GGS-NN model的instance.
一个简要的关于算法运行的一开始的算法在Fig3,
每一个斗鱼算法的每一行相关</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/alg1.png" /> &gt; Separation
logic formula prediction procedure <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Input:  Heap graph G with named program variables.</span><br><span class="line">X: compute initial labels from G    // 做了些什么</span><br><span class="line">H: initialize node vectors by 0-extending X</span><br><span class="line"></span><br><span class="line">while exists quantifier needed do // 当需要存在量词的时候  Graph-level Classification</span><br><span class="line">    t: fresh variable name</span><br><span class="line">    v: pick node    // Node selection.</span><br><span class="line">    X: turn on &quot;is-named&quot; for in X</span><br><span class="line">    print &quot;exists t&quot;  // 不懂什么意思</span><br><span class="line">end while</span><br><span class="line"></span><br><span class="line">for node v_l with abel &quot;is-named&quot; in X do</span><br><span class="line">    H: intialize node vectors, turn on &quot;active&quot; label for v_l in X</span><br><span class="line">    pred: pick data structure predicate // Graph-level  Graph-level Classification 好抽象， 随便来抽取数据结构还是怎么会使</span><br><span class="line">    if pred = ls then</span><br><span class="line">        l_&#123;end&#125;: pick list end node  // Node selection</span><br><span class="line">        print &quot;ls(l, l_&#123;end&#125;)  // 这里的print实际上就是输出上面想要分析的表达式</span><br><span class="line">    else </span><br><span class="line">        print &quot;pred(l)*&quot;  // 没有tree等其他情况，等于说</span><br><span class="line">    end if</span><br><span class="line">    X: update node annoatations in X  // Node Annotation</span><br><span class="line">end for</span><br></pre></td></tr></table></figure> &gt;
所以是这里每一步的评论都会用到GGS-NN模型？ &gt; is-named, active,
is-explained等于说是变量的三个状态，
于是heap中的变量的状态就是？，不过依然看不懂fig3</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/fig3.png" /></p>
<h3 id="model-setup-details">6.3 Model Setup Details</h3>
<p>我么使用了full GGS-NN model， <span
class="math inline">\(F_o^k\)</span> 和 <span
class="math inline">\(F_X^k\)</span>有着separate propagation models.
对于所有GG-NN成分，在GGS-NN pipeline中的， 我们展开传播过程为10 time
steps. GGS-NNs与step(|)(决定是否更多存在两次变量需要声明) 和 step(||)
(辨别哪些节点需要声明为存在量词)， 通过使用D=16维的节点表示。 对于all
other GGS-NN成分， D=8使用，Adam优化器用来优化，模型被训练在20
graphs的minibatches， 优化直到训练错误率非常低。
对于图级别的分类任务，我们认为平衡地分类，每一小皮量中甚至都有even
number of examples. 所有的GGS-NN成分包含了少于5k paras, 并且 no
overfitting被观察到，在训练过程中。</p>
<h3 id="batch-predication-details">6.4 Batch Predication Details</h3>
<p>在实际中，一系哦啊部分heap graphs将会被给与input and a single output
formula is expected to describe, 并且与所有的input graphs一直。
不同的heap graphs可以是程序执行过程中不同状态点，
或者运行在相同程序的不同输入。 我们称这个"batch prediction" setup
与描述在主要的paper中的single graph prediction相互对比。</p>
<p>为了得到batch predictions, 我们运行一次GGS-NN对于each graph同时的。
For each prediction step, 所有GGS-NNs的outputs at the step across the
batch of graph被聚集。</p>
<p>对于node selection outputs, 普遍的命名为variables link
nodes在不同的图上，它是聚集prediction in a
batch的关键。我们计算particular named variable t as <span
class="math inline">\(o_t = \sum_g o^g_{V_g(t)}\)</span>, 其中<span
class="math inline">\(V_g(t)\)</span>maps 变量t到图中的一个节点。 <span
class="math inline">\(o^g_{V_g(t)}\)</span>是named variable t in graph
g中的 output score. 当使用一个softmax对于所有names using <span
class="math inline">\(o_t\)</span> as scores, 这个与计算<span
class="math inline">\(p(toselect=t)=II_g
p_g(toselect=V_g(t))\)</span>相同。</p>
<p>对于graph-level 分类输出，我们增加一个特别的类的scores across the
batch of graphs., 或者相等的计算<span
class="math inline">\(p(class=k)=II_gp_g(class=k)\)</span>, Node
annoation outputs作为不同的图在每个图中独立地被更新，有着完全不同的set
of nodes. 然后，当算法尝试更新annotation of one named variable, the node
相关联的所有节点也被更新。在训练中， 所有标记 intermediate
steps对于我们是可获得的，从data generation process, 所以training
process能够在一次的降解到single output single graph training.
一个更complex的情景允许nested（嵌套的）数据结构(list of lists)被讨论。
我们也成功地扩展了GGS-NN model到这个例子，更多的细节在Appendix C.</p>
<h3 id="experiments">6.5 Experiments</h3>
<p>在这篇文章中，我们产生了327 formulas的数据集，包含了三个程序变量，
498 graphs per formula, 产生了大约160,000 formula/heap graph
combinations.
为了评估，我们分隔数据到训练集、验证集和测试集按6:2:2的比例(测试集不会用在训练集上)。我们测试了准确率，是否formula预测在测试时间在逻辑上等于基本事实
等价通过规范化来近似公式的名称和顺序，然后进行比较以求完全相等。</p>
<p>我么比较了我们的GGS-NN 模型和Brockschmidt中提到的方法。
更早的方法把每一个验证不都对待为一个标准的分类任务，并且要求complex,
manual, problem-specific feature engineering,
来达到89.11%的准确率。作为对比，我们的新模型no feature
engineering和少量的domain knowledge来达到了89.96%的准确率。</p>
<p>一个heap graph 的例子和对应的logic formula从我们GGS-NN
model中发现的如fig4所示。 <img
src="gnn-parallel-ggsnn-paper-read/fig4.png" /> &gt;
比较了两个命名变量arg1, arg2. one isolated NULL node, nodel 1.
所有的边指向NULL没有进行显示。
边上的数字暗示了不同的边的类型。我们的GGS-NN模型成功地找到了正确的formul</p>
<p>这个例子还包括了nested data structures和the batching
extension在之前的section中提到的。</p>
<p>我们还成功的用我们的模型在程序验证的framework,
提供所需将定理证明给定理证明这，以证明列表操作集合的正确性比如插入排序等算法。
一下table
4展示了一组基准列表操作程序和由GGS-NN模型找到的分离逻辑公式不等式，分别是在验证框架中成功使用以证明相应程序的正确性。</p>
<p><img data-src="gnn-parallel-ggsnn-paper-read/table4.png" />
一个未来的扩展已经展示成功的证明了更复杂的程序比如排序程序，以及各种l其他list操作的程序。</p>
<h2 id="相关工作">7. 相关工作</h2>
<p>最相关的工作是GNNs, 我们之前讨论过。
Micheli建议了另一个紧密相关的模型，它不同于GNNs，主要在输出上。
GNNs被应用在许多领域，但是他们没有广阔地在ICLR社区传播。我么的部分目的publicize
GNNs是一个useful and interesting neural network variant.</p>
<p>一个比喻可以秒速我们从GNNs到GG-NNs的掉成， the work of Domke and
Stoyanov在strutured prediction setting.
这里的信念传播（这里必须运行在能够convergence,
以取得好的梯度）被替代为truncated(截断的)信念传播更新。然后这个模型被训练，所以truncated
iteration produce good results在一些迭代次数后。同样地，RNN扩展到了Tree
LSTM也相似与我们使用GRU来更新GG-NNs而不是使用标准的GNN，目的在于提高长时间的信息在graph
struture中的前向传播。 the general
idea表现在paper中的是组装特定问题的神经网络学习成分已经有一个很长的历史，可以追溯到the
work of Hinton， 用神经网络来根据家族数结构预测人与人之间的关系。 Graph
kernel能够被用于一系列的kernel-based learning tasks使用graph-strutured
inputs， 但是我们不知道learn the kernel 和outputs sequences.
Perozzi转化graph为sequences通过following random walks在图上，学习node
embedding 使用sequence-based的方法。 Sperduti map graphs to graph
vectors 然后output neural
network来分类这里有多种模型使用同样的节点表示的前向传播在图结构上。
他们的工作和GNNS的不同点在于卷积和recurrent networks.
Duvenaud认为convolutional想在图上的操作，建立一个可学习的，可微的关于graph
feature的变体。
LUsci转换认为的无向图为一定数量的不同的DAGs，然后向内传播节点表示每个根，训练一组模型。在以上所有内容上，重点都放在了一步式问题上。</p>
<p>GNNs和我们扩展具有滋镇网络的许多相同的理想属性。使用节点选择输出层，可以选择输入中的及诶按作为输出。
有两个区别：
首先，在GNN中，图结构是显式的，这使得模型通用性较低，但可以提供更强的泛化能力；第二，指针网络要求每个节点都有属性（比如,
空间中的位置），而GNN可以表示已定义的节点仅通过他们在图形中的位置即可，这使它们在不同维度上更具有通用性。
GGS-NN与软对齐和注意力名有关。有两个方面，第一，eg7中，等式的图形表示，
使用上下文将注意力集中在哪些节点对当前决策很重要；第二，节点程序验证例子中的注释跟踪已注释的哪些节点，因此，它地公馆一种明确的机制来确保输入中的每个节点都已经使用在产生输出的顺序上。</p>
<h2 id="discussion">8. Discussion</h2>
<ol type="1">
<li>What is being learned?
有启发的去思考GG-NNs能学到什么。为此，我们可以在bAbI任务的方式之间类比,
bAbI task15将会通过logical
formulation得到解决。作为一个例子，考虑右边的一个例子中需要回答的行。</li>
</ol>
<p>为了做逻辑推断，我们需要的不仅仅是关于故事中事实的逻辑编码，而且，关于世界的背景知识也编码为一个inference
rules</p>
<p>我们对任务的编码简化了将故事解析为图形形式的过程，但没有提供任何背景知识，可以将GG-NN模型视为学习此方法的结果存储在神经网络权重中。</p>
<ol start="2" type="1">
<li>Discussion
论文的结果表明，GGS-NN在整个过程中都具有理想的感应偏差，一系列具有固有图结构的问题，我们相信在很多问题，在更多情况下，GGS-NN将很有用。但是，有一些限制需要克服这些障碍，使其广泛地应用。我们前面提到的两个显示是bAbI任务翻译未包含输入的时间顺序或三进制或更高解关系。我们可以想象一下接触这些限制的可能性，例如将一个座位会议论文在ICLR2016上发布的一系列GG-NN,
每个边缘有一个GG-NN,
代表更高阶的关系，作为因子图。一个更大的挑战是如何处理结构化的输入表示形式。例如，在bAbI任务中，最好不要使用输入的符号形式。
一种可能的方法是在我们的GGS-NN中合并结构较少的输入和潜在向量。但是，需要进行实验以找到解决这些问题的最佳方法
## Analysis</li>
</ol>
<p>相比于传统的图数据处理方式，GNN、
GG-NN、GGS-NN等模型可以有效地利用图的拓扑结构信息，这也是为什么GGS-NN可以在Path
Finding任务中取得优异效果的原因。另外，由于这些模型在每个实践部都需要展开所有的节点，因而这些模型也可以用于处理各种图（包括有向图、无向图、有环图、无环图等）。但是，这同样也带来了一些问题：
1.
由于在每个实践部都需要展开所有的节点，每个节点还需要使用D维向量进行表示；当图很大且向量表示很大时，模型的效率问题就会变得很重要
2.
在GNN中需要保证凸的整体映射是一个压缩映射，这显然就减小了该模型所能建模的问题空间???(好像的要求时对于寻找压缩映射问题而言的)。为了解决这一问题，GG-NN将传播时间步固定为T,
虽然不需要保证收敛，但是，图上的信息传输却也因此受到了约束(???，信息的传输与时间步有关，因为终止条件不再是收敛，而是具体的时间步)。例如，在可达性预测任务中，在两个时间步后节点1的信息才可以到达节点4.虽然本文中采取了一定的措施来缓解这一问题（即不仅定义了沿边方向的转移举证，还定义了与边相反方向的转移矩阵）。但是只要T的大小有限，节点之间的信息就只能沿着路径传播T步，而不能像GNN那样到模型收敛才停止。换句话说，GG-NN实际上是以损失图中较长路径信息的代价换取了模型可建模的问题空间。</p>
<h2 id="参考文献">参考文献</h2>
<p>https://zhuanlan.zhihu.com/p/28170197</p>
<p>jianshu.com/p/40362662014a
https://github.com/microsoft/tf-gnn-samples</p>
<p>cnblogs.com/wacc/p/5341670.html
https://zhuanlan.zhihu.com/p/38051458</p>
<h2 id="阅读思考">阅读思考</h2>
<p>有空再阅读一遍，通过阅读发现，这里的图神经网络实际上node-selection,以及graph-level,
都是graph-level级别的任务，即得到每个节点的表示，然后由节点进行预测。</p>
<p>该论文和上篇semi-gnn形成鲜明对比 1. 上篇论文是inductive learning,
这篇是transactive learning. 2. 上篇是node-level
分类任务，这篇是graph-level任务，即node-selection.</p>
<p>好像是为了偷懒，所以才用三维，把对应的给空着</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/gnn-parallel/" rel="tag"># gnn-parallel</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/linux-keyboard-using-e7578316642f/" rel="prev" title="linux keyboard using">
      <i class="fa fa-chevron-left"></i> linux keyboard using
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/gnn-parallel-ggsnn-code-read-41f35521ee50/" rel="next" title="gnn-parallel-ggsnn-code-read">
      gnn-parallel-ggsnn-code-read <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
      <div class="tabs tabs-comment">
        <ul class="nav-tabs">
            <li class="tab"><a href="#comment-livere">livere</a></li>
            <li class="tab"><a href="#comment-gitalk">gitalk</a></li>
        </ul>
        <div class="tab-content">
            <div class="tab-pane livere" id="comment-livere">
              
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC80NjU1Mi8yMzA2Mg=="></div>
  </div>
  
            </div>
            <div class="tab-pane gitalk" id="comment-gitalk">
              <div class="comments" id="gitalk-container"></div>
            </div>
        </div>
      </div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#graph-neural-networks"><span class="nav-number">3.</span> <span class="nav-text">2. Graph Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#propagation-model"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Propagation Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#output-model-and-learning"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 Output Model and Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gated-graph-neural-networks"><span class="nav-number">4.</span> <span class="nav-text">3. Gated Graph Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#node-annatations"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Node Annatations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#propagation-model-%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%A8%A1%E5%9E%8B"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Propagation Model
前向传播模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#output-models"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Output Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#explanatory-applications"><span class="nav-number">5.</span> <span class="nav-text">5. Explanatory Applications</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#babi-tasks"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 bAbI Tasks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E8%BE%93%E5%87%BA%E4%BB%BB%E5%8A%A1"><span class="nav-number">5.1.1.</span> <span class="nav-text">单输出任务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E8%BE%93%E5%87%BA%E4%BB%BB%E5%8A%A1"><span class="nav-number">5.1.2.</span> <span class="nav-text">序列输出任务</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-graph-algorithms"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 Learning Graph Algorithms</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A8%8B%E5%BA%8F%E9%AA%8C%E8%AF%81%E9%97%AE%E9%A2%98-ggs-nns"><span class="nav-number">6.</span> <span class="nav-text">6 程序验证问题 GGS-NNs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#formulation-as-ggs-nns"><span class="nav-number">6.1.</span> <span class="nav-text">6.2 Formulation As GGS-NNs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-setup-details"><span class="nav-number">6.2.</span> <span class="nav-text">6.3 Model Setup Details</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-predication-details"><span class="nav-number">6.3.</span> <span class="nav-text">6.4 Batch Predication Details</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#experiments"><span class="nav-number">6.4.</span> <span class="nav-text">6.5 Experiments</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">7.</span> <span class="nav-text">7. 相关工作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discussion"><span class="nav-number">8.</span> <span class="nav-text">8. Discussion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">9.</span> <span class="nav-text">参考文献</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E6%80%9D%E8%80%83"><span class="nav-number">10.</span> <span class="nav-text">阅读思考</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Yun-Pan Wang"
      src="/images/default-avatar.png">
  <p class="site-author-name" itemprop="name">Yun-Pan Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">115</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">72</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">69</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AugF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AugF" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangyp@smail.nju.edu.cn" title="E-Mail → mailto:wangyp@smail.nju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/Joswxe" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;Joswxe" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.google.com/" title="https:&#x2F;&#x2F;www.google.com" rel="noopener" target="_blank">Google</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://cn.bing.com/" title="https:&#x2F;&#x2F;cn.bing.com" rel="noopener" target="_blank">Bing</a>
        </li>
        <li class="links-of-blogroll-item">
          <a href="https://www.baidu.com/" title="https:&#x2F;&#x2F;www.baidu.com" rel="noopener" target="_blank">Baidu</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">[object Object]</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">466k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:04</span>
</div>!

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '9552a863f697c06e137f',
      clientSecret: '65716dc606be017204585b55c9858ede23eae0b9',
      repo        : 'augf.github.io',
      owner       : 'AugF',
      admin       : ['AugF'],
      id          : 'dcec4a5d61a2e5ffbbc265cc29d64762',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
